{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dec8a5ff48d192d3613b9e6713a72fe103c8e5d5c44dc7b8ee41f3376e4a4f37",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Faster R-CNN 구현\n",
    "### 텐서플로우 기반\n",
    "### 주석은 한글 위주로 작성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "source": [
    "## 훈련 이미지 가져오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/JPEGImages'\n",
    "train_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/Annotations'\n",
    "\n",
    "test_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/JPEGImages'\n",
    "test_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5011\n4952\n"
     ]
    }
   ],
   "source": [
    "list_train_x = sorted([x for x in glob.glob(train_x_path + '/**')])    \n",
    "list_train_y = sorted([x for x in glob.glob(train_y_path + '/**')]) \n",
    "\n",
    "list_test_x = sorted([x for x in glob.glob(test_x_path + '/**')])    \n",
    "list_test_y = sorted([x for x in glob.glob(test_y_path + '/**')]) \n",
    "\n",
    "print(len(list_train_x))\n",
    "print(len(list_test_x))"
   ]
  },
  {
   "source": [
    "## 훈련용 데이터로 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 테스트용\n",
    "def get_image_224(image_file_path) :\n",
    "    image = cv2.imread(image_file_path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력용 이미지 생성. 224, 224로 변환시키고 채널 값(0~255)를 0~1 사이의 값으로 정규화 시켜줌\n",
    "def make_input(image_file_list): \n",
    "    images_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_file_list)), desc=\"get image\") :\n",
    "        \n",
    "        image = cv2.imread(image_file_list[i])\n",
    "        image = cv2.resize(image, (224, 224))/255\n",
    "        \n",
    "        images_list.append(image)\n",
    "    \n",
    "    return np.asarray(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPN 훈련할 때는 객체 종류를 알 필요가 없다. 왜냐하면 RPN은 객체가 존재하는 박스 위치만 알면 되기 때문이다. \n",
    "# 그러면 객체 종류를 어디서 알아야 하는 것일까. 바로 Detector를 훈련시킬 때 필요하다. \n",
    "# 존재하는 객체 종류를 알아내자\n",
    "def get_Classes_inImage(xml_file_list):\n",
    "    classes = []\n",
    "\n",
    "    for xml_file_path in xml_file_list: \n",
    "\n",
    "        f = open(xml_file_path)\n",
    "        xml_file = xmltodict.parse(f.read())\n",
    "        # 사진에 객체가 여러개 있을 경우\n",
    "        try: \n",
    "            for obj in xml_file['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) # 들어있는 객체 종류를 알아낸다\n",
    "        # 사진에 객체가 하나만 있을 경우\n",
    "        except TypeError as e: \n",
    "            classes.append(xml_file['annotation']['object']['name'].lower()) \n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) # set은 중복된걸 다 제거하고 유니크한? 아무튼 하나만 가져온다. 그걸 리스트로 만든다\n",
    "    classes.sort() # 정렬\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "source": [
    "## RPN 훈련을 위한 데이터셋 생성에 필요한 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지에 어떤 Ground Truth Box가 있는지\n",
    "def get_label_fromImage_RPN(xml_file_path): # xml_file_path은 파일 하나의 경로를 나타낸다\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    xml_file = xmltodict.parse(f.read()) \n",
    "\n",
    "    # 우선 원래 이미지 크기를 얻는다. 왜냐하면 앵커는 224*224 기준으로 만들었는데 원본 이미지는 224*224가 아니기 때문.\n",
    "    # 224*224에 맞게 줄일려고 하는거다\n",
    "    Image_Height = float(xml_file['annotation']['size']['height'])\n",
    "    Image_Width  = float(xml_file['annotation']['size']['width'])\n",
    "\n",
    "    Ground_Truth_Box_list = [] \n",
    "\n",
    "    # multi-objects in image\n",
    "    try:\n",
    "        for obj in xml_file['annotation']['object']:\n",
    "            \n",
    "            # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "            x_min = float(obj['bndbox']['xmin']) \n",
    "            y_min = float(obj['bndbox']['ymin'])\n",
    "            x_max = float(obj['bndbox']['xmax']) \n",
    "            y_max = float(obj['bndbox']['ymax'])\n",
    "\n",
    "            # 224*224에 맞게 변형시켜줌\n",
    "            x_min = float((224/Image_Width)*x_min)\n",
    "            y_min = float((224/Image_Height)*y_min)\n",
    "            x_max = float((224/Image_Width)*x_max)\n",
    "            y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "            Ground_Truth_Box = [x_min, y_min, x_max, y_max]\n",
    "            Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    # single-object in image\n",
    "    except TypeError as e : \n",
    "        # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "        x_min = float(xml_file['annotation']['object']['bndbox']['xmin']) \n",
    "        y_min = float(xml_file['annotation']['object']['bndbox']['ymin']) \n",
    "        x_max = float(xml_file['annotation']['object']['bndbox']['xmax']) \n",
    "        y_max = float(xml_file['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "        # 224*224에 맞게 변형시켜줌\n",
    "        x_min = float((224/Image_Width)*x_min)\n",
    "        y_min = float((224/Image_Height)*y_min)\n",
    "        x_max = float((224/Image_Width)*x_max)\n",
    "        y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "        Ground_Truth_Box = [x_min, y_min, x_max, y_max]  \n",
    "        Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    \n",
    "    Ground_Truth_Box_list = np.asarray(Ground_Truth_Box_list)\n",
    "    Ground_Truth_Box_list = np.reshape(Ground_Truth_Box_list, (-1, 4))\n",
    "\n",
    "    return Ground_Truth_Box_list # 이미지에 있는 Ground Truth Box 리스트 받기(numpy)\n"
   ]
  },
  {
   "source": [
    "## RPN - 앵커 준비\n",
    "#### 입력 이미지 기준으로 앵커를 생성한다.\n",
    "#### 풀링을 3번 하므로 2^3 = 8이니 (8, 8)부터 (16, 8)...등 8픽셀식 중심 좌표를 옮겨가며 앵커들을 k개씩 생성한다\n",
    "#### 생성한 앵커들 중 사용할 가치가 있는 앵커를 걸러낸다(이미지 범위를 벗어나지 않는 앵커들만 선정)\n",
    "#### 선정한 앵커 중 실제 물체의 box와 얼마나 곂치는지(IoU) 계산해본다. 확실히 겹친다 하는 애들을 Positive 앵커로, 거의 안겹친다 하는 애들은 Negataive 앵커로 선정한다. 애매한 애들은 거른다.\n",
    "#### 이렇게 생성된 앵커들로 미니배치를 생성한다. Positive 128개, Negative 128개로 만드는게 ideal한 구성이긴 한데 Positive한 앵커가 별로 없다. 그래서 Positive를 128개 못채웠으면 Negataive 앵커로 채워준다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 생성 함수. \n",
    "def make_anchor(anchor_size, anchor_aspect_ratio) :\n",
    "    # 입력 이미지(그래봤자 224*224긴 하지만)에 맞춰 앵커를 생성해보자 \n",
    "\n",
    "    anchors = [] # [x,y,w,h]로 이루어진 리스트 \n",
    "    anchors_state = [] # 이 앵커를 훈련에 쓸건가? 각 앵커별로 사용 여부를 나타낸다. \n",
    "\n",
    "    # 앵커 중심좌표 간격\n",
    "    interval_x = 16\n",
    "    interval_y = 16\n",
    "    Center_max_x = 208 # 224 - 16, 중심좌표가 224가 될 수는 없다.\n",
    "    Center_max_y = 208 # 224 - 16\n",
    "\n",
    "    # 2단 while문 생성\n",
    "    x = 8\n",
    "    y = 8\n",
    "    index_count = 0\n",
    "    while(y <= 224): # 8~208 = 14개 \n",
    "        while(x <= 224): # 8~208 = 14개 \n",
    "            # k개의 앵커 생성. 여기서 k = len(anchor_size) * len(anchor_aspect_ratio)다\n",
    "            for i in range(0, len(anchor_size)) : \n",
    "                for j in range(0, len(anchor_aspect_ratio)) :\n",
    "                    anchor_width = anchor_aspect_ratio[j][0] * anchor_size[i]\n",
    "                    anchor_height = anchor_aspect_ratio[j][1] * anchor_size[i]\n",
    "\n",
    "                    anchor = [x, y, anchor_width, anchor_height]\n",
    "                    anchors.append(anchor)\n",
    "                    # 앵커가 이미지 경계선을 넘나드나? 필터링\n",
    "                    if((x - (anchor_width/2) >= 0) and (y - (anchor_height/2) >= 0) and\n",
    "                    (x + (anchor_width/2) <= 224) and (y + (anchor_height/2) <= 224)):\n",
    "                        # 경계 안에 있으면 1\n",
    "                        anchors_state.append(int(1))\n",
    "                    else :\n",
    "                        anchors_state.append(int(0))\n",
    "            x = x + interval_x \n",
    "        y = y + interval_y\n",
    "        x = 8\n",
    "    return np.asarray(anchors), np.asarray(anchors_state) # 넘파이로 반환"
   ]
  },
  {
   "source": [
    "### IoU 계산 후 Positive, Negative 앵커 분류"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커들을 Positive, Negative 앵커로 나누고 각 앵커가 참고한 Ground Truth Box와 Class를 반환하자\n",
    "# RPN에는 '어떤 클래스인가?'는 알 필요가 없다. '객체인가 아닌가'이거 하나만 필요할 뿐. \n",
    "def align_anchor(anchors, anchors_state, Ground_Truth_Box_list):\n",
    "\n",
    "    # 각 앵커는 해당 위치에서 구한 여러가지 Ground truth Box와의 ioU 중 제일 높은거만 가져온다. \n",
    "    IoU_List = np.array([])\n",
    "    Ground_truth_box_Highest_IoU_List = [] # 각 앵커가 어떤 Ground Truth Box를 보고 IoU를 계산했는가?\n",
    "\n",
    "    #start = time.time()\n",
    "\n",
    "    for i in range(0, len(anchors)):\n",
    "        if anchors_state[i] == 0 :\n",
    "            IoU_List = np.append(IoU_List, 0)\n",
    "            Ground_truth_box_Highest_IoU_List.append([0,0,0,0])\n",
    "\n",
    "            if i % 9 == 8 :\n",
    "                IoU_List_inOneSpot = IoU_List[i-8:i+1]\n",
    "                for num in list(range(i-8, i + 1)):\n",
    "                    if IoU_List[num] > 0.7 or (max(IoU_List_inOneSpot) == IoU_List[num] and IoU_List[num] >= 0.3): # positive anchor\n",
    "                        anchors_state[num] = 2\n",
    "                    elif IoU_List[num] < 0.3 : # negative anchor\n",
    "                        anchors_state[num] = 1\n",
    "                    else: # 애매한 앵커들\n",
    "                        anchors_state[num] = 0    \n",
    "        else:\n",
    "            anchor_minX = anchors[i][0] - (anchors[i][2]/2)\n",
    "            anchor_minY = anchors[i][1] - (anchors[i][3]/2)\n",
    "            anchor_maxX = anchors[i][0] + (anchors[i][2]/2)\n",
    "            anchor_maxY = anchors[i][1] + (anchors[i][3]/2)\n",
    "\n",
    "            anchor = [anchor_minX, anchor_minY, anchor_maxX, anchor_maxY]\n",
    "\n",
    "            # 연산 속도 때문에 Box대신 Ground Truth Box의 인덱스를 저장\n",
    "            IoU_max = 0\n",
    "            ground_truth_box_Highest_IoU = [0,0,0,0]\n",
    "\n",
    "            for j in range(0, len(Ground_Truth_Box_list)):\n",
    "\n",
    "                ground_truth_box = Ground_Truth_Box_list[j]\n",
    "\n",
    "                InterSection_min_x = max(anchor[0], ground_truth_box[0])\n",
    "                InterSection_min_y = max(anchor[1], ground_truth_box[1])\n",
    "\n",
    "                InterSection_max_x = min(anchor[2], ground_truth_box[2])\n",
    "                InterSection_max_y = min(anchor[3], ground_truth_box[3])\n",
    "\n",
    "                InterSection_Area = 0\n",
    "\n",
    "                if (InterSection_max_x - InterSection_min_x + 1) >= 0 and (InterSection_max_y - InterSection_min_y + 1) >= 0 :\n",
    "                    InterSection_Area = (InterSection_max_x - InterSection_min_x + 1) * (InterSection_max_y - InterSection_min_y + 1)\n",
    "\n",
    "                box1_area = (anchor[2] - anchor[0]) * (anchor[3] - anchor[1])\n",
    "                box2_area = (ground_truth_box[2] - ground_truth_box[0]) * (ground_truth_box[3] - ground_truth_box[1])\n",
    "                Union_Area = box1_area + box2_area - InterSection_Area\n",
    "\n",
    "                IoU = (InterSection_Area/Union_Area)\n",
    "                if IoU > IoU_max :\n",
    "                    IoU_max = IoU\n",
    "                    ground_truth_box_Highest_IoU = ground_truth_box\n",
    "\n",
    "            IoU_List = np.append(IoU_List, IoU_max)\n",
    "            Ground_truth_box_Highest_IoU_List.append(ground_truth_box_Highest_IoU)\n",
    "\n",
    "            # 한 위치에 9개의 앵커 존재 -> 9개 앵커에 대한 IoU를 계산할 때마다 모아서 Positive, Negative 앵커 분류\n",
    "            if i % 9 == 8 :\n",
    "                IoU_List_inOneSpot = IoU_List[i-8:i+1]\n",
    "                for num in list(range(i-8, i + 1)):\n",
    "                    if IoU_List[num] > 0.7 or (max(IoU_List_inOneSpot) == IoU_List[num] and IoU_List[num] >= 0.3): # positive anchor\n",
    "                        anchors_state[num] = 2\n",
    "                    elif IoU_List[num] < 0.3 : # negative anchor\n",
    "                        anchors_state[num] = 1\n",
    "                    else: # 애매한 앵커들\n",
    "                        anchors_state[num] = 0     \n",
    "\n",
    "    Ground_truth_box_Highest_IoU_List = np.asarray(Ground_truth_box_Highest_IoU_List)\n",
    "    Ground_truth_box_Highest_IoU_List = np.reshape(Ground_truth_box_Highest_IoU_List, (-1, 4))\n",
    "            \n",
    "    return anchors_state, Ground_truth_box_Highest_IoU_List # 각 앵커의 상태, (모든)앵커가 IoU 계산에 참조한 Ground Truth Box"
   ]
  },
  {
   "source": [
    "## RPN을 위한 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPN훈련을 위한 데이터셋. \n",
    "# RPN과 Detector는 별개의 모델이다. 즉, 두 모델을 훈련시킬 때 필요한 데이터셋은 따로따로 만들어야한다. \n",
    "# 여기선 RPN 훈련에 필요한 데이터만 만들거다. \n",
    "def make_dataset_forRPN(input_list) :\n",
    "    image_file_list = input_list[0]\n",
    "    xml_file_list = input_list[1]\n",
    "    anchors = input_list[2]\n",
    "    anchors_state = input_list[3]\n",
    "\n",
    "    image_list = make_input(image_file_list) # 입력\n",
    "    # 출력\n",
    "    cls_layer_label_list = np.array([])\n",
    "    reg_layer_label_list = np.array([])\n",
    "\n",
    "    # 값 계속 생성하는거 막기위한 변수\n",
    "    cls_label_forPositive = np.array([1.0,0.0])\n",
    "    cls_label_forNegative = np.array([0.0,1.0])\n",
    "    cls_label_forUseless  = np.array([0.0,0.0])\n",
    "\n",
    "    reg_label_forNotPositive = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for i in tqdm(range(0, len(xml_file_list)), desc=\"get label\"): # 각 이미지별로 데이터셋 생성(5011개)\n",
    "\n",
    "        anchors_state_for = anchors_state # anchors_state는 매 사진마다 다르니까 원본값(?)을 복사해서 쓴다. \n",
    "        Ground_Truth_Box_list = get_label_fromImage_RPN(xml_file_list[i]) # 여기서는 Ground Truth Box에 대한 정보만 필요하다\n",
    "        anchors_state_for, Ground_truth_box_Highest_IoU_List = align_anchor(anchors, anchors_state_for, Ground_Truth_Box_list)\n",
    "        # 어떤 앵커가 Pos, neg 앵커인지, (모든)앵커가 참조한 ground truth box는 뭔지\n",
    "    \n",
    "        #start = time.time()\n",
    "        # 연산 시간 때문에 생성(1761개치 모아놨다가 한 번에 추가하기)\n",
    "        cls_layer_label_list_for = np.array([])\n",
    "        reg_layer_label_list_for = np.array([])\n",
    "        for j in range(0, len(anchors_state_for)) :\n",
    "            if anchors_state_for[j] == 2 : # positive\n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forPositive)\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, Ground_truth_box_Highest_IoU_List[j]) # IoU계산에 참조한(pos, neg 분류에 기여한) Ground Truth Box의 정보 휙득\n",
    "            elif anchors_state_for[j] == 1 : # negative는 Ground Truth Box 정보가 필요없으니 [0,0,0,0]을 넣는다. \n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forNegative) # 해당 앵커 output이 [0,1] -> negative\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, reg_label_forNotPositive)\n",
    "            else : \n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forUseless) # 해당 앵커 output이 [0.5, 0.5] -> 무의미한 값\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, reg_label_forNotPositive)\n",
    "        \n",
    "        # 넘파이 배열로 변환\n",
    "        cls_layer_label_list = np.append(cls_layer_label_list, cls_layer_label_list_for)\n",
    "        reg_layer_label_list = np.append(reg_layer_label_list, reg_layer_label_list_for)\n",
    "        #print(\"\\nmaking label time :\", time.time() - start)\n",
    "\n",
    "    # 논문에서 말한 출력값 크기에 맞게 reshape\n",
    "    cls_layer_label_list = np.reshape(cls_layer_label_list, (-1, 14,14,18)) \n",
    "    reg_layer_label_list = np.reshape(reg_layer_label_list, (-1, 14,14,36))\n",
    "\n",
    "    return image_list, cls_layer_label_list, reg_layer_label_list # 훈련 데이터들(입, 출력)"
   ]
  },
  {
   "source": [
    "## RPN에 쓰일 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'image_file_list' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dd42813ca534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mimage_file_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml_file_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image_file_list' is not defined"
     ]
    }
   ],
   "source": [
    "del image_file_list, xml_file_list, anchor_size, anchor_aspect_ratio, anchors, anchors_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get image: 100%|██████████| 5011/5011 [00:17<00:00, 280.19it/s]\n",
      "get label: 100%|██████████| 5011/5011 [15:01<00:00,  5.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# 파일 리스트\n",
    "image_file_list = sorted([x for x in glob.glob(train_x_path + '/**')])\n",
    "xml_file_list = sorted([x for x in glob.glob(train_y_path + '/**')])\n",
    "\n",
    "anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준 \n",
    "anchors, anchors_state = make_anchor(anchor_size, anchor_aspect_ratio) # 앵커 생성 + 유효한 앵커 인덱스 휙득\n",
    "\n",
    "image_list, cls_layer_label_list, reg_layer_label_list = make_dataset_forRPN([image_file_list, xml_file_list, anchors, anchors_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image_list.shape :  (5011, 224, 224, 3)\ncls_layer_label_list.shape :  (5011, 14, 14, 18)\nreg_layer_label_list.shape :  (5011, 14, 14, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"image_list.shape : \", image_list.shape)\n",
    "print(\"cls_layer_label_list.shape : \", cls_layer_label_list.shape)\n",
    "print(\"reg_layer_label_list.shape : \", reg_layer_label_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[  9.856       16.72533333 112.448      163.072     ]\n[112.448       16.72533333 212.8        159.488     ]\n[112.448       16.72533333 212.8        159.488     ]\n"
     ]
    }
   ],
   "source": [
    "test = np.reshape(cls_layer_label_list[56], (-1, 2))\n",
    "test_reg = np.reshape(reg_layer_label_list[56], (-1, 4))\n",
    "\n",
    "for i in range(0, len(test)):\n",
    "    if test[i][0] == 1.0 :\n",
    "        print(test_reg[i])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 훈련을 위한 Loss 함수\n",
    "### Loss함수 내에서 미니배치를 선별"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch_index(output):\n",
    "    # 2개의 loss함수에서 불리는 함수. 우선 cls_layer의 loss에서 불린 뒤 reg_layer의 loss에서 불릴거다. \n",
    "    # 그러니 우선 cls에서 부를 때 미니배치 인덱스를 선별해 반환하고 그 다음 reg에서 부를 때는 선별된 인덱스 리스트를 반환 후 초기화한다.\n",
    "    # print(\"\\n output shape : \", output.shape)\n",
    "    index_list = np.array([])\n",
    "    if len(get_minibatch_index.index_list) == 0:\n",
    "        get_minibatch_index.index_list = np.zeros(14*14*9) # 각 앵커가 미니배치 뽑혔나 안뽑혔나\n",
    "        index_pos = np.array([])\n",
    "        index_neg = np.array([])\n",
    "        # cls_layer_output을 보고 긍정, 부정 앵커 분류. 그렇게 데이터셋을 구성함\n",
    "        for i in range(0, 1764):\n",
    "            if output[i][0] == 1.0 : # positive anchor\n",
    "                index_pos = np.append(index_pos, i)\n",
    "            elif output[i][0] == 0.0 : # negative anchor\n",
    "                index_neg = np.append(index_neg, i)\n",
    "\n",
    "        max_for = min([128, len(index_pos)])\n",
    "        ran_list = random.sample(range(0, len(index_pos)), max_for)\n",
    "\n",
    "        for i in range(0, len(ran_list)) :\n",
    "            index = int(index_pos[ran_list[i]])\n",
    "            get_minibatch_index.index_list[index] = 1\n",
    "\n",
    "        ran_list = random.sample(range(0, len(index_neg)), 256 - max_for) # 랜덤성 증가?를 위해 또다시 난수 생성\n",
    "        for i in range(0, len(ran_list)) :\n",
    "            index = int(index_neg[ran_list[i]])\n",
    "            get_minibatch_index.index_list[index] = 1\n",
    "\n",
    "        index_list = get_minibatch_index.index_list\n",
    "\n",
    "    else : # 뽑아놓은 미니배치가 이미 존재\n",
    "        index_list = get_minibatch_index.index_list\n",
    "        get_minibatch_index.index_list = np.array([])\n",
    "\n",
    "    return index_list\n",
    "get_minibatch_index.index_list = np.array([])"
   ]
  },
  {
   "source": [
    "## Loss 계산을 위한 함수들"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_cls\n",
    "def Loss_Classes(cls_layer_output, pi_ground_truth) :\n",
    "    log_loss = -pi_ground_truth * math.log10(cls_layer_output[0]) - (1 - pi_ground_truth) * math.log10(cls_layer_output[1])\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rpn_cls(y_true, y_pred): \n",
    "    cls_layer_output = y_pred.numpy()[0]\n",
    "    cls_layer_output_label = y_true.numpy()[0]\n",
    "\n",
    "    cls_layer_output = np.reshape(cls_layer_output, (-1, 2))\n",
    "    cls_layer_output_label = np.reshape(cls_layer_output_label, (-1, 2))\n",
    "\n",
    "    minibatch_index_list = get_minibatch_index(cls_layer_output_label)\n",
    "    cls_layer_output_minibatch = np.array([])\n",
    "    cls_layer_output_label_minibatch = np.array([])\n",
    "\n",
    "    # 미니배치 선별\n",
    "    for i in range(0, len(minibatch_index_list)) :  \n",
    "        if minibatch_index_list[i] == 1 : # 미니배치로 뽑혔으면 \n",
    "\n",
    "            object_score_exp = np.exp(cls_layer_output[i][0])\n",
    "            non_object_score_exp = np.exp(cls_layer_output[i][1])\n",
    "            exp_sum = object_score_exp + non_object_score_exp\n",
    "            score = np.array([object_score_exp / exp_sum, non_object_score_exp / exp_sum])\n",
    "\n",
    "            cls_layer_output_minibatch = np.append(cls_layer_output_minibatch, score)\n",
    "            cls_layer_output_label_minibatch = np.append(cls_layer_output_label_minibatch, cls_layer_output_label[i])\n",
    "\n",
    "    cls_layer_output_minibatch = np.reshape(cls_layer_output_minibatch, (-1, 2))\n",
    "    cls_layer_output_label_minibatch = np.reshape(cls_layer_output_label_minibatch, (-1, 2))\n",
    "\n",
    "    Lcls_sum = 0.0\n",
    "    # loss 계산\n",
    "    for i in range(0, 256):\n",
    "        pi_star = cls_layer_output_label_minibatch[i][0] # postive면 1, negative면 0\n",
    "        Lcls_sum = Lcls_sum + Loss_Classes(cls_layer_output_minibatch[i], pi_star)\n",
    "\n",
    "    loss = (1.0/256.0) * Lcls_sum\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls Loss를 구하기 위해 만든 Output 처리 함수\n",
    "def get_output_reg_output_RPN(reg_layer_output):\n",
    "    # 예측값 가공\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (14,14,36))\n",
    "\n",
    "    for y in range(0, 14):\n",
    "        for x in range(0, 14):\n",
    "                for i in range(0, len(anchor_size)):\n",
    "                    for j in range(0, len(anchor_aspect_ratio)):\n",
    "\n",
    "                        center_x = 8 + 16 * x \n",
    "                        center_y = 8 + 16 * y\n",
    "                        w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                        h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                        # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                        reg_layer_output[y][x][4*(3*i+j)] = center_x + reg_layer_output[y][x][4*(3*i+j)]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 1] = center_y + reg_layer_output[y][x][4*(3*i+j) + 1]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 2] = w + reg_layer_output[y][x][4*(3*i+j) + 2]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 3] = h + reg_layer_output[y][x][4*(3*i+j) + 3]\n",
    "    return reg_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smooth_L1(ti, ti_star) :\n",
    "    difference_ti = ti - ti_star\n",
    "    smooth_L1 = 0\n",
    "    if abs(difference_ti) < 1: smooth_L1 = 0.5 * (difference_ti * difference_ti)\n",
    "    else : smooth_L1 = abs(difference_ti) - 0.5\n",
    "\n",
    "    return smooth_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss_Regression. Lreg에 해당\n",
    "def Loss_Regression(predict_box, anchor_box, groundTruth_box) : \n",
    "    groundTruth_box = np.array([(groundTruth_box[2] + groundTruth_box[0])/2, (groundTruth_box[1] + groundTruth_box[3])/2, groundTruth_box[2] - groundTruth_box[0], groundTruth_box[3] - groundTruth_box[1]])\n",
    "\n",
    "    t_x = (predict_box[0] - anchor_box[0])/anchor_box[2]\n",
    "    t_y = (predict_box[1] - anchor_box[1])/anchor_box[3]\n",
    "    t_w = math.log10(predict_box[2]/anchor_box[2])\n",
    "    t_h = math.log10(predict_box[3]/anchor_box[3])\n",
    "\n",
    "\n",
    "    t_x_star = (groundTruth_box[0] - anchor_box[0])/anchor_box[2]\n",
    "    t_y_star = (groundTruth_box[1] - anchor_box[1])/anchor_box[3]\n",
    "    t_w_star = math.log10(groundTruth_box[2]/anchor_box[2])\n",
    "    t_h_star = math.log10(groundTruth_box[3]/anchor_box[3])\n",
    "\n",
    "    # Smooth L1 구하기\n",
    "    # 구성요소가 4개니까 4번 구해야겠지?\n",
    "    smooth_L1_x = Smooth_L1(t_x, t_x_star)\n",
    "    smooth_L1_y = Smooth_L1(t_y, t_y_star)\n",
    "    smooth_L1_w = Smooth_L1(t_w, t_w_star)\n",
    "    smooth_L1_h = Smooth_L1(t_h, t_h_star)\n",
    "\n",
    "    sum_smooth_L1 = smooth_L1_x + smooth_L1_y + smooth_L1_w + smooth_L1_h\n",
    "    return sum_smooth_L1 # 모아서 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rpn_reg(y_true, y_pred): \n",
    "    \n",
    "    lambda_forLoss = 10.0\n",
    "    N_reg = 14.0*14.0*9.0\n",
    "\n",
    "    reg_layer_output = y_pred.numpy()[0]\n",
    "    reg_layer_output_label = y_true.numpy()[0]\n",
    "\n",
    "    reg_layer_output = get_output_reg_output_RPN(reg_layer_output) # 각 앵커를 기준으로 하는 추측값으로 변환\n",
    "\n",
    "    # (-1, 4)로 변환 -> 각 앵커별 (x,y,w,h)\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (-1, 4))\n",
    "    reg_layer_output_label = np.reshape(reg_layer_output_label, (-1, 4))\n",
    "\n",
    "    minibatch_index_list = get_minibatch_index(reg_layer_output_label)\n",
    "    reg_layer_output_minibatch = np.array([])\n",
    "    reg_layer_output_label_minibatch = np.array([])\n",
    "    anchors_minibatch = np.array([])\n",
    "\n",
    "\n",
    "    # 미니배치 선별\n",
    "    for i in range(0, len(minibatch_index_list)) : \n",
    "        if minibatch_index_list[i] == 1 : # 미니배치로 뽑혔으면 \n",
    "            reg_layer_output_minibatch = np.append(reg_layer_output_minibatch, reg_layer_output[i])\n",
    "            reg_layer_output_label_minibatch = np.append(reg_layer_output_label_minibatch, reg_layer_output_label[i])\n",
    "            anchors_minibatch = np.append(anchors_minibatch, anchors[i])\n",
    "\n",
    "    # 미니배치 reshape\n",
    "    reg_layer_output_minibatch = np.reshape(reg_layer_output_minibatch, (-1, 4))\n",
    "    reg_layer_output_label_minibatch = np.reshape(reg_layer_output_label_minibatch, (-1, 4))\n",
    "    anchors_minibatch = np.reshape(anchors_minibatch, (-1, 4))\n",
    "\n",
    "    Lreg_sum = 0.0\n",
    "    # loss 계산\n",
    "    for i in range(0, 256):\n",
    "        # x,y,w,h 합이 0이 아니면 로스에 포함\n",
    "        if reg_layer_output_label_minibatch[i][0] + reg_layer_output_label_minibatch[i][1] + reg_layer_output_label_minibatch[i][2] + reg_layer_output_label_minibatch[i][3] > 0 :\n",
    "            Lreg_sum = Lreg_sum + Loss_Regression(reg_layer_output_minibatch[i], anchors_minibatch[i], reg_layer_output_label_minibatch[i])\n",
    "\n",
    "    \n",
    "    loss = (lambda_forLoss/N_reg)*Lreg_sum\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "source": [
    "## RPN 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(tf.keras.Model):\n",
    "  def __init__(self, initializer, regularizer, shared_convNet, anchor_size, anchor_aspect_ratio):\n",
    "    super(RPN, self).__init__(name='rpn')\n",
    "\n",
    "    self.anchor_size = anchor_size\n",
    "    self.anchors_state = anchors_state\n",
    "\n",
    "    # 공용 레이어\n",
    "    self.conv1_1 = SharedConvNet.layers[0]\n",
    "    self.conv1_2 = SharedConvNet.layers[1]\n",
    "    self.pooling_1 = SharedConvNet.layers[2]\n",
    "\n",
    "    self.conv2_1 = SharedConvNet.layers[3]\n",
    "    self.conv2_2 = SharedConvNet.layers[4]\n",
    "    self.pooling_2 = SharedConvNet.layers[5]\n",
    "\n",
    "    self.conv3_1 = SharedConvNet.layers[6]\n",
    "    self.conv3_2 = SharedConvNet.layers[7]\n",
    "    self.conv3_3 = SharedConvNet.layers[8]\n",
    "    self.pooling_3 = SharedConvNet.layers[9]\n",
    "\n",
    "    self.conv4_1 = SharedConvNet.layers[10]\n",
    "    self.conv4_2 = SharedConvNet.layers[11]\n",
    "    self.conv4_3 = SharedConvNet.layers[12]\n",
    "    self.pooling_4 = SharedConvNet.layers[13]\n",
    "\n",
    "    self.conv5_1 = SharedConvNet.layers[14]\n",
    "    self.conv5_2 = SharedConvNet.layers[15]\n",
    "    self.conv5_3 = SharedConvNet.layers[16]\n",
    "    \n",
    "    # RPN만의 레이어\n",
    "    self.intermediate_layer = tf.keras.layers.Conv2D(512, (3, 3), padding = 'SAME' , activation = 'relu', name = \"intermediate_layer\", input_shape = (14,14,512))\n",
    "    self.cls_Layer = tf.keras.layers.Conv2D(18, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_1\")\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(36, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_2\")\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    # 정방향 연산\n",
    "    output1_1 = self.conv1_1(inputs)\n",
    "    output1_2 = self.conv1_2(output1_1)\n",
    "    output1_pooling = self.pooling_1(output1_2)\n",
    "\n",
    "    output2_1 = self.conv2_1(output1_pooling)\n",
    "    output2_2 = self.conv2_2(output2_1)\n",
    "    output2_pooling = self.pooling_2(output2_2)\n",
    "\n",
    "    output3_1 = self.conv3_1(output2_pooling)\n",
    "    output3_2 = self.conv3_2(output3_1)\n",
    "    output3_3 = self.conv3_3(output3_2)\n",
    "    output3_pooling = self.pooling_3(output3_3)\n",
    "\n",
    "    output4_1 = self.conv4_1(output3_pooling)\n",
    "    output4_2 = self.conv4_2(output4_1)\n",
    "    output4_3 = self.conv4_3(output4_2)\n",
    "    output4_pooling = self.pooling_4(output4_3)\n",
    "\n",
    "    output5_1 = self.conv5_1(output4_pooling)\n",
    "    output5_2 = self.conv5_2(output5_1)\n",
    "    output5_3 = self.conv5_3(output5_2)\n",
    "    # RPN\n",
    "    feature_map = self.intermediate_layer(output5_3)\n",
    "    cls_Layer_output = self.cls_Layer(feature_map)\n",
    "    reg_Layer_output = self.reg_layer(feature_map)\n",
    "\n",
    "    return cls_Layer_output, reg_Layer_output # 4k, 2k개 아웃풋 반환\n",
    "\n",
    "\n",
    "  def Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio):\n",
    "\n",
    "\n",
    "    for y in range(0, 14):\n",
    "          for x in range(0, 14):\n",
    "                  for i in range(0, len(anchor_size)):\n",
    "                      for j in range(0, len(anchor_aspect_ratio)):\n",
    "                          # 오브젝트다 vs 아니다 2개 항목에 대한 스코어 저장(각 앵커당)\n",
    "                          index = 3 * i + j\n",
    "                          object_score_exp = np.exp(cls_layer_output[y][x][2 * index] * 1e32) # 출력값이 작아서 1e32를 구현하다\n",
    "                          non_object_score_exp = np.exp(cls_layer_output[y][x][2 * index + 1] * 1e32) \n",
    "\n",
    "                          # print(\"object_score_exp, non_object_score_exp : \", object_score_exp, non_object_score_exp, \"\\n\")\n",
    "\n",
    "                          exp_sum = object_score_exp + non_object_score_exp\n",
    "                          score = np.array([object_score_exp / exp_sum, non_object_score_exp / exp_sum])\n",
    "                          # print(score[0], score[1], \"\\n\")\n",
    "                          cls_layer_output[y][x][2*(3*i+j)] = score[0]\n",
    "                          cls_layer_output[y][x][2*(3*i+j) + 1] = score[1]\n",
    "                          \n",
    "                          center_x = 8 + 16 * x \n",
    "                          center_y = 8 + 16 * y\n",
    "                          w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                          h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                          # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                          reg_layer_output[y][x][4*(3*i+j)] = center_x + reg_layer_output[y][x][4*(3*i+j)]\n",
    "                          reg_layer_output[y][x][4*(3*i+j) + 1] = center_y + reg_layer_output[y][x][4*(3*i+j) + 1]\n",
    "                          reg_layer_output[y][x][4*(3*i+j) + 2] = w + reg_layer_output[y][x][4*(3*i+j) + 2]\n",
    "                          reg_layer_output[y][x][4*(3*i+j) + 3] = h + reg_layer_output[y][x][4*(3*i+j) + 3]\n",
    "\n",
    "      cls_layer_output = np.reshape(cls_layer_output, (-1,2))\n",
    "      reg_layer_output = np.reshape(reg_layer_output, (-1,4))\n",
    "\n",
    "      return cls_layer_output, reg_layer_output\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_RPN(model, image_list, reg_layer_label_list, cls_layer_label_list, traning_step, EPOCH):\n",
    "    \n",
    "    if traning_step == 1 : # 처음에는 conv3_1까지만 훈련\n",
    "        model.conv1_1.trainable = False\n",
    "        model.conv1_2.trainable = False\n",
    "        model.conv2_1.trainable = False\n",
    "        model.conv2_2.trainable = False\n",
    "    elif traning_step == 3 : # 두번 째, 그러니까 training step == 3일 때는 sharedConvLayer를 고정\n",
    "        model.conv1_1.trainable = False\n",
    "        model.conv1_2.trainable = False\n",
    "        model.conv2_1.trainable = False\n",
    "        model.conv2_2.trainable = False\n",
    "        model.conv3_1.trainable = False\n",
    "        model.conv3_2.trainable = False\n",
    "        model.conv3_3.trainable = False\n",
    "        model.conv4_1.trainable = False\n",
    "        model.conv4_2.trainable = False\n",
    "        model.conv4_3.trainable = False\n",
    "        model.conv5_1.trainable = False\n",
    "        model.conv5_2.trainable = False\n",
    "        model.conv5_3.trainable = False       \n",
    "\n",
    "    losses = {'output_1' : loss_rpn_cls, 'output_2' : loss_rpn_reg}\n",
    "    lossWeights = {\"output_1\": 1.0, \"output_2\": 1.0}\n",
    "    \n",
    "    Optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9)\n",
    "    model.compile(optimizer=Optimizer, loss=losses, run_eagerly=True)\n",
    "\n",
    "    history = model.fit(x = np.asarray(image_list), y = {\"output_1\":cls_layer_label_list, \"output_2\":reg_layer_label_list}, batch_size = 1, epochs = EPOCH,verbose=1)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "source": [
    "## RPN 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "regularizer = tf.keras.regularizers.l2(0.0005)\n",
    "\n",
    "for layer in SharedConvNet.layers:\n",
    "    # 'kernel_regularizer' 속성이 있는 인스턴스를 찾아 regularizer를 추가\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        setattr(layer, 'kernel_regularizer', regularizer)\n",
    "\n",
    "RPN_Model = RPN(initializer, regularizer, SharedConvNet)\n",
    "RPN_Model.run_eagerly = True # 모델 내부 함수에서 쉽게 연산할 수 있게 설정"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['block3_conv1/kernel:0', 'block3_conv1/bias:0', 'block3_conv2/kernel:0', 'block3_conv2/bias:0', 'block3_conv3/kernel:0', 'block3_conv3/bias:0', 'block4_conv1/kernel:0', 'block4_conv1/bias:0', 'block4_conv2/kernel:0', 'block4_conv2/bias:0', 'block4_conv3/kernel:0', 'block4_conv3/bias:0', 'block5_conv1/kernel:0', 'block5_conv1/bias:0', 'block5_conv2/kernel:0', 'block5_conv2/bias:0', 'block5_conv3/kernel:0', 'block5_conv3/bias:0', 'rpn/intermediate_layer/kernel:0', 'rpn/intermediate_layer/bias:0', 'rpn/output_1/bias:0', 'rpn/output_2/bias:0'] when minimizing the loss.\n",
      "   1/5011 [..............................] - ETA: 40:23 - loss: 0.3367 - output_1_loss: 0.3047 - output_2_loss: 0.0306WARNING:tensorflow:Gradients do not exist for variables ['block3_conv1/kernel:0', 'block3_conv1/bias:0', 'block3_conv2/kernel:0', 'block3_conv2/bias:0', 'block3_conv3/kernel:0', 'block3_conv3/bias:0', 'block4_conv1/kernel:0', 'block4_conv1/bias:0', 'block4_conv2/kernel:0', 'block4_conv2/bias:0', 'block4_conv3/kernel:0', 'block4_conv3/bias:0', 'block5_conv1/kernel:0', 'block5_conv1/bias:0', 'block5_conv2/kernel:0', 'block5_conv2/bias:0', 'block5_conv3/kernel:0', 'block5_conv3/bias:0', 'rpn/intermediate_layer/kernel:0', 'rpn/intermediate_layer/bias:0', 'rpn/output_1/bias:0', 'rpn/output_2/bias:0'] when minimizing the loss.\n",
      "   2/5011 [..............................] - ETA: 38:53 - loss: 0.3368 - output_1_loss: 0.3041 - output_2_loss: 0.0314WARNING:tensorflow:Gradients do not exist for variables ['block3_conv1/kernel:0', 'block3_conv1/bias:0', 'block3_conv2/kernel:0', 'block3_conv2/bias:0', 'block3_conv3/kernel:0', 'block3_conv3/bias:0', 'block4_conv1/kernel:0', 'block4_conv1/bias:0', 'block4_conv2/kernel:0', 'block4_conv2/bias:0', 'block4_conv3/kernel:0', 'block4_conv3/bias:0', 'block5_conv1/kernel:0', 'block5_conv1/bias:0', 'block5_conv2/kernel:0', 'block5_conv2/bias:0', 'block5_conv3/kernel:0', 'block5_conv3/bias:0', 'rpn/intermediate_layer/kernel:0', 'rpn/intermediate_layer/bias:0', 'rpn/output_1/bias:0', 'rpn/output_2/bias:0'] when minimizing the loss.\n",
      "   3/5011 [..............................] - ETA: 40:41 - loss: 0.3362 - output_1_loss: 0.3035 - output_2_loss: 0.0313WARNING:tensorflow:Gradients do not exist for variables ['block3_conv1/kernel:0', 'block3_conv1/bias:0', 'block3_conv2/kernel:0', 'block3_conv2/bias:0', 'block3_conv3/kernel:0', 'block3_conv3/bias:0', 'block4_conv1/kernel:0', 'block4_conv1/bias:0', 'block4_conv2/kernel:0', 'block4_conv2/bias:0', 'block4_conv3/kernel:0', 'block4_conv3/bias:0', 'block5_conv1/kernel:0', 'block5_conv1/bias:0', 'block5_conv2/kernel:0', 'block5_conv2/bias:0', 'block5_conv3/kernel:0', 'block5_conv3/bias:0', 'rpn/intermediate_layer/kernel:0', 'rpn/intermediate_layer/bias:0', 'rpn/output_1/bias:0', 'rpn/output_2/bias:0'] when minimizing the loss.\n",
      "   4/5011 [..............................] - ETA: 41:22 - loss: 0.3362 - output_1_loss: 0.3032 - output_2_loss: 0.0317WARNING:tensorflow:Gradients do not exist for variables ['block3_conv1/kernel:0', 'block3_conv1/bias:0', 'block3_conv2/kernel:0', 'block3_conv2/bias:0', 'block3_conv3/kernel:0', 'block3_conv3/bias:0', 'block4_conv1/kernel:0', 'block4_conv1/bias:0', 'block4_conv2/kernel:0', 'block4_conv2/bias:0', 'block4_conv3/kernel:0', 'block4_conv3/bias:0', 'block5_conv1/kernel:0', 'block5_conv1/bias:0', 'block5_conv2/kernel:0', 'block5_conv2/bias:0', 'block5_conv3/kernel:0', 'block5_conv3/bias:0', 'rpn/intermediate_layer/kernel:0', 'rpn/intermediate_layer/bias:0', 'rpn/output_1/bias:0', 'rpn/output_2/bias:0'] when minimizing the loss.\n",
      "   5/5011 [..............................] - ETA: 41:41 - loss: 0.3367 - output_1_loss: 0.3027 - output_2_loss: 0.0328"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-da4a18805f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_minibatch_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRPN_Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_RPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRPN_Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_layer_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_layer_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraning_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-39490886992f>\u001b[0m in \u001b[0;36mtraining_RPN\u001b[0;34m(model, image_list, reg_layer_label_list, cls_layer_label_list, traning_step, EPOCH)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"output_1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcls_layer_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mreg_layer_label_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    797\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m       loss = self.compiled_loss(\n\u001b[0m\u001b[1;32m    756\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_dtype_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0mloss_metric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m    154\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    254\u001b[0m           y_pred, y_true)\n\u001b[1;32m    255\u001b[0m     \u001b[0mag_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/03/c86g8tkn02z_1s353z9pq6540000gn/T/tmp_gijipqr.py\u001b[0m in \u001b[0;36mtf__loss_rpn_reg\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Lreg_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Lreg_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_forLoss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLreg_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0m_py_for_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    471\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m       \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36mprotected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0moriginal_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprotected_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotected_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m       \u001b[0moriginal_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotected_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m       \u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/03/c86g8tkn02z_1s353z9pq6540000gn/T/tmp_gijipqr.py\u001b[0m in \u001b[0;36mloop_body_1\u001b[0;34m(itr_1)\u001b[0m\n\u001b[1;32m     80\u001b[0m                         \u001b[0;32mnonlocal\u001b[0m \u001b[0mLreg_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Lreg_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Lreg_sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_forLoss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLreg_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0m_tf_if_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morelse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m     \u001b[0m_py_if_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morelse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_py_if_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morelse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m   \u001b[0;34m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0morelse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/03/c86g8tkn02z_1s353z9pq6540000gn/T/tmp_gijipqr.py\u001b[0m in \u001b[0;36mif_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mif_body_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0;32mnonlocal\u001b[0m \u001b[0mLreg_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                         \u001b[0mLreg_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLreg_sum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss_Regression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_layer_output_label_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0melse_body_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m       \u001b[0m_attach_error_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/03/c86g8tkn02z_1s353z9pq6540000gn/T/tmp4kl1wbci.py\u001b[0m in \u001b[0;36mtf__Loss_Regression\u001b[0;34m(predict_box, anchor_box, groundTruth_box)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mt_y_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundTruth_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mt_w_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundTruth_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mt_h_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundTruth_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0msmooth_L1_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSmooth_L1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0msmooth_L1_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSmooth_L1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_y_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    376\u001b[0m         options=options)\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0minspect_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misbuiltin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mpy_builtins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_in_original_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller_fn_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\u001b[0m in \u001b[0;36misbuiltin\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misbuiltin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;34m\"\"\"Returns True if the argument is a built-in function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbuiltin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbuiltin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuiltinFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misbuiltin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;34m\"\"\"Returns True if the argument is a built-in function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbuiltin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbuiltin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuiltinFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_minibatch_index.index_list = np.array([])\n",
    "RPN_Model, history = training_RPN(RPN_Model, image_list, reg_layer_label_list, cls_layer_label_list, traning_step = 1, EPOCH = 2)"
   ]
  },
  {
   "source": [
    "### RPN에서 값 얻어내기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio):\n",
    "\n",
    "    for y in range(0, 14):\n",
    "        for x in range(0, 14):\n",
    "                for i in range(0, len(anchor_size)):\n",
    "                    for j in range(0, len(anchor_aspect_ratio)):\n",
    "                        # 오브젝트다 vs 아니다 2개 항목에 대한 스코어 저장(각 앵커당)\n",
    "                        index = 3 * i + j\n",
    "                        object_score_exp = np.exp(cls_layer_output[y][x][2 * index] * 1e32) # 출력값이 작아서 1e32를 구현하다\n",
    "                        non_object_score_exp = np.exp(cls_layer_output[y][x][2 * index + 1] * 1e32) \n",
    "\n",
    "                        # print(\"object_score_exp, non_object_score_exp : \", object_score_exp, non_object_score_exp, \"\\n\")\n",
    "\n",
    "                        exp_sum = object_score_exp + non_object_score_exp\n",
    "                        score = np.array([object_score_exp / exp_sum, non_object_score_exp / exp_sum])\n",
    "                        # print(score[0], score[1], \"\\n\")\n",
    "                        cls_layer_output[y][x][2*(3*i+j)] = score[0]\n",
    "                        cls_layer_output[y][x][2*(3*i+j) + 1] = score[1]\n",
    "                        \n",
    "                        center_x = 8 + 16 * x \n",
    "                        center_y = 8 + 16 * y\n",
    "                        w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                        h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                        # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                        reg_layer_output[y][x][4*(3*i+j)] = center_x + reg_layer_output[y][x][4*(3*i+j)]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 1] = center_y + reg_layer_output[y][x][4*(3*i+j) + 1]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 2] = w + reg_layer_output[y][x][4*(3*i+j) + 2]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 3] = h + reg_layer_output[y][x][4*(3*i+j) + 3]\n",
    "\n",
    "    cls_layer_output = np.reshape(cls_layer_output, (-1,2))\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (-1,4))\n",
    "\n",
    "    return cls_layer_output, reg_layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Output_RPN(RPN_Model, image_list) :\n",
    "    cls_layer_output_list = np.array([])\n",
    "    reg_layer_output_list = np.array([])\n",
    "\n",
    "    anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "    anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준\n",
    "\n",
    "    for i in tqdm(range(0, len(image_list)), desc = \"get_output\"):\n",
    "        cls_layer_output, reg_layer_output = RPN_Model(np.expand_dims(image_list[i], axis=0))\n",
    "\n",
    "        cls_layer_output = cls_layer_output[0].numpy()\n",
    "        reg_layer_output = reg_layer_output[0].numpy()\n",
    "\n",
    "        # 가공하자\n",
    "        cls_layer_output, reg_layer_output = Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio)\n",
    "\n",
    "        cls_layer_output_list = np.append(cls_layer_output_list, cls_layer_output)\n",
    "        reg_layer_output_list = np.append(reg_layer_output_list, reg_layer_output)\n",
    "    \n",
    "    cls_layer_output_list = np.reshape(cls_layer_output_list, (-1,1764,2))\n",
    "    reg_layer_output_list = np.reshape(reg_layer_output_list, (-1,1764,4))\n",
    "\n",
    "    return cls_layer_output_list, reg_layer_output_list"
   ]
  },
  {
   "source": [
    "cls_layer_output_list, reg_layer_output_list = Get_Output_RPN(RPN_Model, image_list)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get_output:   5%|▍         | 230/5011 [00:40<13:59,  5.69it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9a3cd2b115e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcls_layer_output_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_layer_output_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGet_Output_RPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRPN_Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-32cc7ba10dea>\u001b[0m in \u001b[0;36mGet_Output_RPN\u001b[0;34m(RPN_Model, image_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# 가공하자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcls_layer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_layer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_aspect_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcls_layer_output_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_layer_output_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_layer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-de608dd97d01>\u001b[0m in \u001b[0;36mProcess_output\u001b[0;34m(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                         \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_layer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.004617825295358861"
      ]
     },
     "metadata": {},
     "execution_count": 219
    }
   ],
   "source": [
    "np.exp(-5.3778314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 8.,  8., 32., 32.])"
      ]
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "reg_layer_output_list[0][0]"
   ]
  },
  {
   "source": [
    "#### 일단 RoI를 특성맵에 맞게 변환시켜서 표시해보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fast R-CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 입력값 : 공유 특성맵 + RoI\n",
    "### 출력값 : 예측한 객체 종류, 객체의 좌표(r, c, h, w). (r,c)는 왼쪽위 좌표고 h,w는 너비, 높이"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detector 구조\n",
    "# 입력 이미지 -> 특성맵.\n",
    "# 특성맵 + RoI -> RoI위치만 추출. 그러면 나한테 있는 데이터는 RoI 위치별 특성맵, 각 RoI의 Truth Box, Classes\n",
    "# Roi 특성맵을 연산을 통해 cls, loc 휙득\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(cls_layer_output, reg_layer_output, reg_layer_label, index_classes): # 한 이미지에서 뽑아낸 RPN의 출력값 2개, 이미지에 있는 모든 앵커들이 참조한 Ground Truth Box, 그 Ground Truth Box의 Class\n",
    "    nms_RoI_List = [] # 각 이미지마다 RoI 갯수가 다르다. 그래서 리스트로 저장한다. 리스트 안에 리스트 저장하는 방식으로 len = 5011인 리스트를 생성할거다 \n",
    "    nms_GroundTruthBox_List = [] # NMS에 들어간 RoI가 참조한 Ground Truth Box \n",
    "    nms_Classes_List = []\n",
    "    for i in range(0, len(cls_layer_output)) :\n",
    "        if cls_layer_output[i][0] > 0.7 : # 해당 앵커의 object score가 0.7을 넘겼으면\n",
    "            # 저장\n",
    "            nms_RoI_List.append(reg_layer_output[i]) # RoI\n",
    "            nms_GroundTruthBox_List.append(reg_layer_output_list[i]) # Ground Truth Box\n",
    "            # 그 Box가 가리키는 클래스 인덱스\n",
    "            nms_Classes_List.append(index_classes[i]\n",
    "\n",
    "\n",
    "\n",
    "    return nms_RoI_List, nms_GroundTruthBox_List, nms_Classes_List # 선별한 애들을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nms_list(cls_layer_output_list, reg_layer_output_list, reg_layer_label_list, index_classes_list) :\n",
    "    # RPN output 2개, 이미지에서 각 앵커(1764)들이 어떤 Ground Truth Box보고 IoU 계산했는지, 이미지에서 각 앵커(1764)들이 어떤 '클래스'의 Ground Truth Box보고 IoU 계산했는지\n",
    "    # Detector 훈련에 필요한 데이터를 얻는 곳이다\n",
    "    \n",
    "    NMS_RoIs_List = [] # 전체 입력 이미지의 RoI를 이미지별로 저장(리스트 안에 리스트)\n",
    "    NMS_GroundTruthBoxes_List = []\n",
    "    NMS_Classes_List = []\n",
    "\n",
    "    for i in tqdm(range(0, len(cls_layer_output_list)), desc = \"get_RoI\"): # 5011개에 대한 nms 구한다\n",
    "        nms_RoI_List, nms_GroundTruthBox_List, nms_Classes_List = nms(cls_layer_output_list[i], reg_layer_output_list[i], reg_layer_label_list[i], index_classes_list[i])\n",
    "        NMS_RoIs_List.append(nms_RoI_List)\n",
    "        NMS_GroundTruthBoxes_List.append(NMS_GroundTruthBoxes_List)\n",
    "        NMS_Classes_List.append(nms_Classes_List)\n",
    "\n",
    "    return NMS_RoIs_List, NMS_GroundTruthBoxes_List, NMS_Classes_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_RoI(nms_RoI_List) :\n",
    "    # RoI_list : (-1, 4) <- (x,y,w,h)\n",
    "    RoI_list_forDetector = np.array([])\n",
    "    for i in range(0, len(RoI_list)) :\n",
    "        r = RoI_list[i][0] - (RoI_list[i][2]/2)\n",
    "        c = RoI_list[i][1] - (RoI_list[i][3]/2)\n",
    "        w = RoI_list[i][2]\n",
    "        h = RoI_list[i][3]\n",
    "\n",
    "        roi_forDetector = np.array([r,c,w,h])\n",
    "        RoI_list_forDetector = np.append(RoI_list_forDetector, roi_forDetector)\n",
    "    RoI_list_forDetector = np.reshape(RoI_list_forDetector, (-1,4)) \n",
    "\n",
    "    return RoI_list_forDetector"
   ]
  },
  {
   "source": [
    "### (r,c,w,h)는 RoI Pooling Layer에만 쓰인다.\n",
    "### Detector에는 (x,y,w,h)가 쓰이는데 이 때 데이터는 따로 만들면 된다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Detector 훈련에 필요한 데이터셋 휙득"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMS_RoIs_List, NMS_GroundTruthBoxes_List, NMS_Classes_List = get_nms_list(cls_layer_output_list, reg_layer_output_list, reg_layer_label_list, index_classes_list)"
   ]
  },
  {
   "source": [
    "## Detector 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RoI Pooling Layer 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 14*14를 7*7로 Pooling할건데 Pooling 영역이 RoI임\n",
    "### RoI는 (r,c,h,w)를 받는데 각(왼쪽 위 x, 왼쪽 위 y, RoI의 h, RoI의 w)다.\n",
    "### 만약 pooling Layer가 2*2면 2/7*2/7로 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIPoolingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_size = 7): # 레이어에 필요한 매개변수 받음\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.outputsize = 7\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_variable(\"kernel\", shape=[int(input_shape[-1], self.num_outputs)])\n",
    "\n",
    "    def call(self, input, RoI): # 정방향 연산 진행\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(Detector, self).__init__(name='rpn')\n",
    "    # 레이어 \n",
    "    # 공용 레이어\n",
    "    self.conv1_1 = SharedConvNet.layers[0]\n",
    "    self.conv1_2 = SharedConvNet.layers[1]\n",
    "    self.pooling_1 = SharedConvNet.layers[2]\n",
    "    self.conv2_1 = SharedConvNet.layers[3]\n",
    "    self.conv2_2 = SharedConvNet.layers[4]\n",
    "    self.pooling_2 = SharedConvNet.layers[5]\n",
    "    self.conv3_1 = SharedConvNet.layers[6]\n",
    "    self.conv3_2 = SharedConvNet.layers[7]\n",
    "    self.conv3_3 = SharedConvNet.layers[8]\n",
    "    self.pooling_3 = SharedConvNet.layers[9]\n",
    "    self.conv4_1 = SharedConvNet.layers[10]\n",
    "    self.conv4_2 = SharedConvNet.layers[11]\n",
    "    self.conv4_3 = SharedConvNet.layers[12]\n",
    "    self.pooling_4 = SharedConvNet.layers[13]\n",
    "    self.conv5_1 = SharedConvNet.layers[14]\n",
    "    self.conv5_2 = SharedConvNet.layers[15]\n",
    "    self.conv5_3 = SharedConvNet.layers[16]\n",
    "\n",
    "    Classify_layer_initializer tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "    Box_regression_layer_initializer tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None)\n",
    "\n",
    "    # RoI Pooling : H*W(7*7)에 맞게 입력 특성맵을 pooling. RoI에 해당하는 영역을 7*7로 Pooling한다. \n",
    "    self.RoIPoolingLayer = tf.keras.layers # 7*7로 만들어버린다. \n",
    "    self.Flatten_layer = tf.keras.layers.Flatten() # 여기서 4096개가 되어야한다.\n",
    "    self.Classify_layer = tf.keras.layers.Dense(21, activation='softmax', kernel_initializer = Classify_layer_initializer, name = \"output_1\")\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(84, activation= None, kernel_initializer = Box_regression_layer_initializer, name = \"output_2\")\n",
    "    \n",
    "  def call(self, inputs): # input으로 리스트를 넣으면 된다.\n",
    "\n",
    "    Input_Image = inputs[0]\n",
    "    RoI_List = inputs[1]\n",
    "\n",
    "    # 정방향 연산\n",
    "    output1_1 = self.conv1_1(Input_Image)\n",
    "    output1_2 = self.conv1_2(output1_1)\n",
    "    output1_pooling = self.pooling_1(output1_2)\n",
    "    output2_1 = self.conv2_1(output1_pooling)\n",
    "    output2_2 = self.conv2_2(output2_1)\n",
    "    output2_pooling = self.pooling_2(output2_2)\n",
    "    output3_1 = self.conv3_1(output2_pooling)\n",
    "    output3_2 = self.conv3_2(output3_1)\n",
    "    output3_3 = self.conv3_3(output3_2)\n",
    "    output3_pooling = self.pooling_3(output3_3)\n",
    "    output4_1 = self.conv4_1(output3_pooling)\n",
    "    output4_2 = self.conv4_2(output4_1)\n",
    "    output4_3 = self.conv4_3(output4_2)\n",
    "    output4_pooling = self.pooling_4(output4_3)\n",
    "    output5_1 = self.conv5_1(output4_pooling)\n",
    "    output5_2 = self.conv5_2(output5_1)\n",
    "    output5_3 = self.conv5_3(output5_2)\n",
    "    # Detector\n",
    "    # Pooling\n",
    "\n",
    "    Classify_layer_output = self.Classify_layer()\n",
    "    reg_layer_output = self.reg_layer()\n",
    "\n",
    "    return Classify_layer_output, reg_layer_output # 두 아웃풋을 내놓는다. "
   ]
  },
  {
   "source": [
    "## Loss 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_detector_loc(y_true, y_pred):\n",
    "    # y_true : 해당 RoI에 있는 클래스의 '진짜' 좌표\n",
    "    # y_pred : 해당 영역에 존재하는 클래스별 좌표\n",
    "    # 해당 영역에 있는 클래스에 대한 Bounding Box에 대해서만 Loss 계산\n",
    "    \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "Detector_Model = Detector(SharedConvNet)\n",
    "RPN_Model.run_eagerly = True # 모델 내부 함수에서 쉽게 연산할 수 있게 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Detector(model, , traning_step, EPOCH):\n",
    "\n",
    "    losses = {'output_1' : 'categorical_crossentropy', 'output_2' : loss_detector_loc} # classify layer는 20 + 1가지 클래스에 대한 softmax, reg_layer는 따로 만든 loss함수\n",
    "\n",
    "    model.compile(optimizer=Optimizer, loss=losses, run_eagerly=True)\n",
    "\n",
    "    history = model.fit(np.asarray(image_list), [cls_layer_label_list, reg_layer_label_list], batch_size = 1, epochs = EPOCH)\n",
    "\n",
    "    return model, history\n"
   ]
  }
 ]
}
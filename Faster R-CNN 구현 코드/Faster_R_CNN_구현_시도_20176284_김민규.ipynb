{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dec8a5ff48d192d3613b9e6713a72fe103c8e5d5c44dc7b8ee41f3376e4a4f37",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Faster R-CNN 구현\n",
    "### 텐서플로우 기반\n",
    "### 주석은 한글 위주로 작성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import logging\n"
   ]
  },
  {
   "source": [
    "## 훈련 이미지 가져오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/JPEGImages'\n",
    "train_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/Annotations'\n",
    "\n",
    "test_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/JPEGImages'\n",
    "test_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5011\n4952\n"
     ]
    }
   ],
   "source": [
    "list_train_x = sorted([x for x in glob.glob(train_x_path + '/**')])    \n",
    "list_train_y = sorted([x for x in glob.glob(train_y_path + '/**')]) \n",
    "\n",
    "list_test_x = sorted([x for x in glob.glob(test_x_path + '/**')])    \n",
    "list_test_y = sorted([x for x in glob.glob(test_y_path + '/**')]) \n",
    "\n",
    "print(len(list_train_x))\n",
    "print(len(list_test_x))"
   ]
  },
  {
   "source": [
    "## 훈련용 데이터로 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력용 이미지 생성. 224, 224로 변환시키고 채널 값(0~255)를 0~1 사이의 값으로 정규화 시켜줌\n",
    "def make_input(image_file_list): \n",
    "    images_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_file_list)), desc=\"get image\") :\n",
    "        \n",
    "        image = cv2.imread(image_file_list[i])\n",
    "        image = cv2.resize(image, (224, 224))/255\n",
    "        \n",
    "        images_list.append(image)\n",
    "    \n",
    "    return np.asarray(images_list)"
   ]
  },
  {
   "source": [
    "## RPN 훈련을 위한 데이터셋 생성에 필요한 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지에 어떤 Ground Truth Box가 있는지\n",
    "def get_Ground_Truth_Box_fromImage(xml_file_path): # xml_file_path은 파일 하나의 경로를 나타낸다\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    xml_file = xmltodict.parse(f.read()) \n",
    "\n",
    "    # 우선 원래 이미지 크기를 얻는다. 왜냐하면 앵커는 224*224 기준으로 만들었는데 원본 이미지는 224*224가 아니기 때문.\n",
    "    # 224*224에 맞게 줄일려고 하는거다\n",
    "    Image_Height = float(xml_file['annotation']['size']['height'])\n",
    "    Image_Width  = float(xml_file['annotation']['size']['width'])\n",
    "\n",
    "    Ground_Truth_Box_list = [] \n",
    "\n",
    "    # multi-objects in image\n",
    "    try:\n",
    "        for obj in xml_file['annotation']['object']:\n",
    "            \n",
    "            # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "            x_min = float(obj['bndbox']['xmin']) \n",
    "            y_min = float(obj['bndbox']['ymin'])\n",
    "            x_max = float(obj['bndbox']['xmax']) \n",
    "            y_max = float(obj['bndbox']['ymax'])\n",
    "\n",
    "            # 224*224에 맞게 변형시켜줌\n",
    "            x_min = float((224/Image_Width)*x_min)\n",
    "            y_min = float((224/Image_Height)*y_min)\n",
    "            x_max = float((224/Image_Width)*x_max)\n",
    "            y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "            Ground_Truth_Box = [x_min, y_min, x_max, y_max]\n",
    "            Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    # single-object in image\n",
    "    except TypeError as e : \n",
    "        # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "        x_min = float(xml_file['annotation']['object']['bndbox']['xmin']) \n",
    "        y_min = float(xml_file['annotation']['object']['bndbox']['ymin']) \n",
    "        x_max = float(xml_file['annotation']['object']['bndbox']['xmax']) \n",
    "        y_max = float(xml_file['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "        # 224*224에 맞게 변형시켜줌\n",
    "        x_min = float((224/Image_Width)*x_min)\n",
    "        y_min = float((224/Image_Height)*y_min)\n",
    "        x_max = float((224/Image_Width)*x_max)\n",
    "        y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "        Ground_Truth_Box = [x_min, y_min, x_max, y_max]  \n",
    "        Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    \n",
    "    Ground_Truth_Box_list = np.asarray(Ground_Truth_Box_list)\n",
    "    Ground_Truth_Box_list = np.reshape(Ground_Truth_Box_list, (-1, 4))\n",
    "\n",
    "    return Ground_Truth_Box_list # 이미지에 있는 Ground Truth Box 리스트 받기(numpy)\n"
   ]
  },
  {
   "source": [
    "## RPN - 앵커 준비\n",
    "#### 입력 이미지 기준으로 앵커를 생성한다.\n",
    "#### 풀링을 3번 하므로 2^3 = 8이니 (8, 8)부터 (16, 8)...등 8픽셀식 중심 좌표를 옮겨가며 앵커들을 k개씩 생성한다\n",
    "#### 생성한 앵커들 중 사용할 가치가 있는 앵커를 걸러낸다(이미지 범위를 벗어나지 않는 앵커들만 선정)\n",
    "#### 선정한 앵커 중 실제 물체의 box와 얼마나 곂치는지(IoU) 계산해본다. 확실히 겹친다 하는 애들을 Positive 앵커로, 거의 안겹친다 하는 애들은 Negataive 앵커로 선정한다. 애매한 애들은 거른다.\n",
    "#### 이렇게 생성된 앵커들로 미니배치를 생성한다. Positive 128개, Negative 128개로 만드는게 ideal한 구성이긴 한데 Positive한 앵커가 별로 없다. 그래서 Positive를 128개 못채웠으면 Negataive 앵커로 채워준다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 생성 함수. \n",
    "def make_anchor(anchor_size, anchor_aspect_ratio) :\n",
    "    # 입력 이미지(그래봤자 224*224긴 하지만)에 맞춰 앵커를 생성해보자 \n",
    "\n",
    "    anchors = [] # [x,y,w,h]로 이루어진 리스트 \n",
    "    anchors_state = [] # 이 앵커를 훈련에 쓸건가? 각 앵커별로 사용 여부를 나타낸다. \n",
    "\n",
    "    # 앵커 중심좌표 간격\n",
    "    interval_x = 16\n",
    "    interval_y = 16\n",
    "    Center_max_x = 208 # 224 - 16, 중심좌표가 224가 될 수는 없다.\n",
    "    Center_max_y = 208 # 224 - 16\n",
    "\n",
    "    # 2단 while문 생성\n",
    "    x = 8\n",
    "    y = 8\n",
    "    index_count = 0\n",
    "    while(y <= 224): # 8~208 = 14개 \n",
    "        while(x <= 224): # 8~208 = 14개 \n",
    "            # k개의 앵커 생성. 여기서 k = len(anchor_size) * len(anchor_aspect_ratio)다\n",
    "            for i in range(0, len(anchor_size)) : \n",
    "                for j in range(0, len(anchor_aspect_ratio)) :\n",
    "                    anchor_width = anchor_aspect_ratio[j][0] * anchor_size[i]\n",
    "                    anchor_height = anchor_aspect_ratio[j][1] * anchor_size[i]\n",
    "\n",
    "                    anchor = [x, y, anchor_width, anchor_height]\n",
    "                    anchors.append(anchor)\n",
    "                    # 앵커가 이미지 경계선을 넘나드나? 필터링\n",
    "                    if((x - (anchor_width/2) >= 0) and (y - (anchor_height/2) >= 0) and\n",
    "                    (x + (anchor_width/2) <= 224) and (y + (anchor_height/2) <= 224)):\n",
    "                        # 경계 안에 있으면 1\n",
    "                        anchors_state.append(int(1))\n",
    "                    else :\n",
    "                        anchors_state.append(int(0))\n",
    "            x = x + interval_x \n",
    "        y = y + interval_y\n",
    "        x = 8\n",
    "    return np.asarray(anchors), np.asarray(anchors_state) # 넘파이로 반환"
   ]
  },
  {
   "source": [
    "### IoU 계산 후 Positive, Negative 앵커 분류"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커들을 Positive, Negative 앵커로 나누고 각 앵커가 참고한 Ground Truth Box와 Class를 반환하자\n",
    "# RPN에는 '어떤 클래스인가?'는 알 필요가 없다. '객체인가 아닌가'이거 하나만 필요할 뿐. \n",
    "def align_anchor(anchors, anchors_state, Ground_Truth_Box_list):\n",
    "\n",
    "    # 각 앵커는 해당 위치에서 구한 여러가지 Ground truth Box와의 ioU 중 제일 높은거만 가져온다. \n",
    "    IoU_List = np.array([])\n",
    "    Ground_truth_box_Highest_IoU_List = [] # 각 앵커가 어떤 Ground Truth Box를 보고 IoU를 계산했는가?\n",
    "\n",
    "    #start = time.time()\n",
    "\n",
    "    for i in range(0, len(anchors)):\n",
    "        if anchors_state[i] == 0 :\n",
    "            IoU_List = np.append(IoU_List, 0)\n",
    "            Ground_truth_box_Highest_IoU_List.append([0,0,0,0])\n",
    "\n",
    "            if i % 9 == 8 :\n",
    "                IoU_List_inOneSpot = IoU_List[i-8:i+1]\n",
    "                for num in list(range(i-8, i + 1)):\n",
    "                    if IoU_List[num] > 0.7 or (max(IoU_List_inOneSpot) == IoU_List[num] and IoU_List[num] >= 0.3): # positive anchor\n",
    "                        anchors_state[num] = 2\n",
    "                    elif IoU_List[num] < 0.3 : # negative anchor\n",
    "                        anchors_state[num] = 1\n",
    "                    else: # 애매한 앵커들\n",
    "                        anchors_state[num] = 0    \n",
    "        else:\n",
    "            anchor_minX = anchors[i][0] - (anchors[i][2]/2)\n",
    "            anchor_minY = anchors[i][1] - (anchors[i][3]/2)\n",
    "            anchor_maxX = anchors[i][0] + (anchors[i][2]/2)\n",
    "            anchor_maxY = anchors[i][1] + (anchors[i][3]/2)\n",
    "\n",
    "            anchor = [anchor_minX, anchor_minY, anchor_maxX, anchor_maxY]\n",
    "\n",
    "            # 연산 속도 때문에 Box대신 Ground Truth Box의 인덱스를 저장\n",
    "            IoU_max = 0\n",
    "            ground_truth_box_Highest_IoU = [0,0,0,0]\n",
    "\n",
    "            for j in range(0, len(Ground_Truth_Box_list)):\n",
    "\n",
    "                ground_truth_box = Ground_Truth_Box_list[j]\n",
    "\n",
    "                InterSection_min_x = max(anchor[0], ground_truth_box[0])\n",
    "                InterSection_min_y = max(anchor[1], ground_truth_box[1])\n",
    "\n",
    "                InterSection_max_x = min(anchor[2], ground_truth_box[2])\n",
    "                InterSection_max_y = min(anchor[3], ground_truth_box[3])\n",
    "\n",
    "                InterSection_Area = 0\n",
    "\n",
    "                if (InterSection_max_x - InterSection_min_x + 1) >= 0 and (InterSection_max_y - InterSection_min_y + 1) >= 0 :\n",
    "                    InterSection_Area = (InterSection_max_x - InterSection_min_x + 1) * (InterSection_max_y - InterSection_min_y + 1)\n",
    "\n",
    "                box1_area = (anchor[2] - anchor[0]) * (anchor[3] - anchor[1])\n",
    "                box2_area = (ground_truth_box[2] - ground_truth_box[0]) * (ground_truth_box[3] - ground_truth_box[1])\n",
    "                Union_Area = box1_area + box2_area - InterSection_Area\n",
    "\n",
    "                IoU = (InterSection_Area/Union_Area)\n",
    "                if IoU > IoU_max :\n",
    "                    IoU_max = IoU\n",
    "                    ground_truth_box_Highest_IoU = ground_truth_box\n",
    "\n",
    "            IoU_List = np.append(IoU_List, IoU_max)\n",
    "            Ground_truth_box_Highest_IoU_List.append(ground_truth_box_Highest_IoU)\n",
    "\n",
    "            # 한 위치에 9개의 앵커 존재 -> 9개 앵커에 대한 IoU를 계산할 때마다 모아서 Positive, Negative 앵커 분류\n",
    "            if i % 9 == 8 :\n",
    "                IoU_List_inOneSpot = IoU_List[i-8:i+1]\n",
    "                for num in list(range(i-8, i + 1)):\n",
    "                    if IoU_List[num] > 0.7 or (max(IoU_List_inOneSpot) == IoU_List[num] and IoU_List[num] >= 0.3): # positive anchor\n",
    "                        anchors_state[num] = 2\n",
    "                    elif IoU_List[num] < 0.3 : # negative anchor\n",
    "                        anchors_state[num] = 1\n",
    "                    else: # 애매한 앵커들\n",
    "                        anchors_state[num] = 0     \n",
    "\n",
    "    Ground_truth_box_Highest_IoU_List = np.asarray(Ground_truth_box_Highest_IoU_List)\n",
    "    Ground_truth_box_Highest_IoU_List = np.reshape(Ground_truth_box_Highest_IoU_List, (-1, 4))\n",
    "            \n",
    "    return anchors_state, Ground_truth_box_Highest_IoU_List # 각 앵커의 상태, (모든)앵커가 IoU 계산에 참조한 Ground Truth Box"
   ]
  },
  {
   "source": [
    "## RPN을 위한 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPN훈련을 위한 데이터셋. \n",
    "# RPN과 Detector는 별개의 모델이다. 즉, 두 모델을 훈련시킬 때 필요한 데이터셋은 따로따로 만들어야한다. \n",
    "# 여기선 RPN 훈련에 필요한 데이터만 만들거다. \n",
    "def make_dataset_forRPN(input_list) :\n",
    "    image_file_list = input_list[0]\n",
    "    xml_file_list = input_list[1]\n",
    "    anchors = input_list[2]\n",
    "    anchors_state = input_list[3]\n",
    "\n",
    "    image_list = make_input(image_file_list) # 입력\n",
    "    # 출력\n",
    "    cls_layer_label_list = np.array([])\n",
    "    reg_layer_label_list = np.array([])\n",
    "\n",
    "    # 값 계속 생성하는거 막기위한 변수\n",
    "    cls_label_forPositive = np.array([1.0,0.0])\n",
    "    cls_label_forNegative = np.array([0.0,1.0])\n",
    "    cls_label_forUseless  = np.array([0.0,0.0])\n",
    "\n",
    "    reg_label_forNotPositive = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for i in tqdm(range(0, len(xml_file_list)), desc=\"get label\"): # 각 이미지별로 데이터셋 생성(5011개)\n",
    "\n",
    "        anchors_state_for = anchors_state # anchors_state는 매 사진마다 다르니까 원본값(?)을 복사해서 쓴다. \n",
    "        Ground_Truth_Box_list = get_Ground_Truth_Box_fromImage(xml_file_list[i]) # 여기서는 Ground Truth Box에 대한 정보만 필요하다\n",
    "        anchors_state_for, Ground_truth_box_Highest_IoU_List = align_anchor(anchors, anchors_state_for, Ground_Truth_Box_list)\n",
    "        # 어떤 앵커가 Pos, neg 앵커인지, (모든)앵커가 참조한 ground truth box는 뭔지\n",
    "    \n",
    "        #start = time.time()\n",
    "        # 연산 시간 때문에 생성(1761개치 모아놨다가 한 번에 추가하기)\n",
    "        cls_layer_label_list_for = np.array([])\n",
    "        reg_layer_label_list_for = np.array([])\n",
    "        for j in range(0, len(anchors_state_for)) :\n",
    "            if anchors_state_for[j] == 2 : # positive\n",
    "                Ground_truth_box = np.array([Ground_truth_box_Highest_IoU_List[j][0] + Ground_truth_box_Highest_IoU_List[j][2]/2, Ground_truth_box_Highest_IoU_List[j][1] + Ground_truth_box_Highest_IoU_List[j][2]/2, Ground_truth_box_Highest_IoU_List[j][2] - Ground_truth_box_Highest_IoU_List[j][0], Ground_truth_box_Highest_IoU_List[j][3] - Ground_truth_box_Highest_IoU_List[j][1]])\n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forPositive)\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, Ground_truth_box) # IoU계산에 참조한(pos, neg 분류에 기여한) Ground Truth Box의 정보 휙득\n",
    "            elif anchors_state_for[j] == 1 : # negative는 Ground Truth Box 정보가 필요없으니 [0,0,0,0]을 넣는다. \n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forNegative) # 해당 앵커 output이 [0,1] -> negative\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, reg_label_forNotPositive)\n",
    "            else : \n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forUseless) # 해당 앵커 output이 [0.5, 0.5] -> 무의미한 값\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, reg_label_forNotPositive)\n",
    "        \n",
    "        # 넘파이 배열로 변환\n",
    "        cls_layer_label_list = np.append(cls_layer_label_list, cls_layer_label_list_for)\n",
    "        reg_layer_label_list = np.append(reg_layer_label_list, reg_layer_label_list_for)\n",
    "        #print(\"\\nmaking label time :\", time.time() - start)\n",
    "\n",
    "    # 논문에서 말한 출력값 크기에 맞게 reshape\n",
    "    cls_layer_label_list = np.reshape(cls_layer_label_list, (-1, 1764, 2)) \n",
    "    reg_layer_label_list = np.reshape(reg_layer_label_list, (-1, 1764, 4))\n",
    "\n",
    "    image_list = image_list.astype('float32')\n",
    "\n",
    "    return image_list, cls_layer_label_list, reg_layer_label_list # 훈련 데이터들(입, 출력)"
   ]
  },
  {
   "source": [
    "## RPN에 쓰일 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get image: 100%|██████████| 5011/5011 [00:21<00:00, 236.59it/s]\n",
      "get label: 100%|██████████| 5011/5011 [18:03<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# 파일 리스트\n",
    "image_file_list = sorted([x for x in glob.glob(train_x_path + '/**')])\n",
    "xml_file_list = sorted([x for x in glob.glob(train_y_path + '/**')])\n",
    "\n",
    "anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준 \n",
    "anchors, anchors_state = make_anchor(anchor_size, anchor_aspect_ratio) # 앵커 생성 + 유효한 앵커 인덱스 휙득\n",
    "\n",
    "image_list, cls_layer_label_list, reg_layer_label_list = make_dataset_forRPN([image_file_list, xml_file_list, anchors, anchors_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'repeat'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3609e3e8f980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'repeat'"
     ]
    }
   ],
   "source": [
    "image = tf.constant([[\n",
    " [1,2],\n",
    " [3,4],\n",
    "]])\n",
    "test = tf.repeat(image, repeats=[7,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: id=17, shape=(1, 2, 2), dtype=int32, numpy=\n",
       "array([[[1, 2],\n",
       "        [3, 4]]], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cls_layer_label_list' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aca684d9166e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcls_layer_label_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cls_layer_label_list' is not defined"
     ]
    }
   ],
   "source": [
    "cls_layer_label_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image_list.shape :  (5011, 224, 224, 3)\ncls_layer_label_list.shape :  (5011, 1764, 2)\nreg_layer_label_list.shape :  (5011, 1764, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"image_list.shape : \", image_list.shape)\n",
    "print(\"cls_layer_label_list.shape : \", cls_layer_label_list.shape)\n",
    "print(\"reg_layer_label_list.shape : \", reg_layer_label_list.shape)"
   ]
  },
  {
   "source": [
    "## RPN 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(tf.keras.Model):\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(RPN, self).__init__(name='rpn')\n",
    "\n",
    "    self.anchors = anchors\n",
    "    self.anchor_size = anchor_size\n",
    "    self.anchor_aspect_ratio = anchor_aspect_ratio\n",
    "    self.Optimizers = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9)\n",
    "\n",
    "    # 공용 레이어\n",
    "    self.conv1_1 = SharedConvNet.layers[0]\n",
    "    self.conv1_2 = SharedConvNet.layers[1]\n",
    "    self.pooling_1 = SharedConvNet.layers[2]\n",
    "\n",
    "    self.conv2_1 = SharedConvNet.layers[3]\n",
    "    self.conv2_2 = SharedConvNet.layers[4]\n",
    "    self.pooling_2 = SharedConvNet.layers[5]\n",
    "\n",
    "    self.conv3_1 = SharedConvNet.layers[6]\n",
    "    self.conv3_2 = SharedConvNet.layers[7]\n",
    "    self.conv3_3 = SharedConvNet.layers[8]\n",
    "    self.pooling_3 = SharedConvNet.layers[9]\n",
    "\n",
    "    self.conv4_1 = SharedConvNet.layers[10]\n",
    "    self.conv4_2 = SharedConvNet.layers[11]\n",
    "    self.conv4_3 = SharedConvNet.layers[12]\n",
    "    self.pooling_4 = SharedConvNet.layers[13]\n",
    "\n",
    "    self.conv5_1 = SharedConvNet.layers[14]\n",
    "    self.conv5_2 = SharedConvNet.layers[15]\n",
    "    self.conv5_3 = SharedConvNet.layers[16]\n",
    "    \n",
    "    # RPN만의 레이어\n",
    "    self.intermediate_layer = tf.keras.layers.Conv2D(512, (3, 3), padding = 'SAME' , activation = 'relu', name = \"intermediate_layer\", dtype='float32')\n",
    "    self.cls_Layer = tf.keras.layers.Conv2D(18, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_1\", dtype='float32')\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(36, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_2\", dtype='float32')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    # 정방향 연산\n",
    "    # inputs = np.array(inputs)\n",
    "    output = self.conv1_1(inputs)\n",
    "    output = self.conv1_2(output)\n",
    "    output = self.pooling_1(output)\n",
    "\n",
    "    output = self.conv2_1(output)\n",
    "    output = self.conv2_2(output)\n",
    "    output = self.pooling_2(output)\n",
    "\n",
    "    output = self.conv3_1(output)\n",
    "    output = self.conv3_2(output)\n",
    "    output = self.conv3_3(output)\n",
    "    output = self.pooling_3(output)\n",
    "\n",
    "    output = self.conv4_1(output)\n",
    "    output = self.conv4_2(output)\n",
    "    output = self.conv4_3(output)\n",
    "    output = self.pooling_4(output)\n",
    "\n",
    "    output = self.conv5_1(output)\n",
    "    output = self.conv5_2(output)\n",
    "    shared_output = self.conv5_3(output)\n",
    "    # RPN\n",
    "    shared_output = self.intermediate_layer(shared_output)\n",
    "    cls_Layer_output = self.cls_Layer(shared_output)\n",
    "    reg_Layer_output = self.reg_layer(shared_output) \n",
    "\n",
    "    return cls_Layer_output, reg_Layer_output # (1,14,14,18), (1,14,14,36) 반환\n",
    "\n",
    "  def get_minibatch_index(self, cls_layer_output): # 라벨이니까 (1764,2) 넘파이 온다\n",
    "\n",
    "    index_list = np.array([])\n",
    "      \n",
    "    index_list = np.zeros(1764) # 각 앵커가 미니배치 뽑혔나 안뽑혔나\n",
    "    index_pos = np.array([])\n",
    "    index_neg = np.array([])\n",
    "    # cls_layer_output을 보고 긍정, 부정 앵커 분류. 그렇게 데이터셋을 구성함\n",
    "    for i in range(0, 1764):\n",
    "        if cls_layer_output[i][0] == 1.0 : # positive anchor\n",
    "            index_pos = np.append(index_pos, i)\n",
    "        elif cls_layer_output[i][0] == 0.0 : # negative anchor\n",
    "            index_neg = np.append(index_neg, i)\n",
    "\n",
    "    max_for = min([128, len(index_pos)])\n",
    "    ran_list = random.sample(range(0, len(index_pos)), max_for)\n",
    "\n",
    "    for i in range(0, len(ran_list)) :\n",
    "        index = int(index_pos[ran_list[i]])\n",
    "        index_list[index] = 1\n",
    "\n",
    "    ran_list = random.sample(range(0, len(index_neg)), 256 - max_for) # 랜덤성 증가?를 위해 또다시 난수 생성\n",
    "    for i in range(0, len(ran_list)) :\n",
    "        index = int(index_neg[ran_list[i]])\n",
    "        index_list[index] = 1\n",
    "\n",
    "    return index_list # (1764,1) <- 1,0으로 이루어진 boolean 넘파이 배열\n",
    "\n",
    "  # multi task loss\n",
    "  def multi_task_loss(self ,image ,cls_layer_output_label, reg_layer_output_label):\n",
    "\n",
    "    cls_layer_output, reg_layer_output = self.call(image) # 텐서 반환(상관성 존재). (1,14,14,18), (1,14,14,36)\n",
    "    minibatch_index_list = self.get_minibatch_index(cls_layer_output_label) # 미니배치 인덱스 휙득\n",
    "\n",
    "    # 모든 cls output을 softmax\n",
    "    cls_layer_output = tf.reshape(cls_layer_output[0], [1764,2])\n",
    "    cls_activation = tf.nn.softmax(cls_layer_output) # 잘됨\n",
    "\n",
    "    # reg output는 앵커의 변화값임\n",
    "    reg_layer_output = tf.reshape(reg_layer_output[0], [1764,4])\n",
    "    anchor_Use = np.reshape(anchors, (-1,4))\n",
    "    anchor_tensor = tf.convert_to_tensor(anchor_Use, dtype=tf.float32)\n",
    "    reg_layer_output = tf.math.add(reg_layer_output, anchor_tensor)\n",
    "\n",
    "    # label은 (1764,2)와 (1764,4)임\n",
    "    tensor_cls_label = tf.convert_to_tensor(cls_layer_output_label, dtype=tf.float32)\n",
    "    tensor_reg_label = tf.convert_to_tensor(reg_layer_output_label, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    # loss 계산(Loss 텐서에서 미니배치에 해당되는 애들만 걸러내야함)\n",
    "    print(\"loss 계산\")\n",
    "    Cls_Loss = tf.nn.softmax_cross_entropy_with_logits(labels=tensor_cls_label, logits=cls_activation) # (1764,1) 텐서\n",
    "\n",
    "    ## 여기까지 이상 무\n",
    "\n",
    "    filter_x = tf.Variable([[1.0],[0.0],[0.0], [0.0]])\n",
    "    filter_y = tf.Variable([[0.0],[1.0],[0.0], [0.0]])\n",
    "    filter_w = tf.Variable([[0.0],[0.0],[1.0], [0.0]])\n",
    "    filter_h = tf.Variable([[0.0],[0.0],[0.0], [1.0]])\n",
    "\n",
    "    # 여기서 터짐. 4개만 떼서 계산하니까 잘됨\n",
    "    x = tf.matmul(reg_layer_output,filter_x)\n",
    "    y = tf.matmul(reg_layer_output,filter_y)\n",
    "    w = tf.matmul(reg_layer_output,filter_w)\n",
    "    h = tf.matmul(reg_layer_output,filter_h)\n",
    "\n",
    "    x_a = tf.matmul(anchor_tensor,filter_x)\n",
    "    y_a = tf.matmul(anchor_tensor,filter_y)\n",
    "    w_a = tf.matmul(anchor_tensor,filter_w)\n",
    "    h_a = tf.matmul(anchor_tensor,filter_h)\n",
    "\n",
    "    x_star = tf.matmul(tensor_reg_label,filter_x)\n",
    "    y_star = tf.matmul(tensor_reg_label,filter_y)\n",
    "    w_star = tf.matmul(tensor_reg_label,filter_w)\n",
    "    h_star = tf.matmul(tensor_reg_label,filter_h)\n",
    "\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype)) # 텐서 로그는 ln밖에 없어서 ln10을 구한 뒤 나누는 방식으로 log10을 구한다(로그의 밑변환 공식)\n",
    "\n",
    "    # 4개만 떼서 계산하니까 잘됨\n",
    "    t_x = tf.math.divide(tf.subtract(x, x_a), w_a)\n",
    "    t_y = tf.math.divide(tf.subtract(y, y_a), h_a)\n",
    "    t_w = tf.math.divide(tf.math.log(tf.math.divide(w, w_a)), denominator)\n",
    "    t_w = tf.math.divide(tf.math.log(tf.math.divide(h, h_a)), denominator)\n",
    "\n",
    "    t_x_star = tf.math.divide(tf.math.subtract(x_star, x_a), w_a)\n",
    "    t_y_star = tf.math.divide(tf.math.subtract(y_star, y_a), h_a)\n",
    "    t_w_star = tf.math.devide(tf.math.log(tf.divide(w_star, w_a)), denominator)\n",
    "    t_h_star = tf.math.devide(tf.math.log(tf.divide(h_star, h_a)), denominator)\n",
    "\n",
    "    # (1764,1)에 해당하는 t_x, t_y...을 구했다. 여기서 미니배치에 해당되는 애들만 걸러낸다. \n",
    "\n",
    "    # 미니배치에 해당되는 애들만 0이 아닌 값으로 만들기. 미니배치 리스트는 미니배치에 해당되는 인덱스는 1이고 나머지는 다 0이니까 tf.math.multiply를 사용해 원소별 곱을 해주면 미니배치에 해당되는 값들만 얻을 수 있다. \n",
    "    minibatch_index_list = np.reshape(minibatch_index_list, (1764,1)) # (1764,1)로 reshape해주기\n",
    "    minibatch_index_tensor = np.reshape(minibatch_index_list, dtype=tf.float32) # 텐서로 변환\n",
    "\n",
    "    # 다 곱해서 미니배치 성분만 남기기\n",
    "    t_x_minibatch = tf.math.multiply(t_x, minibatch_index_tensor)\n",
    "    t_y_minibatch = tf.math.multiply(t_x, minibatch_index_tensor)\n",
    "    t_w_minibatch = tf.math.multiply(t_x, minibatch_index_tensor)\n",
    "    t_h_minibatch = tf.math.multiply(t_x, minibatch_index_tensor)\n",
    "\n",
    "    t_x_star_minibatch = tf.math.multiply(t_x_star, minibatch_index_tensor)\n",
    "    t_y_star_minibatch = tf.math.multiply(t_y_star, minibatch_index_tensor)\n",
    "    t_w_star_minibatch = tf.math.multiply(t_w_star, minibatch_index_tensor)\n",
    "    t_h_star_minibatch = tf.math.multiply(t_h_star, minibatch_index_tensor)\n",
    "\n",
    "    Cls_Loss_minibatch = tf.math.multiply(Cls_Loss, minibatch_index_tensor)\n",
    "\n",
    "    # 각 성분별로 1764개분 Loss를 다 합친 4개의 값이 나왔다\n",
    "    # X성질에 대한 Smooth L1. huber_loss에서 delta = 1로 하면 smooth L1과 같다.\n",
    "    # 미니배치 성분만 뽑아내서 미니배치가 아닌 인덱스의 값은 0인데 Smooth L1에서 |x| < 1이면 0.5*x^2니까 0이 나오며 이는 loss에 어떠한 영향을 미치지 않는다. \n",
    "    x_huber_loss = tf.compat.v1.losses.huber_loss(t_x_star_minibatch, t_x_minibatch) \n",
    "    y_huber_loss = tf.compat.v1.losses.huber_loss(t_y_star_minibatch, t_y_minibatch)\n",
    "    w_huber_loss = tf.compat.v1.losses.huber_loss(t_w_star_minibatch, t_w_minibatch)\n",
    "    h_huber_loss = tf.compat.v1.losses.huber_loss(t_h_star_minibatch, t_h_minibatch)\n",
    "\n",
    "    # 한 번에 더하니까 에러가 발생해 tf.math.add로 두개씩 더한다.\n",
    "    Reg_Loss = tf.math.add(x_huber_loss, y_huber_loss)   \n",
    "    Reg_Loss = tf.math.add(Reg_Loss, w_huber_loss) # (x_huber_loss + y_huber_loss) + w_huber_loss\n",
    "    Reg_Loss = tf.math.add(Reg_Loss, h_huber_loss) # (x_huber_loss + y_huber_loss + w_huber_loss) + h_huber_loss\n",
    "\n",
    "    N_reg = tf.constant([1764.0])\n",
    "    N_cls = tf.constant([10.0/256.0]) # lambda도 곱한 값\n",
    "\n",
    "    loss_cls = tf.multiply(N_reg, tf.reduce_sum(Cls_Loss_minibatch))\n",
    "    loss_reg = tf.multiply(N_cls, Reg_Loss)\n",
    "\n",
    "    loss = tf.add(loss_cls, loss_reg)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "    # get gradients\n",
    "  def get_grad(self, Loss, training_step, cls_reg_boolean): # cls_reg_boolean = 0이면 cls, cls_reg_boolean = 1 이면 reg\n",
    "    with tf.GradientTape() as tape:\n",
    "      if training_step == 1 : # 처음에는 conv3_1까지만 훈련\n",
    "        tape.watch(self.conv3_1.variables)\n",
    "        tape.watch(self.conv3_2.variables)\n",
    "        tape.watch(self.conv3_3.variables)\n",
    "        tape.watch(self.conv4_1.variables)\n",
    "        tape.watch(self.conv4_2.variables)\n",
    "        tape.watch(self.conv4_3.variables)\n",
    "        tape.watch(self.conv5_1.variables)\n",
    "        tape.watch(self.conv5_2.variables)\n",
    "        tape.watch(self.conv5_3.variables)\n",
    "        tape.watch(self.intermediate_layer.variables)\n",
    "\n",
    "        if cls_reg_boolean == 0:\n",
    "          tape.watch(self.cls_Layer.variables)\n",
    "        else:\n",
    "          tape.watch(self.reg_layer.variables)\n",
    "\n",
    "      elif training_step == 3:\n",
    "        shared_output = self.intermediate_layer(shared_output)\n",
    "        if cls_reg_boolean == 0:\n",
    "          tape.watch(self.cls_Layer.variables)\n",
    "        else:\n",
    "          tape.watch(self.reg_layer.variables)\n",
    "\n",
    "      g = 0\n",
    "      if training_step == 1 :\n",
    "        if cls_reg_boolean == 0:\n",
    "          g = tape.gradient(Loss, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]])\n",
    "        else:\n",
    "          g = tape.gradient(Loss, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]])\n",
    "\n",
    "      elif training_step == 3:\n",
    "        if cls_reg_boolean == 0:\n",
    "          g = tape.gradient(Loss, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]])\n",
    "        else:\n",
    "          g = tape.gradient(Loss, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]])\n",
    "\n",
    "    return g\n",
    "    \n",
    "  def App_Gradient(self, Loss, training_step) :\n",
    "    if training_step == 1:\n",
    "      g_cls = self.get_grad(Loss, training_step, 0)\n",
    "      self.Optimizers.apply_gradients(zip(g_cls, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]]))\n",
    "\n",
    "      g_reg = self.get_grad(Loss, training_step, 1)\n",
    "      self.Optimizers.apply_gradients(zip(g_reg, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]]))\n",
    "\n",
    "    if training_step == 3:\n",
    "\n",
    "      g_cls = self.get_grad(Loss, training_step, 0)\n",
    "      self.Optimizers.apply_gradients(zip(g_cls, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]]))\n",
    "\n",
    "      g_reg = self.get_grad(Loss, training_step, 1)\n",
    "      self.Optimizers.apply_gradients(zip(g_reg, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]]))\n",
    "\n",
    "  # perform gradient descent\n",
    "\n",
    "  \n",
    "  def Training_model(self, image_list, cls_layer_ouptut_label_list, reg_layer_ouptut_label_list, training_step):\n",
    "    Loss_acc = 0\n",
    "    for i in tqdm(range(0, len(image_list)), desc = \"training\"):\n",
    "\n",
    "      image = np.expand_dims(image_list[i], axis = 0) # (1,224,224,3)으로 제작\n",
    "      Loss = self.multi_task_loss(image, cls_layer_ouptut_label_list[i], reg_layer_ouptut_label_list[i])\n",
    "      Loss_acc = Loss_acc + Loss\n",
    "      if i % 19 == 0:\n",
    "        Loss_acc = Loss_acc / 20\n",
    "        self.App_Gradient(Loss, training_step)\n",
    "        Loss_acc = 0\n"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RPN 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "regularizer = tf.keras.regularizers.l2(0.0005)\n",
    "\n",
    "for layer in SharedConvNet.layers:\n",
    "    # 'kernel_regularizer' 속성이 있는 인스턴스를 찾아 regularizer를 추가\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        setattr(layer, 'kernel_regularizer', regularizer)\n",
    "\n",
    "RPN_Model = RPN(initializer, regularizer, SharedConvNet)"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training:   0%|          | 0/5011 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "RPN_Model.Training_model(image_list, cls_layer_label_list, reg_layer_label_list, 1)"
   ]
  },
  {
   "source": [
    "### RPN에서 값 얻어내기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio):\n",
    "\n",
    "    for y in range(0, 14):\n",
    "        for x in range(0, 14):\n",
    "            for i in range(0, len(anchor_size)):\n",
    "                for j in range(0, len(anchor_aspect_ratio)):\n",
    "                    # 오브젝트다 vs 아니다 2개 항목에 대한 스코어 저장(각 앵커당)\n",
    "                    index = 3 * i + j\n",
    "                    object_score_exp = np.exp(cls_layer_output[y][x][2 * index]) # 출력값이 작아서 1e32를 구현하다\n",
    "                    non_object_score_exp = np.exp(cls_layer_output[y][x][2 * index + 1]) \n",
    "\n",
    "                    exp_sum = object_score_exp + non_object_score_exp\n",
    "                    score = np.array([object_score_exp / exp_sum, non_object_score_exp / exp_sum])\n",
    "                    cls_layer_output[y][x][2*(3*i+j)] = score[0]\n",
    "                    cls_layer_output[y][x][2*(3*i+j) + 1] = score[1]\n",
    "                        \n",
    "                    center_x = 8 + 16 * x \n",
    "                    center_y = 8 + 16 * y\n",
    "                    w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                    h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                    # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                    reg_layer_output[y][x][4*index] = center_x + reg_layer_output[y][x][4*index]*1e32\n",
    "                    reg_layer_output[y][x][4*index + 1] = center_y + reg_layer_output[y][x][4*index + 1]*1e32\n",
    "                    reg_layer_output[y][x][4*index + 2] = w + reg_layer_output[y][x][4*index + 2]*1e32\n",
    "                    reg_layer_output[y][x][4*index + 3] = h + reg_layer_output[y][x][4*index + 3]*1e32\n",
    "\n",
    "    cls_layer_output = np.reshape(cls_layer_output, (-1,2))\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (-1,4))\n",
    "\n",
    "    return cls_layer_output, reg_layer_output\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "source": [
    "def Get_Output_RPN(RPN_Model, image_list) :\n",
    "\n",
    "    cls_layer_output_list = np.array([])\n",
    "    reg_layer_output_list = np.array([])\n",
    "\n",
    "    anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "    anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준\n",
    "\n",
    "    for i in tqdm(range(0, len(image_list)), desc = \"get_output\"):\n",
    "        cls_layer_output, reg_layer_output = RPN_Model(np.expand_dims(image_list[i], axis=0))\n",
    "\n",
    "        cls_layer_output = cls_layer_output[0].numpy()\n",
    "        reg_layer_output = reg_layer_output[0].numpy()\n",
    "\n",
    "        # 가공하자\n",
    "        cls_layer_output, reg_layer_output = Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio)\n",
    "\n",
    "        cls_layer_output_list = np.append(cls_layer_output_list, cls_layer_output)\n",
    "        reg_layer_output_list = np.append(reg_layer_output_list, reg_layer_output)\n",
    "    \n",
    "    cls_layer_output_list = np.reshape(cls_layer_output_list, (-1,1764,2))\n",
    "    reg_layer_output_list = np.reshape(reg_layer_output_list, (-1,1764,4))\n",
    "\n",
    "    return cls_layer_output_list, reg_layer_output_list"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "source": [
    "cls_layer_output_list, reg_layer_output_list = Get_Output_RPN(RPN_Model, image_list) # 출력값 얻기"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get_output: 100%|██████████| 5011/5011 [56:07<00:00,  1.49it/s]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "#### 일단 RoI를 특성맵에 맞게 변환시켜서 표시해보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fast R-CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 한 이미지에 대한 출력값 중 object score가 0.7 이상인 앵커들을 선별한다\n",
    "### 앞서 가공한 출력값을 사용한다. 여기서 cls_output은 (1764,2)고 reg_output은 (1764,4)다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Fast R-CNN을 위한 클래스 라벨 데이터를 생성\n",
    "#### 이미지별 Ground Truth Box와 Classes 정보를 얻는다 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 존재하는 클래스가 얼마나 있는지 알아낸다\n",
    "def get_Classes_inImage(xml_file_list):\n",
    "    classes = []\n",
    "\n",
    "    for xml_file_path in xml_file_list: \n",
    "\n",
    "        f = open(xml_file_path)\n",
    "        xml_file = xmltodict.parse(f.read())\n",
    "        # 사진에 객체가 여러개 있을 경우\n",
    "        try: \n",
    "            for obj in xml_file['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) # 들어있는 객체 종류를 알아낸다\n",
    "        # 사진에 객체가 하나만 있을 경우\n",
    "        except TypeError as e: \n",
    "            classes.append(xml_file['annotation']['object']['name'].lower()) \n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) # set은 중복된걸 다 제거하고 유니크한? 아무튼 하나만 가져온다. 그걸 리스트로 만든다\n",
    "    classes.sort() # 정렬\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Cls_DataSet_forFastRCNN(xml_file_list):\n",
    "    # Label List를 받아 데이터셋에 어떤 클래스가 있는지 알아내고 클래스 종류를 받아 이미지별 어떤 클래스가 있는지 Ground Truth Box별로 one-hot encoding을 해서 반환한다\n",
    "    # 이미지별 GroundTruthBox도 반환한다\n",
    "    Classes_List = get_Classes_inImage(xml_file_list) # ['car', ...] 리스트를 받는다.\n",
    "    num_Classes = len(Classes_List) # 데이터셋에 클래스가 몇종류인가?\n",
    "\n",
    "    # 클래스 리스트를 알았으니 데이터셋을 만들어보자\n",
    "    # 훈련 이미지 5011개 분의 데이터를 얻어야한다\n",
    "    Cls_labels_for_FastRCNN = []\n",
    "    Reg_labels_for_FastRCNN = []\n",
    "\n",
    "    for i in range(0, len(xml_file_list)):\n",
    "        GroundTruthBoxes_inImage = get_Ground_Truth_Box_fromImage(xml_file_list[i]) # 이미지별 Ground Truth Box 리스트. (n, 4)크기의 리스트 받음\n",
    "\n",
    "        classes = []\n",
    "        f = open(xml_file_list[i])\n",
    "        xml_file = xmltodict.parse(f.read())\n",
    "        # 사진에 객체가 여러개 있을 경우\n",
    "        try: \n",
    "            for obj in xml_file['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) # 들어있는 객체 종류를 알아낸다\n",
    "        # 사진에 객체가 하나만 있을 경우\n",
    "        except TypeError as e: \n",
    "            classes.append(xml_file['annotation']['object']['name'].lower()) \n",
    "\n",
    "        # 한 이미지에서 얻은 클래스 리스트에서 각 클래스가 Classes_List 내에서 어떤 인덱스 번호를 갖고 있는지 알아낸다.\n",
    "        # 그 인덱스 번호로 원-핫 인코딩 수행\n",
    "        cls_index_list = []\n",
    "        for class_val in classes :\n",
    "            cls_index = Classes_List.index(class_val) # 클래스가 Classes_List 내에서 어떤 인덱스 번호를 갖고 있는가?\n",
    "            cls_index_list.append(cls_index)# 한 이미지 내에 있는 Ground Truth Box별로 갖고 있는 클래스 인덱스를 저장\n",
    "        cls_onehot_inImage = np.eye(len(cls_index_list))[cls_index_list] # (n,21) 크기의 리스트 받음. 여기서 n은 한 이미지 내에 있는 객체 숫자\n",
    "\n",
    "        # 저장\n",
    "        Reg_labels_for_FastRCNN.append(GroundTruthBoxes_inImage)\n",
    "        Cls_labels_for_FastRCNN.append(cls_onehot_inImage)\n",
    "\n",
    "    return Reg_labels_for_FastRCNN, Cls_labels_for_FastRCNN # 이미지별 Ground Truth Box와 Classes 리스트"
   ]
  },
  {
   "source": [
    "### RPN에서 뽑아낸 1761(224*224 이미지를 입력으로 받는 VGG16 기준)개의 Box 중 object score가 0.7보다 큰 것들을 RoI로 선정한다.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(cls_layer_output, reg_layer_output): # 한 이미지 안에 있는 RoI를 선별\n",
    "    nms_RoI_inImage = [] # 한 이미지에 들어있는 RoI 리스트\n",
    "\n",
    "    for i in range(0, len(cls_layer_output)) :\n",
    "        if cls_layer_output[i][0] > 0.7 : # 해당 앵커의 object score가 0.7을 넘겼으면 RoI로 선정\n",
    "            nms_RoI_inImage.append(reg_layer_label[i].tolist) # RoI 추가\n",
    "\n",
    "    return nms_RoI_inImage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nms_list(cls_layer_output_list, reg_layer_output_list) :\n",
    "    # RPN output 2개, 이미지에서 각 앵커(1764)들이 어떤 Ground Truth Box보고 IoU 계산했는지, 이미지에서 각 앵커(1764)들이 어떤 '클래스'의 Ground Truth Box보고 IoU 계산했는지\n",
    "    # Detector 훈련에 필요한 데이터를 얻는 곳이다\n",
    "    \n",
    "    NMS_RoIs_List = [] # 전체 입력 이미지의 RoI를 이미지별로 저장(리스트 안에 리스트)\n",
    "    NMS_GroundTruthBoxes_List = []\n",
    "    NMS_Classes_List = []\n",
    "\n",
    "    for i in tqdm(range(0, len(cls_layer_output_list)), desc = \"get_RoI\"): # 5011개에 대한 nms 구한다\n",
    "        nms_RoI_inImage = nms(cls_layer_output_list[i], reg_layer_output_list[i]) # # 각 이미지에서 RoI들 구하기\n",
    "        NMS_RoIs_List.append(nms_RoI_inImage) # 각 이미지에서 얻은 RoI를 넣기\n",
    "\n",
    "    return NMS_RoIs_List # (5011, list) 리스트를 반환"
   ]
  },
  {
   "source": [
    "## Detector 훈련에 필요한 데이터셋 휙득"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMS_RoIs_List = get_nms_list(cls_layer_output_list, reg_layer_output_list) # 입력 데이터\n",
    "Reg_labels_for_FastRCNN, Cls_labels_for_FastRCNN = make_Cls_DataSet_forFastRCNN(xml_file_list) # 라벨 데이터"
   ]
  },
  {
   "source": [
    "## Detector 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RoI Pooling Layer 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling 작업을 RoI 지역에 대해 한다. 14*14*512를 7*7*512로\n",
    "class RoiPoolingLayer(tf.keras.layers):\n",
    "    def __init__(self, pool_size, **kwargs):\n",
    "        super(RoiPoolingLayer, self).__init__(name='RoI_Pooling_Layer')\n",
    "        self.pool_size = pool_size # VGG16에서는 7*7\n",
    "        \n",
    "    def build(self, input_shape): # input shape로 (1,14,14,512)와 같이 받으니까 3번 원소 자리의 값인 512를 가져간다. \n",
    "        self.nb_channels = input_shape[0][3] # 채널 조정\n",
    "        # 맨처음 입력받을 때 채널 숫자를 받는다. 풀링이라 채널 개수를 유지해야하기 때문\n",
    "\n",
    "    def compute_output_shape(self, input_shape): # If the layer has not been built, this method will call build on the layer. \n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, image, RoI_inImage): # 정방향 연산. shared FeatureMap (1,14,14,512)와 입력 이미지에 있는 RoI 리스트를 받는다\n",
    "        # 해야될거\n",
    "        # 1. RoiPooling을 위해 RoI 영역을 (14,14)에 맞게 변형하기\n",
    "\n",
    "        RoiPooling_outputs_List = [] # RoI 부근을 잘라낸 뒤 7*7로 만들어낸 것들을 여기에 모은다. 그러면 (n,7,7,512)가 되겠지\n",
    "\n",
    "        for roi_idx in range(0, len(RoI_inImage)): # 이미지 당 RoI 갯수만큼 for문 반복 -> RoI 갯수만큼 특성맵 얻으려고\n",
    "            # 224 -> 14로 16배 줄어들었으니 이에 맞춰 RoI도 줄인다. \n",
    "            x = round(RoI_List[i][0] / 16.0)\n",
    "            y = round(RoI_List[i][1] / 16.0)\n",
    "            w = round(RoI_List[i][2] / 16.0)\n",
    "            h = round(RoI_List[i][3] / 16.0)\n",
    "\n",
    "            # 제일 큰 앵커 사이즈가 128*128인데 이는 (14,14)에서 (8,8)이 된다. \n",
    "            # 제일 작은 앵커는 (2,2)이다. 그래서 나는 'by copying 7 times each cell and then max-pooling back to 7x7', 즉 이미지의 각 셀을 7*7로 복사한 뒤 (7*7)로 max Pooling하는 방식을 사용하고자 한다. \n",
    "            # 아이디어 출처 : https://stackoverflow.com/questions/48163961/how-do-you-do-roi-pooling-on-areas-smaller-than-the-target-size\n",
    "            image_inRoI = image[:, y:y+h, x:x+w, :] # RoI에 해당되는 부분을 추출한다.\n",
    "            # 추출 후 각 픽셀을 7*7로 복사한 뒤 7*7로 만들어버리는 max_pooling을 수행한다(원래 이미지 크기를 sub-window로 하고 stride역시 원래 이미지 크기로 한다. 그러면 7*7로 줄어든다. \n",
    "            \n",
    "        Pooling_output = tf.concat(outputs, axis=0)\n",
    "\n",
    "        Pooling_output = tf.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels)) # output 양식이 (1,...)니까 (1, RoI개수, 7,7,512)와 같이 만들어준다. \n",
    "        Pooling_output = tf.keras.backend.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return Pooling_output\n",
    "    \n",
    "    def get_config(self): # 구성요소 반환\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(Detector, self).__init__(name='Detector')\n",
    "    # 레이어 \n",
    "    # 공용 레이어(14*14*512)를 받아서 RoI Pooling Layer에 넣어 7*7*512를 만들고 FCs를 거쳐 두가지 Output을 생성한다. \n",
    "\n",
    "    # 클래스 분류 레이어와 박스 위치 회귀 레이어의 초기화를 다르게 해야한다. \n",
    "    Classify_layer_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "    Box_regression_layer_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None)\n",
    "\n",
    "    # RoI Pooling : H*W(7*7)에 맞게 입력 특성맵을 pooling. RoI에 해당하는 영역을 7*7로 Pooling한다. \n",
    "    self.RoIPoolingLayer = RoiPoolingLayer(7, num_rois) # Pooling 이후 크기를 7*7*512로 만든다. \n",
    "    self.Flatten_layer = tf.keras.layers.Flatten() # 여기서 4096개가 되어야한다.\n",
    "    self.Fully_Connected = tf.keras.layers.Dense(4096, activation='relu') # 별 말이 없으니 기본적으로 지정된 kernel_initializer를 사용하자\n",
    "    self.Classify_layer = tf.keras.layers.Dense(21, activation='softmax', kernel_initializer = Classify_layer_initializer, name = \"output_1\")\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(84, activation= None, kernel_initializer = Box_regression_layer_initializer, name = \"output_2\")\n",
    "    \n",
    "  def call(self, input1, input2): # input으로 공용 컨볼루션 레이어 텐서(1,14,14,512)와 RoI를 받는다. RoI는 앞서 RPN에서 뽑아낸걸 쓴다. \n",
    "    \n",
    "    output = self.RoIPoolingLayer(input1, input2) # input1 : (1,14,14,512), input2 : RoI -> 7*7 output\n",
    "    output = self.Flatten_layer(output) # Flatten으로 한줄 세우기\n",
    "    output = self.Fully_Connected(output) # FCs로 만들기\n",
    "    # 객체 분류 레이어, 박스 회귀 레이어\n",
    "    cls_output = Classify_layer_output = self.Classify_layer(output) # 각 객체에 \n",
    "    reg_layer_output = self.reg_layer(output)\n",
    "\n",
    "    return Classify_layer_output, reg_layer_output # 두 아웃풋을 내놓는다. \n",
    "\n",
    "  # 필요한거 : multi task loss, gradient 계산, 적용\n",
    "\n",
    "  def get_grad(self, Loss, training_step, cls_reg_boolean):\n",
    "  \n",
    "  def App_Gradient(self, Loss, training_step) :\n",
    "\n",
    "  \n",
    "  def training_Detector():\n"
   ]
  },
  {
   "source": [
    "## Loss 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "Detector_Model = Detector(SharedConvNet)\n",
    "RPN_Model.run_eagerly = True # 모델 내부 함수에서 쉽게 연산할 수 있게 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Detector(model, , traning_step, EPOCH):\n",
    "\n",
    "    losses = {'output_1' : 'categorical_crossentropy', 'output_2' : loss_detector_loc} # classify layer는 20 + 1가지 클래스에 대한 softmax, reg_layer는 따로 만든 loss함수\n",
    "\n",
    "    model.compile(optimizer=Optimizer, loss=losses, run_eagerly=True)\n",
    "\n",
    "    history = model.fit(np.asarray(image_list), [cls_layer_label_list, reg_layer_label_list], batch_size = 1, epochs = EPOCH)\n",
    "\n",
    "    return model, history\n"
   ]
  }
 ]
}
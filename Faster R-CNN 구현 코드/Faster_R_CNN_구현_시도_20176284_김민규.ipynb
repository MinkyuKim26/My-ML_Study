{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dec8a5ff48d192d3613b9e6713a72fe103c8e5d5c44dc7b8ee41f3376e4a4f37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Faster R-CNN 구현\n",
    "### 텐서플로우 기반\n",
    "### 주석은 한글 위주로 작성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "source": [
    "## 훈련 이미지 가져오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/JPEGImages'\n",
    "train_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/Annotations'\n",
    "\n",
    "test_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/JPEGImages'\n",
    "test_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5011\n4952\n"
     ]
    }
   ],
   "source": [
    "list_train_x = sorted([x for x in glob.glob(train_x_path + '/**')])    \n",
    "list_train_y = sorted([x for x in glob.glob(train_y_path + '/**')]) \n",
    "\n",
    "list_test_x = sorted([x for x in glob.glob(test_x_path + '/**')])    \n",
    "list_test_y = sorted([x for x in glob.glob(test_y_path + '/**')]) \n",
    "\n",
    "print(len(list_train_x))\n",
    "print(len(list_test_x))"
   ]
  },
  {
   "source": [
    "## 훈련용 데이터로 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 테스트용\n",
    "def get_image_224(image_file_path) :\n",
    "    image = cv2.imread(image_file_path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력용 이미지 생성. 224, 224로 변환시키고 채널 값(0~255)를 0~1 사이의 값으로 정규화 시켜줌\n",
    "def make_input(image_file_list): \n",
    "    images_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_file_list)), desc=\"get image\") :\n",
    "        \n",
    "        image = cv2.imread(image_file_list[i])\n",
    "        image = cv2.resize(image, (224, 224))/255\n",
    "        \n",
    "        images_list.append(image)\n",
    "    \n",
    "    return np.asarray(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 존재하는 객체 종류를 알아내자\n",
    "def get_Classes_inImage(xml_file_list):\n",
    "    classes = []\n",
    "\n",
    "    for xml_file_path in xml_file_list: \n",
    "\n",
    "        f = open(xml_file_path)\n",
    "        xml_file = xmltodict.parse(f.read())\n",
    "        # 사진에 객체가 여러개 있을 경우\n",
    "        try: \n",
    "            for obj in xml_file['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) # 들어있는 객체 종류를 알아낸다\n",
    "        # 사진에 객체가 하나만 있을 경우\n",
    "        except TypeError as e: \n",
    "            classes.append(xml_file['annotation']['object']['name'].lower()) \n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) # set은 중복된걸 다 제거하고 유니크한? 아무튼 하나만 가져온다. 그걸 리스트로 만든다\n",
    "    classes.sort() # 정렬\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 하나에 대한 라벨값(어떤 클래스가 있는지, 그 클래스는 어떤 박스를 갖고 있는지)\n",
    "def get_label_fromImage(xml_file_path, Classes_inDataSet): # xml_file_path은 파일 하나의 경로를 나타낸다\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    xml_file = xmltodict.parse(f.read()) \n",
    "\n",
    "    # 우선 원래 이미지 크기를 얻는다. 왜냐하면 앵커는 224*224 기준으로 만들었는데 원본 이미지는 224*224가 아니기 때문.\n",
    "    # 224*224에 맞게 줄일려고 하는거다\n",
    "    Image_Height = float(xml_file['annotation']['size']['height'])\n",
    "    Image_Width  = float(xml_file['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    Classes_list = [] \n",
    "    Ground_Truth_Box_list = [] \n",
    "    class_label_list = [] # 원-핫 인코딩으로 만든 y값. 이건 Fast R-CNN 학습에 쓰인다.\n",
    "\n",
    "    # multi-objects in image\n",
    "    try:\n",
    "        for obj in xml_file['annotation']['object']:\n",
    "            obj_class = obj['name'].lower() \n",
    "            # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "            x_min = float(obj['bndbox']['xmin']) \n",
    "            y_min = float(obj['bndbox']['ymin'])\n",
    "            x_max = float(obj['bndbox']['xmax']) \n",
    "            y_max = float(obj['bndbox']['ymax'])\n",
    "\n",
    "            # 224*224에 맞게 변형시켜줌\n",
    "            x_min = float((224/Image_Width)*x_min)\n",
    "            y_min = float((224/Image_Height)*y_min)\n",
    "            x_max = float((224/Image_Width)*x_max)\n",
    "            y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "            Ground_Truth_Box = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            index = Classes_inDataSet.index(obj_class) \n",
    "\n",
    "            Classes_list.append(index)\n",
    "            Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    # single-object in image\n",
    "    except TypeError as e : \n",
    "        \n",
    "        obj_class = xml_file['annotation']['object']['name'] \n",
    "        # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "        x_min = float(xml_file['annotation']['object']['bndbox']['xmin']) \n",
    "        y_min = float(xml_file['annotation']['object']['bndbox']['ymin']) \n",
    "        x_max = float(xml_file['annotation']['object']['bndbox']['xmax']) \n",
    "        y_max = float(xml_file['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "        # 224*224에 맞게 변형시켜줌\n",
    "        x_min = float((224/Image_Width)*x_min)\n",
    "        y_min = float((224/Image_Height)*y_min)\n",
    "        x_max = float((224/Image_Width)*x_max)\n",
    "        y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "        Ground_Truth_Box = [x_min, y_min, x_max, y_max]  \n",
    "\n",
    "        index = Classes_inDataSet.index(obj_class) \n",
    "\n",
    "        Classes_list.append(index)\n",
    "        Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    return Classes_list, Ground_Truth_Box_list # 각 이미지에 있는 클래스의 갯수는 다 다르기 때문에 리스트로 받는다. 리스트를 포함한 리스트 형태로 데이터를 만들 예정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 리스트\n",
    "image_file_list = sorted([x for x in glob.glob(train_x_path + '/**')])\n",
    "xml_file_list = sorted([x for x in glob.glob(train_y_path + '/**')])"
   ]
  },
  {
   "source": [
    "## 데이터에 존재하는 클래스 종류 알아내기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes_inDataSet = get_Classes_inImage(xml_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(Classes_inDataSet)"
   ]
  },
  {
   "source": [
    "훈련용 이미지는 5011개, 테스트용 이미지는 4952개가 있음을 알 수 있다. 라벨값 역시 마찬가지.\n",
    "<br>\n",
    "이미지에 존재하는 클래스는 20종류다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RPN - 앵커 준비\n",
    "#### 입력 이미지 기준으로 앵커를 생성한다.\n",
    "#### 풀링을 3번 하므로 2^3 = 8이니 (8, 8)부터 (16, 8)...등 8픽셀식 중심 좌표를 옮겨가며 앵커들을 k개씩 생성한다\n",
    "#### 생성한 앵커들 중 사용할 가치가 있는 앵커를 걸러낸다(이미지 범위를 벗어나지 않는 앵커들만 선정)\n",
    "#### 선정한 앵커 중 실제 물체의 box와 얼마나 곂치는지(IoU) 계산해본다. 확실히 겹친다 하는 애들을 Positive 앵커로, 거의 안겹친다 하는 애들은 Negataive 앵커로 선정한다. 애매한 애들은 거른다.\n",
    "#### 이렇게 생성된 앵커들로 미니배치를 생성한다. Positive 128개, Negative 128개로 만드는게 ideal한 구성이긴 한데 Positive한 앵커가 별로 없다. 그래서 Positive를 128개 못채웠으면 Negataive 앵커로 채워준다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 함수로 만드는 이유?\n",
    "#### 이미지 별로 라벨에서 박스 좌표랑 객체 종류 뽑아내서 loss를 계산해야한다. 그래서 코드의 간결화?를 위해 함수를 만드는 것"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 생성 함수. \n",
    "# 얘가 좀 이상하다\n",
    "def make_anchor(anchor_size, anchor_aspect_ratio) :\n",
    "    # 입력 이미지(그래봤자 224*224긴 하지만)에 맞춰 앵커를 생성해보자 \n",
    "\n",
    "    anchors = [] # [x,y,w,h]로 이루어진 리스트 \n",
    "    anchors_state = [] # 이 앵커를 훈련에 쓸건가? 각 앵커별로 사용 여부를 나타낸다. \n",
    "\n",
    "    # 앵커 중심좌표 간격\n",
    "    interval_x = 16\n",
    "    interval_y = 16\n",
    "    Center_max_x = 208 # 224 - 16, 중심좌표가 224가 될 수는 없다.\n",
    "    Center_max_y = 208 # 224 - 16\n",
    "\n",
    "    # 2단 while문 생성\n",
    "    x = 8\n",
    "    y = 8\n",
    "    index_count = 0\n",
    "    while(y <= 224): # 8~208 = 14개 \n",
    "        while(x <= 224): # 8~208 = 14개 \n",
    "            # k개의 앵커 생성. 여기서 k = len(anchor_size) * len(anchor_aspect_ratio)다\n",
    "            for i in range(0, len(anchor_size)) : \n",
    "                for j in range(0, len(anchor_aspect_ratio)) :\n",
    "                    anchor_width = anchor_aspect_ratio[j][0] * anchor_size[i]\n",
    "                    anchor_height = anchor_aspect_ratio[j][1] * anchor_size[i]\n",
    "\n",
    "                    anchor = [x, y, anchor_width, anchor_height]\n",
    "                    anchors.append(anchor)\n",
    "                    # 앵커가 이미지 경계선을 넘나드나? 필터링\n",
    "                    if((x - (anchor_width/2) >= 0) and (y - (anchor_height/2) >= 0) and\n",
    "                    (x + (anchor_width/2) <= 224) and (y + (anchor_height/2) <= 224)):\n",
    "                        # 경계 안에 있으면 1\n",
    "                        anchors_state.append(int(1))\n",
    "                    else :\n",
    "                        anchors_state.append(int(0))\n",
    "            x = x + interval_x \n",
    "        y = y + interval_y\n",
    "        x = 8\n",
    "    return np.array(anchors), np.array(anchors_state) # 넘파이로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준 \n",
    "anchors, anchors_state = make_anchor(anchor_size, anchor_aspect_ratio) # 앵커 생성 + 유효한 앵커 인덱스 휙득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "anchors_state[184]"
   ]
  },
  {
   "source": [
    "#### Loss 사용을 위해 Class list도 만들자.\n",
    "#### 논문의 loss 함수를 보면 pi가 있다. pi는 리스트인데 PASCAL VOC에 존재하는 객체들이 몇 %의 확률로 있느냐 나타내는거다\n",
    "#### 이를 위해 어떤 객체들이 PASCAL VOC에 존재하는지 알아야한다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(anchor, Ground_Truth_Box_List, Class_list) : # anchor, ground truth box list를 받아 각 IOU을 계산하고 제일 큰걸 반환 + 그 ground truth box가 가리키는 클래스 인덱스도 반환\n",
    "\n",
    "    IoU_max = 0\n",
    "    index_ground_truth_box = -1\n",
    "    class_index = -1\n",
    "    for i in range(0, len(Ground_Truth_Box_List)):\n",
    "\n",
    "        ground_truth_box = Ground_Truth_Box_List[i]\n",
    "\n",
    "        InterSection_min_x = max(anchor[0], ground_truth_box[0])\n",
    "        InterSection_min_y = max(anchor[1], ground_truth_box[1])\n",
    "\n",
    "        InterSection_max_x = min(anchor[2], ground_truth_box[2])\n",
    "        InterSection_max_y = min(anchor[3], ground_truth_box[3])\n",
    "\n",
    "        InterSection_Area = 0\n",
    "\n",
    "        if (InterSection_max_x - InterSection_min_x + 1) >= 0 and (InterSection_max_y - InterSection_min_y + 1) >= 0 :\n",
    "            InterSection_Area = (InterSection_max_x - InterSection_min_x + 1) * (InterSection_max_y - InterSection_min_y + 1)\n",
    "\n",
    "        box1_area = (anchor[2] - anchor[0]) * (anchor[3] - anchor[1])\n",
    "        box2_area = (ground_truth_box[2] - ground_truth_box[0]) * (ground_truth_box[3] - ground_truth_box[1])\n",
    "        Union_Area = box1_area + box2_area - InterSection_Area\n",
    "\n",
    "        IoU = (InterSection_Area/Union_Area)\n",
    "        if IoU > IoU_max :\n",
    "            IoU_max = IoU\n",
    "            index_ground_truth_box = i\n",
    "            class_index = Class_list[i]\n",
    "\n",
    "    return IoU_max, index_ground_truth_box, class_index # 어떤 박스와 IoU가 제일 높았는지, 그 박스는 어떤 클래스를 나타내는지\n",
    "    "
   ]
  },
  {
   "source": [
    "## positive anchor, negative anchor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커들을 Positive, Negative 앵커로 나누고 각 앵커가 참고한 Ground Truth Box와 Class를 반환하자\n",
    "def align_anchor(anchors, anchors_state, Ground_Truth_Box_list, Class_list):\n",
    "\n",
    "    # 각 앵커는 해당 위치에서 구한 여러가지 Ground truth Box와의 ioU 중 제일 높은거만 가져온다. \n",
    "    IoU_List = np.array([])\n",
    "    index_ground_truth_box_list = np.array([]) # 각 앵커가 어떤 Ground Truth Box를 보고 IoU를 계산했는가?\n",
    "    index_class_list = np.array([]) # 각 앵커가 어떤 class를 봤는가?\n",
    "\n",
    "    for i in range(0, len(anchors)):\n",
    "        anchor_minX = anchors[i][0] - (anchors[i][2]/2)\n",
    "        anchor_minY = anchors[i][1] - (anchors[i][3]/2)\n",
    "        anchor_maxX = anchors[i][0] + (anchors[i][2]/2)\n",
    "        anchor_maxY = anchors[i][1] + (anchors[i][3]/2)\n",
    "\n",
    "        anchor = [anchor_minX, anchor_minY, anchor_maxX, anchor_maxY]\n",
    "        IoU, index_ground_truth_box, class_index = get_iou(anchor, Ground_Truth_Box_list, Class_list)\n",
    "        IoU_List = np.append(IoU_List, IoU)\n",
    "        index_ground_truth_box_list = np.append(index_ground_truth_box_list, index_ground_truth_box)\n",
    "        index_class_list = np.append(index_class_list, class_index)\n",
    "\n",
    "\n",
    "    # positive, negative 앵커 분류\n",
    "    for i in range(0, 14*14):\n",
    "        # 각 위치에서 IoU가 가장 큰 앵커 or IoU가 0.7 넘는 앵커를 positive앵커로 한다. \n",
    "\n",
    "        # max 함수를 못믿겠다\n",
    "        maxIoU_inOneSpot = 0\n",
    "        for index in list(range(9*i, 9*i + 9)) :\n",
    "            if IoU_List[index] > maxIoU_inOneSpot :\n",
    "                maxIoU_inOneSpot = IoU_List[index]\n",
    "\n",
    "        for num in range(0, 9):\n",
    "            index = 9 * i + num\n",
    "            if IoU_List[index] > 0.7 or (maxIoU_inOneSpot == IoU_List[index] and IoU_List[index] >= 0.3): # 조건을 좀 조정했다. \n",
    "                anchors_state[index] = 2\n",
    "            elif IoU_List[index] < 0.3 : # negative anchor\n",
    "                anchors_state[index] = 1\n",
    "            else: # 애매한 앵커들\n",
    "                anchors_state[index] = 0\n",
    "\n",
    "\n",
    "    return anchors_state, index_ground_truth_box_list, index_class_list # 모든 앵커에 대한 ground truth box, class list"
   ]
  },
  {
   "source": [
    "## 훈련을 위한 미니배치 256(positive 128, negative 128개) 선별\n",
    "### loss 안에서 실시한다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch_index(output):\n",
    "    # 2개의 loss함수에서 불리는 함수. 우선 cls_layer의 loss에서 불린 뒤 reg_layer의 loss에서 불릴거다. \n",
    "    # 그러니 우선 cls에서 부를 때 미니배치 인덱스를 선별해 반환하고 그 다음 reg에서 부를 때는 선별된 인덱스 리스트를 반환 후 초기화한다.\n",
    "\n",
    "    index_list = np.array([])\n",
    "\n",
    "    if len(get_minibatch_index.index_list) == 0:\n",
    "        get_minibatch_index.index_list = np.zeros(14*14*9) # 각 앵커가 미니배치 뽑혔나 안뽑혔나\n",
    "        index_pos = np.array([])\n",
    "        index_neg = np.array([])\n",
    "        # cls_layer_output을 보고 긍정, 부정 앵커 분류. 그렇게 데이터셋을 구성함\n",
    "        for i in range(0, 1764):\n",
    "            if output[i][0] == 1.0 : # positive anchor\n",
    "                index_pos = np.append(index_pos, i)\n",
    "            elif output[i][0] == 0.0 : # negative anchor\n",
    "                index_neg = np.append(index_neg, i)\n",
    "\n",
    "        max_for = min([128, len(index_pos)])\n",
    "        ran_list = random.sample(range(0, len(index_pos)), max_for)\n",
    "\n",
    "        for i in range(0, len(ran_list)) :\n",
    "            index = int(index_pos[ran_list[i]])\n",
    "            get_minibatch_index.index_list[index] = 1\n",
    "\n",
    "        ran_list = random.sample(range(0, len(index_neg)), 256 - max_for) # 랜덤성 증가?를 위해 또다시 난수 생성\n",
    "        for i in range(0, len(ran_list)) :\n",
    "            index = int(index_neg[ran_list[i]])\n",
    "            get_minibatch_index.index_list[index] = 1\n",
    "\n",
    "        index_list = get_minibatch_index.index_list\n",
    "\n",
    "    else : # 데이터가 이미 존재\n",
    "        index_list = get_minibatch_index.index_list\n",
    "        get_minibatch_index.index_list = np.array([])\n",
    "\n",
    "    return index_list\n",
    "get_minibatch_index.index_list = np.array([])"
   ]
  },
  {
   "source": [
    "## Loss 계산을 위한 함수들"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smooth_L1(ti, ti_star) :\n",
    "    difference_ti = ti - ti_star\n",
    "    smooth_L1 = 0\n",
    "    if abs(difference_ti) < 1: smooth_L1 = 0.5 * (difference_ti * difference_ti)\n",
    "    else : smooth_L1 = abs(difference_ti) - 0.5\n",
    "\n",
    "    return smooth_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss_Regression. Lreg에 해당\n",
    "def Loss_Regression(predict_box, anchor_box, groundTruth_box) : \n",
    "    groundTruth_box = np.array([(groundTruth_box[2] + groundTruth_box[0])/2, (groundTruth_box[1] + groundTruth_box[3])/2, groundTruth_box[2] - groundTruth_box[0], groundTruth_box[3] - groundTruth_box[1]])\n",
    "\n",
    "    t_x = (predict_box[0] - anchor_box[0])/anchor_box[2]\n",
    "    t_y = (predict_box[1] - anchor_box[1])/anchor_box[3]\n",
    "    t_w = math.log10(predict_box[2]/anchor_box[2])\n",
    "    t_h = math.log10(predict_box[3]/anchor_box[3])\n",
    "\n",
    "\n",
    "    t_x_star = (groundTruth_box[0] - anchor_box[0])/anchor_box[2]\n",
    "    t_y_star = (groundTruth_box[1] - anchor_box[1])/anchor_box[3]\n",
    "    t_w_star = math.log10(groundTruth_box[2]/anchor_box[2])\n",
    "    t_h_star = math.log10(groundTruth_box[3]/anchor_box[3])\n",
    "\n",
    "    # Smooth L1 구하기\n",
    "    # 구성요소가 4개니까 4번 구해야겠지?\n",
    "    smooth_L1_x = Smooth_L1(t_x, t_x_star)\n",
    "    smooth_L1_y = Smooth_L1(t_y, t_y_star)\n",
    "    smooth_L1_w = Smooth_L1(t_w, t_w_star)\n",
    "    smooth_L1_h = Smooth_L1(t_h, t_h_star)\n",
    "\n",
    "    smooth_L1_list = smooth_L1_x + smooth_L1_y + smooth_L1_w + smooth_L1_h\n",
    "\n",
    "    return smooth_L1_list # 모아서 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_cls\n",
    "def Loss_Classes(cls_layer_output, pi_ground_truth) :\n",
    "    log_loss = -pi_ground_truth * math.log10(cls_layer_output[0]) - (1-pi_ground_truth)*math.log10(1-cls_layer_output[0])\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rpn_cls(y_true, y_pred): \n",
    "    # 60k -> 60*9 = 540개의 미니배치 훈련률과 20k -> 20*9 = 180개의 미니배치 훈련률이 다르다\n",
    "    lr_coefficient = 0\n",
    "    if loss_rpn_cls.counter <= 540 :\n",
    "        lr_coefficient = 1\n",
    "    else :\n",
    "        lr_coefficient = 0.1\n",
    "\n",
    "    \n",
    "    cls_layer_output = y_pred.numpy()[0]\n",
    "    cls_layer_output_label = y_true.numpy()[0]\n",
    "\n",
    "    cls_layer_output = np.reshape(cls_layer_output, (-1, 2))\n",
    "    cls_layer_output_label = np.reshape(cls_layer_output_label, (-1, 2))\n",
    "\n",
    "    minibatch_index_list = get_minibatch_index(cls_layer_output_label)\n",
    "    cls_layer_output_minibatch = np.array([])\n",
    "    cls_layer_output_label_minibatch = np.array([])\n",
    "\n",
    "    # 미니배치 선별\n",
    "    for i in range(0, len(minibatch_index_list)) :  \n",
    "        if minibatch_index_list[i] == 1 : # 미니배치로 뽑혔으면 \n",
    "            score = [cls_layer_output[i][0]  , cls_layer_output[i][1]] # 오브젝트다 vs 아니다 2개 항목에 대한 스코어 저장(각 앵커당)\n",
    "            score = tf.nn.softmax(score)\n",
    "            cls_layer_output_minibatch = np.append(cls_layer_output_minibatch, score)\n",
    "            cls_layer_output_label_minibatch = np.append(cls_layer_output_label_minibatch, cls_layer_output_label[i])\n",
    "\n",
    "    cls_layer_output_minibatch = np.reshape(cls_layer_output_minibatch, (-1, 2))\n",
    "    cls_layer_output_label_minibatch = np.reshape(cls_layer_output_label_minibatch, (-1, 2))\n",
    "\n",
    "    Lcls_sum = 0\n",
    "    # loss 계산\n",
    "    for i in range(0, 256):\n",
    "        pi_star = cls_layer_output_label_minibatch[i][0] # postive면 1, negative면 0\n",
    "        Lcls_sum = Lcls_sum + Loss_Classes(cls_layer_output_minibatch[i], pi_star)\n",
    "\n",
    "    loss = (1/256) * Lcls_sum\n",
    "    loss = loss * lr_coefficient\n",
    "    \n",
    "    loss_rpn_cls.counter = loss_rpn_cls.counter + 1\n",
    "\n",
    "    if loss_rpn_cls.counter > 80*9 :\n",
    "        loss_rpn_cls.counter = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "loss_rpn_cls.counter = 0 # 미니배치 한번 돌아갈 때마다 1씩 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rpn_reg(y_true, y_pred): \n",
    "    lr_coefficient = 0\n",
    "    if loss_rpn_reg.counter <= 540 :\n",
    "        lr_coefficient = 1\n",
    "    else :\n",
    "        lr_coefficient = 0.1\n",
    "\n",
    "    lambda_forLoss = 10\n",
    "    N_reg = 14*14\n",
    "\n",
    "    reg_layer_output = y_pred.numpy()[0]\n",
    "    reg_layer_output_label = y_true.numpy()[0]\n",
    "\n",
    "\n",
    "    # 예측값 가공\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (14,14,36))\n",
    "\n",
    "    for y in range(0, 14):\n",
    "        for x in range(0, 14):\n",
    "                for i in range(0, len(anchor_size)):\n",
    "                    for j in range(0, len(anchor_aspect_ratio)):\n",
    "\n",
    "                        center_x = 8 + 16 * x \n",
    "                        center_y = 8 + 16 * y\n",
    "                        w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                        h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                        # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                        reg_layer_output[y][x][4*(3*i+j)] = center_x + reg_layer_output[y][x][4*(3*i+j)]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 1] = center_y + reg_layer_output[y][x][4*(3*i+j) + 1]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 2] = w + reg_layer_output[y][x][4*(3*i+j) + 2]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 3] = h + reg_layer_output[y][x][4*(3*i+j) + 3]\n",
    "\n",
    "\n",
    "    # (-1, 4)로 변환 -> 각 앵커별 (x,y,w,h)\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (-1, 4))\n",
    "    reg_layer_output_label = np.reshape(reg_layer_output_label, (-1, 4))\n",
    "\n",
    "    minibatch_index_list = get_minibatch_index(reg_layer_output_label)\n",
    "    reg_layer_output_minibatch = np.array([])\n",
    "    reg_layer_output_label_minibatch = np.array([])\n",
    "    anchors_minibatch = np.array([])\n",
    "\n",
    "\n",
    "    # 미니배치 선별\n",
    "    for i in range(0, len(minibatch_index_list)) : \n",
    "        if minibatch_index_list[i] == 1 : # 미니배치로 뽑혔으면 \n",
    "            reg_layer_output_minibatch = np.append(reg_layer_output_minibatch, reg_layer_output[i])\n",
    "            reg_layer_output_label_minibatch = np.append(reg_layer_output_label_minibatch, reg_layer_output_label[i])\n",
    "            anchors_minibatch = np.append(anchors_minibatch, anchors[i])\n",
    "\n",
    "    # 미니배치 reshape\n",
    "    reg_layer_output_minibatch = np.reshape(reg_layer_output_minibatch, (-1, 4))\n",
    "    reg_layer_output_label_minibatch = np.reshape(reg_layer_output_label_minibatch, (-1, 4))\n",
    "    anchors_minibatch = np.reshape(anchors_minibatch, (-1, 4))\n",
    "\n",
    "    Lreg_sum = 0\n",
    "    # loss 계산\n",
    "    for i in range(0, 256):\n",
    "        # x,y,w,h 합이 0이 아니면 로스에 포함\n",
    "        if sum(reg_layer_output_label_minibatch[i]):\n",
    "            Lreg_sum = Lreg_sum + Loss_Regression(reg_layer_output_minibatch[i], anchors_minibatch[i], reg_layer_output_label_minibatch[i])\n",
    "\n",
    "\n",
    "    loss = lambda_forLoss*(1/N_reg)*Lreg_sum\n",
    "    loss = loss * lr_coefficient\n",
    "    \n",
    "    loss_rpn_reg.counter = loss_rpn_cls.counter + 1\n",
    "\n",
    "    if loss_rpn_reg.counter > 80*9 :\n",
    "        loss_rpn_reg.counter = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "loss_rpn_reg.counter = 0 # 미니배치 한번 돌아갈 때마다 1씩 올라감"
   ]
  },
  {
   "source": [
    "## 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_forRPN(image_file_list, xml_file_list, Classes_inDataSet, anchors, anchors_state) :\n",
    "    image_list = make_input(image_file_list)\n",
    "\n",
    "    valid_minibatch_anchor_list = np.array([])\n",
    "    cls_layer_label_list = np.array([])\n",
    "    reg_layer_label_list = np.array([])\n",
    "\n",
    "    index_ground_truth_boxes_list = np.array([])\n",
    "    index_classes_list = np.array([])\n",
    "\n",
    "    Classes_labels_list = []\n",
    "    Grond_Truth_Boxes_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(xml_file_list)), desc=\"get label\"):\n",
    "        class_label_list, Ground_Truth_Box_list = get_label_fromImage(xml_file_list[i], Classes_inDataSet) \n",
    "        anchors_state, index_ground_truth_box_list, index_class_list = align_anchor(anchors, anchors_state, Ground_Truth_Box_list, class_label_list) \n",
    "        # pos, neg 앵커 뭔지 들어있는 state, 각 앵커가 어떤 ground truth box를 참조했는지, 각 앵커가 어떤 종류의 클래스를 참조했는지\n",
    "        \n",
    "        # '한 이미지'에 있는 Ground Truth Box에 대한 정보\n",
    "        Classes_labels_list.append(class_label_list)\n",
    "        Grond_Truth_Boxes_list.append(Ground_Truth_Box_list)\n",
    "\n",
    "        cls_layer_label = np.array([]) # cls_layer는 14*14*18이므로 그에 맞게 가공\n",
    "        reg_layer_label = np.array([]) # 14*14*36\n",
    "\n",
    "        for j in range(0, len(anchors_state)) :\n",
    "            if anchors_state[j] == 2 : # positive\n",
    "                ground_box = Ground_Truth_Box_list[int(index_ground_truth_box_list[j])] # positive 앵커가 참고한 Ground Truth Box\n",
    "                #RPN\n",
    "                cls_layer_label = np.append(cls_layer_label, np.array([1.0,0.0])) # 해당 앵커의 output이 [1,0] -> positive\n",
    "                reg_layer_label = np.append(reg_layer_label, ground_box)\n",
    "            elif anchors_state[j] == 1 : # negative\n",
    "                cls_layer_label = np.append(cls_layer_label, np.array([0.0,1.0])) # 해당 앵커 output이 [0,1] -> negative\n",
    "                reg_layer_label = np.append(reg_layer_label, np.array([0,0,0,0]))\n",
    "            else : \n",
    "                cls_layer_label = np.append(cls_layer_label, np.array([0.5,0.5])) # 해당 앵커 output이 [0.5, 0.5] -> 무의미한 값\n",
    "                reg_layer_label = np.append(reg_layer_label, np.array([0,0,0,0]))\n",
    "\n",
    "        index_classes_list = np.append(index_classes_list, index_class_list) # 각 앵커마다 어떤 클래스를 참조했는지 -> 1764개씩 쌓임\n",
    "        index_ground_truth_boxes_list = np.append(index_ground_truth_boxes_list, index_ground_truth_box_list) # 각 이미지별 모든 앵커에 대한 Ground Truth Box를 기록 -> 5011 * 1764\n",
    "\n",
    "        cls_layer_label_list = np.append(cls_layer_label_list, cls_layer_label) # (1764 * 2)개씩 입력\n",
    "        reg_layer_label_list = np.append(reg_layer_label_list, reg_layer_label) # (1764 * 4)개씩 입력\n",
    "    \n",
    "    index_classes_list = np.reshape(index_classes_list, (-1,1764))\n",
    "    index_ground_truth_boxes_list = np.reshape(index_ground_truth_boxes_list, (-1, 1764))\n",
    "    cls_layer_label_list = np.reshape(cls_layer_label_list, (-1, 14,14,18))\n",
    "    reg_layer_label_list = np.reshape(reg_layer_label_list, (-1, 14,14,36))\n",
    "\n",
    "\n",
    "\n",
    "    return image_list, cls_layer_label_list, reg_layer_label_list, Classes_labels_list, Grond_Truth_Boxes_list, index_classes_list\n",
    "    # 입력 데이터셋, (RPN)이 앵커가 Postive인가 Negative인가 Useless인가, 이 앵커가 어떤 Ground Truth Box를 보고 IoU를 계산했는가, 각 이미지에 어떤 객체들이 들어있나, 그 객체들의 박스 좌표는 어디인가, 각 이미지의 앵커들이 참조한 클래스는 어떤건가"
   ]
  },
  {
   "source": [
    "## RPN과 Detector 두 모델에 쓰일 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get image: 100%|██████████| 5011/5011 [00:18<00:00, 277.66it/s]\n",
      "get label: 100%|██████████| 5011/5011 [27:44<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준 \n",
    "anchors, anchors_state = make_anchor(anchor_size, anchor_aspect_ratio) # 앵커 생성 + 유효한 앵커 인덱스 휙득\n",
    "\n",
    "image_list, cls_layer_label_list, reg_layer_label_list, Classes_labels_list, Grond_Truth_Boxes_list, index_classes_list = make_dataset_forRPN(image_file_list, xml_file_list, Classes_inDataSet, anchors, anchors_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image_list.shape :  (5011, 224, 224, 3)\ncls_layer_label_list.shape :  (5011, 14, 14, 18)\nreg_layer_label_list.shape :  (5011, 14, 14, 36)\nClasses_labels_list_len :  5011\nGrond_Truth_Boxes_list_len :  5011\nindex_classes_list.shape :  (5011, 1764)\n"
     ]
    }
   ],
   "source": [
    "print(\"image_list.shape : \", image_list.shape)\n",
    "print(\"cls_layer_label_list.shape : \", cls_layer_label_list.shape)\n",
    "print(\"reg_layer_label_list.shape : \", reg_layer_label_list.shape)\n",
    "print(\"Classes_labels_list_len : \", len(Classes_labels_list))\n",
    "print(\"Grond_Truth_Boxes_list_len : \", len(Grond_Truth_Boxes_list))\n",
    "print(\"index_classes_list.shape : \", index_classes_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "int((32)/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "int((32)/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[61.700598802395206, 32.256, 204.5508982035928, 211.904]]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "Grond_Truth_Boxes_list[4]"
   ]
  },
  {
   "source": [
    "### RPN 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(RPN, self).__init__(name='rpn')\n",
    "    # 레이어 \n",
    "\n",
    "    # 공용 레이어\n",
    "    self.conv1_1 = SharedConvNet.layers[0]\n",
    "    self.conv1_2 = SharedConvNet.layers[1]\n",
    "    self.pooling_1 = SharedConvNet.layers[2]\n",
    "    self.conv2_1 = SharedConvNet.layers[3]\n",
    "    self.conv2_2 = SharedConvNet.layers[4]\n",
    "    self.pooling_2 = SharedConvNet.layers[5]\n",
    "    self.conv3_1 = SharedConvNet.layers[6]\n",
    "    self.conv3_2 = SharedConvNet.layers[7]\n",
    "    self.conv3_3 = SharedConvNet.layers[8]\n",
    "    self.pooling_3 = SharedConvNet.layers[9]\n",
    "    self.conv4_1 = SharedConvNet.layers[10]\n",
    "    self.conv4_2 = SharedConvNet.layers[11]\n",
    "    self.conv4_3 = SharedConvNet.layers[12]\n",
    "    self.pooling_4 = SharedConvNet.layers[13]\n",
    "    self.conv5_1 = SharedConvNet.layers[14]\n",
    "    self.conv5_2 = SharedConvNet.layers[15]\n",
    "    self.conv5_3 = SharedConvNet.layers[16]\n",
    "    # RPN만의 레이어\n",
    "    self.intermediate_layer = tf.keras.layers.Conv2D(512, (3, 3), padding = 'SAME' , name = \"intermediate_layer\", input_shape = (14,14,512))\n",
    "    self.cls_Layer = tf.keras.layers.Conv2D(18, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_1\")\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(36, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_2\")\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    # 정방향 연산\n",
    "    output1_1 = self.conv1_1(inputs)\n",
    "    output1_2 = self.conv1_2(output1_1)\n",
    "    output1_pooling = self.pooling_1(output1_2)\n",
    "    output2_1 = self.conv2_1(output1_pooling)\n",
    "    output2_2 = self.conv2_2(output2_1)\n",
    "    output2_pooling = self.pooling_2(output2_2)\n",
    "    output3_1 = self.conv3_1(output2_pooling)\n",
    "    output3_2 = self.conv3_2(output3_1)\n",
    "    output3_3 = self.conv3_3(output3_2)\n",
    "    output3_pooling = self.pooling_3(output3_3)\n",
    "    output4_1 = self.conv4_1(output3_pooling)\n",
    "    output4_2 = self.conv4_2(output4_1)\n",
    "    output4_3 = self.conv4_3(output4_2)\n",
    "    output4_pooling = self.pooling_4(output4_3)\n",
    "    output5_1 = self.conv5_1(output4_pooling)\n",
    "    output5_2 = self.conv5_2(output5_1)\n",
    "    output5_3 = self.conv5_3(output5_2)\n",
    "    # RPN\n",
    "    feature_map = self.intermediate_layer(output5_3)\n",
    "    cls_Layer_output = self.cls_Layer(feature_map)\n",
    "    reg_Layer_output = self.reg_layer(feature_map)\n",
    "\n",
    "    return cls_Layer_output, reg_Layer_output # 4k, 2k개 아웃풋 반환"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_RPN(model, image_list, reg_layer_label_list, cls_layer_label_list, traning_step, EPOCH):\n",
    "    \n",
    "    if traning_step == 1 : # 처음에는 conv3_1까지만 훈련\n",
    "        model.conv1_1.trainable = False\n",
    "        model.conv1_2.trainable = False\n",
    "        model.conv2_1.trainable = False\n",
    "        model.conv2_2.trainable = False\n",
    "    elif traning_step == 3 : # 두번 째, 그러니까 training step == 3일 때는 sharedConvLayer를 고정\n",
    "        model.conv1_1.trainable = False\n",
    "        model.conv1_2.trainable = False\n",
    "        model.conv2_1.trainable = False\n",
    "        model.conv2_2.trainable = False\n",
    "        model.conv3_1.trainable = False\n",
    "        model.conv3_2.trainable = False\n",
    "        model.conv3_3.trainable = False\n",
    "        model.conv4_1.trainable = False\n",
    "        model.conv4_2.trainable = False\n",
    "        model.conv4_3.trainable = False\n",
    "        model.conv5_1.trainable = False\n",
    "        model.conv5_2.trainable = False\n",
    "        model.conv5_3.trainable = False       \n",
    "\n",
    "    losses = {'output_1' : loss_rpn_cls, 'output_2' : loss_rpn_reg}\n",
    "\n",
    "    Optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=0.001, decay = 0.0005,momentum=0.9,epsilon=1e-10) # momentum = 0.9\n",
    "    model.compile(optimizer=Optimizer, loss=losses, run_eagerly=True)\n",
    "\n",
    "    history = model.fit(np.asarray(image_list), [cls_layer_label_list, reg_layer_label_list], batch_size = 1, epochs = EPOCH)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "source": [
    "## RPN 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "regularizer = tf.keras.regularizers.l2(0.0005)\n",
    "\n",
    "RPN_Model = RPN(initializer, regularizer, SharedConvNet)\n",
    "RPN_Model.run_eagerly = True # 모델 내부 함수에서 쉽게 연산할 수 있게 설정"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5011/5011 [==============================] - 2649s 528ms/step - loss: 0.3247 - output_1_loss: 0.2486 - output_2_loss: 0.0761\n"
     ]
    }
   ],
   "source": [
    "RPN_Model, history = training_RPN(RPN_Model, image_list, reg_layer_label_list, cls_layer_label_list, 1, 1)"
   ]
  },
  {
   "source": [
    "### RPN에서 값 얻어내기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_output(cls_layer_output, reg_layer_output):\n",
    "\n",
    "    for y in range(0, 14):\n",
    "        for x in range(0, 14):\n",
    "                for i in range(0, len(anchor_size)):\n",
    "\n",
    "                    for j in range(0, len(anchor_aspect_ratio)):\n",
    "                        score = [cls_layer_output[y][x][2*(3*i+j)]  , cls_layer_output[i][2*(3*i+j) + 1]] # 오브젝트다 vs 아니다 2개 항목에 대한 스코어 저장(각 앵커당)\n",
    "                        score = tf.nn.softmax(score)\n",
    "                        cls_layer_output[y][x][2*(3*i+j)] = score[0]\n",
    "                        cls_layer_output[y][x][2*(3*i+j) + 1] = score[1]\n",
    "                        \n",
    "                        center_x = 8 + 16 * x \n",
    "                        center_y = 8 + 16 * y\n",
    "                        w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                        h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                        # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                        reg_layer_output[y][x][4*(3*i+j)] = center_x + reg_layer_output[y][x][4*(3*i+j)]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 1] = center_y + reg_layer_output[y][x][4*(3*i+j) + 1]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 2] = w + reg_layer_output[y][x][4*(3*i+j) + 2]\n",
    "                        reg_layer_output[y][x][4*(3*i+j) + 3] = h + reg_layer_output[y][x][4*(3*i+j) + 3]\n",
    "\n",
    "    cls_layer_output = np.reshape(cls_layer_output, (-1,2))\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (-1,4))\n",
    "\n",
    "    return cls_layer_output, reg_layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Output_RPN(RPN_Model, image_list) :\n",
    "    cls_layer_output_list = np.array([])\n",
    "    reg_layer_output_list = np.array([])\n",
    "    for i in tqdm(range(0, len(image_list)), desc = \"get_output\"):\n",
    "        cls_layer_output, reg_layer_output = RPN_Model(np.expand_dims(image_list[i], axis=0))\n",
    "\n",
    "        cls_layer_output = cls_layer_output[0].numpy()\n",
    "        reg_layer_output = reg_layer_output[0].numpy()\n",
    "\n",
    "        # 가공하자\n",
    "        cls_layer_output, reg_layer_output = Process_output(reg_layer_output, cls_layer_output)\n",
    "\n",
    "        cls_layer_output_list = np.append(cls_layer_output_list, cls_layer_output)\n",
    "        reg_layer_output_list = np.append(reg_layer_output_list, reg_layer_output)\n",
    "    \n",
    "    cls_layer_output_list = np.reshape(cls_layer_output_list, (-1,1764,2))\n",
    "    reg_layer_output_list = np.reshape(reg_layer_output_list, (-1,1764,4))\n",
    "\n",
    "    return cls_layer_output_list, reg_layer_output_list"
   ]
  },
  {
   "source": [
    "cls_layer_output_list, reg_layer_output_list = Get_Output_RPN(RPN_Model, image_list)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get_output: 100%|██████████| 5011/5011 [23:02<00:00,  3.63it/s]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Fast R-CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 입력값 : 공유 특성맵 + RoI\n",
    "### 출력값 : 예측한 객체 종류, 객체의 좌표(r, c, h, w). (r,c)는 왼쪽위 좌표고 h,w는 너비, 높이"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detector 구조\n",
    "# 입력 이미지 -> 특성맵.\n",
    "# 특성맵 + RoI -> RoI위치만 추출. 그러면 나한테 있는 데이터는 RoI 위치별 특성맵, 각 RoI의 Truth Box, Classes\n",
    "# Roi 특성맵을 연산을 통해 cls, loc 휙득\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(cls_layer_output, reg_layer_output, reg_layer_label, index_classes): # 한 이미지에서 뽑아낸 RPN의 출력값 2개, 이미지에 있는 모든 앵커들이 참조한 Ground Truth Box, 그 Ground Truth Box의 Class\n",
    "    nms_RoI_List = [] # 각 이미지마다 RoI 갯수가 다르다. 그래서 리스트로 저장한다. 리스트 안에 리스트 저장하는 방식으로 len = 5011인 리스트를 생성할거다 \n",
    "    nms_GroundTruthBox_List = [] # NMS에 들어간 RoI가 참조한 Ground Truth Box \n",
    "    nms_Classes_List = []\n",
    "    for i in range(0, len(cls_layer_output)) :\n",
    "        if cls_layer_output[i][0] > 0.7 : # 해당 앵커의 object score가 0.7을 넘겼으면\n",
    "            # 저장\n",
    "            nms_RoI_List.append(reg_layer_output[i]) # RoI\n",
    "            nms_GroundTruthBox_List.append(reg_layer_output_list[i]) # Ground Truth Box\n",
    "            # 그 Box가 가리키는 클래스 인덱스\n",
    "            nms_Classes_List.append(index_classes[i]\n",
    "\n",
    "\n",
    "\n",
    "    return nms_RoI_List, nms_GroundTruthBox_List, nms_Classes_List # 선별한 애들을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nms_list(cls_layer_output_list, reg_layer_output_list, reg_layer_label_list, index_classes_list) :\n",
    "    # RPN output 2개, 이미지에서 각 앵커(1764)들이 어떤 Ground Truth Box보고 IoU 계산했는지, 이미지에서 각 앵커(1764)들이 어떤 '클래스'의 Ground Truth Box보고 IoU 계산했는지\n",
    "    # Detector 훈련에 필요한 데이터를 얻는 곳이다\n",
    "    \n",
    "    NMS_RoIs_List = [] # 전체 입력 이미지의 RoI를 이미지별로 저장(리스트 안에 리스트)\n",
    "    NMS_GroundTruthBoxes_List = []\n",
    "    NMS_Classes_List = []\n",
    "\n",
    "    for i in tqdm(range(0, len(cls_layer_output_list)), desc = \"get_RoI\"): # 5011개에 대한 nms 구한다\n",
    "        nms_RoI_List, nms_GroundTruthBox_List, nms_Classes_List = nms(cls_layer_output_list[i], reg_layer_output_list[i], reg_layer_label_list[i], index_classes_list[i])\n",
    "        NMS_RoIs_List.append(nms_RoI_List)\n",
    "        NMS_GroundTruthBoxes_List.append(NMS_GroundTruthBoxes_List)\n",
    "        NMS_Classes_List.append(nms_Classes_List)\n",
    "\n",
    "    return NMS_RoIs_List, NMS_GroundTruthBoxes_List, NMS_Classes_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_RoI(nms_RoI_List) :\n",
    "    # RoI_list : (-1, 4) <- (x,y,w,h)\n",
    "    RoI_list_forDetector = np.array([])\n",
    "    for i in range(0, len(RoI_list)) :\n",
    "        r = RoI_list[i][0] - (RoI_list[i][2]/2)\n",
    "        c = RoI_list[i][1] - (RoI_list[i][3]/2)\n",
    "        w = RoI_list[i][2]\n",
    "        h = RoI_list[i][3]\n",
    "\n",
    "        roi_forDetector = np.array([r,c,w,h])\n",
    "        RoI_list_forDetector = np.append(RoI_list_forDetector, roi_forDetector)\n",
    "    RoI_list_forDetector = np.reshape(RoI_list_forDetector, (-1,4)) \n",
    "\n",
    "    return RoI_list_forDetector"
   ]
  },
  {
   "source": [
    "## Detector 훈련에 필요한 데이터셋 휙득"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMS_RoIs_List, NMS_GroundTruthBoxes_List, NMS_Classes_List = get_nms_list(cls_layer_output_list, reg_layer_output_list, reg_layer_label_list, index_classes_list)"
   ]
  },
  {
   "source": [
    "## Detector 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(Detector, self).__init__(name='rpn')\n",
    "    # 레이어 \n",
    "    # 공용 레이어\n",
    "    self.conv1_1 = SharedConvNet.layers[0]\n",
    "    self.conv1_2 = SharedConvNet.layers[1]\n",
    "    self.pooling_1 = SharedConvNet.layers[2]\n",
    "    self.conv2_1 = SharedConvNet.layers[3]\n",
    "    self.conv2_2 = SharedConvNet.layers[4]\n",
    "    self.pooling_2 = SharedConvNet.layers[5]\n",
    "    self.conv3_1 = SharedConvNet.layers[6]\n",
    "    self.conv3_2 = SharedConvNet.layers[7]\n",
    "    self.conv3_3 = SharedConvNet.layers[8]\n",
    "    self.pooling_3 = SharedConvNet.layers[9]\n",
    "    self.conv4_1 = SharedConvNet.layers[10]\n",
    "    self.conv4_2 = SharedConvNet.layers[11]\n",
    "    self.conv4_3 = SharedConvNet.layers[12]\n",
    "    self.pooling_4 = SharedConvNet.layers[13]\n",
    "    self.conv5_1 = SharedConvNet.layers[14]\n",
    "    self.conv5_2 = SharedConvNet.layers[15]\n",
    "    self.conv5_3 = SharedConvNet.layers[16]\n",
    "\n",
    "    Classify_layer_initializer tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "    Box_regression_layer_initializer tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None)\n",
    "\n",
    "    # RoI Pooling : H*W(7*7)에 맞게 입력 특성맵을 pooling. RoI에 해당하는 영역을 7*7로 Pooling한다. \n",
    "    self.RoIPoolingLayer = tf.keras.layers # 7*7로 만들어버린다. \n",
    "    self.Flatten_layer = tf.keras.layers.Flatten() # 여기서 4096개가 되어야한다.\n",
    "    self.Classify_layer = tf.keras.layers.Dense(21, activation='softmax', kernel_initializer = Classify_layer_initializer, name = \"output_1\")\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(84, activation= None, kernel_initializer = Box_regression_layer_initializer, name = \"output_2\")\n",
    "    \n",
    "  def call(self, inputs): # input으로 리스트를 넣으면 된다.\n",
    "\n",
    "    Input_Image = inputs[0]\n",
    "    RoI_List = inputs[1]\n",
    "\n",
    "    # 정방향 연산\n",
    "    output1_1 = self.conv1_1(Input_Image)\n",
    "    output1_2 = self.conv1_2(output1_1)\n",
    "    output1_pooling = self.pooling_1(output1_2)\n",
    "    output2_1 = self.conv2_1(output1_pooling)\n",
    "    output2_2 = self.conv2_2(output2_1)\n",
    "    output2_pooling = self.pooling_2(output2_2)\n",
    "    output3_1 = self.conv3_1(output2_pooling)\n",
    "    output3_2 = self.conv3_2(output3_1)\n",
    "    output3_3 = self.conv3_3(output3_2)\n",
    "    output3_pooling = self.pooling_3(output3_3)\n",
    "    output4_1 = self.conv4_1(output3_pooling)\n",
    "    output4_2 = self.conv4_2(output4_1)\n",
    "    output4_3 = self.conv4_3(output4_2)\n",
    "    output4_pooling = self.pooling_4(output4_3)\n",
    "    output5_1 = self.conv5_1(output4_pooling)\n",
    "    output5_2 = self.conv5_2(output5_1)\n",
    "    output5_3 = self.conv5_3(output5_2)\n",
    "    # Detector\n",
    "    # Pooling\n",
    "\n",
    "\n",
    "    return cls_Layer_output, reg_Layer_output # 4k, 2k개 아웃풋 반환"
   ]
  },
  {
   "source": [
    "## Loss 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_detector_loc(y_true, y_pred):\n",
    "    # y_true : 해당 이미지 내에 있는 모든 클래스의 \n",
    "    return loss"
   ]
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "Detector_Model = Detector(SharedConvNet)\n",
    "RPN_Model.run_eagerly = True # 모델 내부 함수에서 쉽게 연산할 수 있게 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Detector(model, , traning_step, EPOCH):\n",
    "\n",
    "    losses = {'output_1' : 'categorical_crossentropy', 'output_2' : loss_detector_loc}\n",
    "\n",
    "    model.compile(optimizer=Optimizer, loss=losses, run_eagerly=True)\n",
    "\n",
    "    history = model.fit(np.asarray(image_list), [cls_layer_label_list, reg_layer_label_list], batch_size = 1, epochs = EPOCH)\n",
    "\n",
    "    return model, history\n"
   ]
  }
 ]
}
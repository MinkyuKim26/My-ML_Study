{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd083d047fb679ad76f0fa37ccf88a1be1bd896d909bea0ccb42113759f7e80378e",
   "display_name": "Python 3.6.13 64-bit ('minguinho': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Faster R-CNN 구현\n",
    "### 텐서플로우 기반\n",
    "### 주석은 한글 위주로 작성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "source": [
    "## 훈련 이미지 가져오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/JPEGImages'\n",
    "train_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/Annotations'\n",
    "\n",
    "test_x_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/JPEGImages'\n",
    "test_y_path = '/Users/minguinho/Documents/AI_Datasets/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5011\n4952\n"
     ]
    }
   ],
   "source": [
    "list_train_x = sorted([x for x in glob.glob(train_x_path + '/**')])    \n",
    "list_train_y = sorted([x for x in glob.glob(train_y_path + '/**')]) \n",
    "\n",
    "list_test_x = sorted([x for x in glob.glob(test_x_path + '/**')])    \n",
    "list_test_y = sorted([x for x in glob.glob(test_y_path + '/**')]) \n",
    "\n",
    "print(len(list_train_x))\n",
    "print(len(list_test_x))"
   ]
  },
  {
   "source": [
    "## 훈련용 데이터로 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 테스트용\n",
    "def get_image_224(image_file_path) :\n",
    "    image = cv2.imread(image_file_path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력용 이미지 생성. 224, 224로 변환시키고 채널 값(0~255)를 0~1 사이의 값으로 정규화 시켜줌\n",
    "def make_input(image_file_list): \n",
    "    images_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_file_list)), desc=\"get image\") :\n",
    "        \n",
    "        image = cv2.imread(image_file_list[i])\n",
    "        image = cv2.resize(image, (224, 224))/255\n",
    "        \n",
    "        images_list.append(image)\n",
    "    \n",
    "    return np.asarray(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPN 훈련할 때는 객체 종류를 알 필요가 없다. 왜냐하면 RPN은 객체가 존재하는 박스 위치만 알면 되기 때문이다. \n",
    "# 그러면 객체 종류를 어디서 알아야 하는 것일까. 바로 Detector를 훈련시킬 때 필요하다. \n",
    "# 존재하는 객체 종류를 알아내자\n",
    "def get_Classes_inImage(xml_file_list):\n",
    "    classes = []\n",
    "\n",
    "    for xml_file_path in xml_file_list: \n",
    "\n",
    "        f = open(xml_file_path)\n",
    "        xml_file = xmltodict.parse(f.read())\n",
    "        # 사진에 객체가 여러개 있을 경우\n",
    "        try: \n",
    "            for obj in xml_file['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) # 들어있는 객체 종류를 알아낸다\n",
    "        # 사진에 객체가 하나만 있을 경우\n",
    "        except TypeError as e: \n",
    "            classes.append(xml_file['annotation']['object']['name'].lower()) \n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) # set은 중복된걸 다 제거하고 유니크한? 아무튼 하나만 가져온다. 그걸 리스트로 만든다\n",
    "    classes.sort() # 정렬\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "source": [
    "## RPN 훈련을 위한 데이터셋 생성에 필요한 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지에 어떤 Ground Truth Box가 있는지\n",
    "def get_label_fromImage_RPN(xml_file_path): # xml_file_path은 파일 하나의 경로를 나타낸다\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    xml_file = xmltodict.parse(f.read()) \n",
    "\n",
    "    # 우선 원래 이미지 크기를 얻는다. 왜냐하면 앵커는 224*224 기준으로 만들었는데 원본 이미지는 224*224가 아니기 때문.\n",
    "    # 224*224에 맞게 줄일려고 하는거다\n",
    "    Image_Height = float(xml_file['annotation']['size']['height'])\n",
    "    Image_Width  = float(xml_file['annotation']['size']['width'])\n",
    "\n",
    "    Ground_Truth_Box_list = [] \n",
    "\n",
    "    # multi-objects in image\n",
    "    try:\n",
    "        for obj in xml_file['annotation']['object']:\n",
    "            \n",
    "            # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "            x_min = float(obj['bndbox']['xmin']) \n",
    "            y_min = float(obj['bndbox']['ymin'])\n",
    "            x_max = float(obj['bndbox']['xmax']) \n",
    "            y_max = float(obj['bndbox']['ymax'])\n",
    "\n",
    "            # 224*224에 맞게 변형시켜줌\n",
    "            x_min = float((224/Image_Width)*x_min)\n",
    "            y_min = float((224/Image_Height)*y_min)\n",
    "            x_max = float((224/Image_Width)*x_max)\n",
    "            y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "            Ground_Truth_Box = [x_min, y_min, x_max, y_max]\n",
    "            Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    # single-object in image\n",
    "    except TypeError as e : \n",
    "        # 박스 좌표(왼쪽 위, 오른쪽 아래) 얻기\n",
    "        x_min = float(xml_file['annotation']['object']['bndbox']['xmin']) \n",
    "        y_min = float(xml_file['annotation']['object']['bndbox']['ymin']) \n",
    "        x_max = float(xml_file['annotation']['object']['bndbox']['xmax']) \n",
    "        y_max = float(xml_file['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "        # 224*224에 맞게 변형시켜줌\n",
    "        x_min = float((224/Image_Width)*x_min)\n",
    "        y_min = float((224/Image_Height)*y_min)\n",
    "        x_max = float((224/Image_Width)*x_max)\n",
    "        y_max = float((224/Image_Height)*y_max)\n",
    "\n",
    "        Ground_Truth_Box = [x_min, y_min, x_max, y_max]  \n",
    "        Ground_Truth_Box_list.append(Ground_Truth_Box)\n",
    "\n",
    "    \n",
    "    Ground_Truth_Box_list = np.asarray(Ground_Truth_Box_list)\n",
    "    Ground_Truth_Box_list = np.reshape(Ground_Truth_Box_list, (-1, 4))\n",
    "\n",
    "    return Ground_Truth_Box_list # 이미지에 있는 Ground Truth Box 리스트 받기(numpy)\n"
   ]
  },
  {
   "source": [
    "## RPN - 앵커 준비\n",
    "#### 입력 이미지 기준으로 앵커를 생성한다.\n",
    "#### 풀링을 3번 하므로 2^3 = 8이니 (8, 8)부터 (16, 8)...등 8픽셀식 중심 좌표를 옮겨가며 앵커들을 k개씩 생성한다\n",
    "#### 생성한 앵커들 중 사용할 가치가 있는 앵커를 걸러낸다(이미지 범위를 벗어나지 않는 앵커들만 선정)\n",
    "#### 선정한 앵커 중 실제 물체의 box와 얼마나 곂치는지(IoU) 계산해본다. 확실히 겹친다 하는 애들을 Positive 앵커로, 거의 안겹친다 하는 애들은 Negataive 앵커로 선정한다. 애매한 애들은 거른다.\n",
    "#### 이렇게 생성된 앵커들로 미니배치를 생성한다. Positive 128개, Negative 128개로 만드는게 ideal한 구성이긴 한데 Positive한 앵커가 별로 없다. 그래서 Positive를 128개 못채웠으면 Negataive 앵커로 채워준다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커 생성 함수. \n",
    "def make_anchor(anchor_size, anchor_aspect_ratio) :\n",
    "    # 입력 이미지(그래봤자 224*224긴 하지만)에 맞춰 앵커를 생성해보자 \n",
    "\n",
    "    anchors = [] # [x,y,w,h]로 이루어진 리스트 \n",
    "    anchors_state = [] # 이 앵커를 훈련에 쓸건가? 각 앵커별로 사용 여부를 나타낸다. \n",
    "\n",
    "    # 앵커 중심좌표 간격\n",
    "    interval_x = 16\n",
    "    interval_y = 16\n",
    "    Center_max_x = 208 # 224 - 16, 중심좌표가 224가 될 수는 없다.\n",
    "    Center_max_y = 208 # 224 - 16\n",
    "\n",
    "    # 2단 while문 생성\n",
    "    x = 8\n",
    "    y = 8\n",
    "    index_count = 0\n",
    "    while(y <= 224): # 8~208 = 14개 \n",
    "        while(x <= 224): # 8~208 = 14개 \n",
    "            # k개의 앵커 생성. 여기서 k = len(anchor_size) * len(anchor_aspect_ratio)다\n",
    "            for i in range(0, len(anchor_size)) : \n",
    "                for j in range(0, len(anchor_aspect_ratio)) :\n",
    "                    anchor_width = anchor_aspect_ratio[j][0] * anchor_size[i]\n",
    "                    anchor_height = anchor_aspect_ratio[j][1] * anchor_size[i]\n",
    "\n",
    "                    anchor = [x, y, anchor_width, anchor_height]\n",
    "                    anchors.append(anchor)\n",
    "                    # 앵커가 이미지 경계선을 넘나드나? 필터링\n",
    "                    if((x - (anchor_width/2) >= 0) and (y - (anchor_height/2) >= 0) and\n",
    "                    (x + (anchor_width/2) <= 224) and (y + (anchor_height/2) <= 224)):\n",
    "                        # 경계 안에 있으면 1\n",
    "                        anchors_state.append(int(1))\n",
    "                    else :\n",
    "                        anchors_state.append(int(0))\n",
    "            x = x + interval_x \n",
    "        y = y + interval_y\n",
    "        x = 8\n",
    "    return np.asarray(anchors), np.asarray(anchors_state) # 넘파이로 반환"
   ]
  },
  {
   "source": [
    "### IoU 계산 후 Positive, Negative 앵커 분류"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앵커들을 Positive, Negative 앵커로 나누고 각 앵커가 참고한 Ground Truth Box와 Class를 반환하자\n",
    "# RPN에는 '어떤 클래스인가?'는 알 필요가 없다. '객체인가 아닌가'이거 하나만 필요할 뿐. \n",
    "def align_anchor(anchors, anchors_state, Ground_Truth_Box_list):\n",
    "\n",
    "    # 각 앵커는 해당 위치에서 구한 여러가지 Ground truth Box와의 ioU 중 제일 높은거만 가져온다. \n",
    "    IoU_List = np.array([])\n",
    "    Ground_truth_box_Highest_IoU_List = [] # 각 앵커가 어떤 Ground Truth Box를 보고 IoU를 계산했는가?\n",
    "\n",
    "    #start = time.time()\n",
    "\n",
    "    for i in range(0, len(anchors)):\n",
    "        if anchors_state[i] == 0 :\n",
    "            IoU_List = np.append(IoU_List, 0)\n",
    "            Ground_truth_box_Highest_IoU_List.append([0,0,0,0])\n",
    "\n",
    "            if i % 9 == 8 :\n",
    "                IoU_List_inOneSpot = IoU_List[i-8:i+1]\n",
    "                for num in list(range(i-8, i + 1)):\n",
    "                    if IoU_List[num] > 0.7 or (max(IoU_List_inOneSpot) == IoU_List[num] and IoU_List[num] >= 0.3): # positive anchor\n",
    "                        anchors_state[num] = 2\n",
    "                    elif IoU_List[num] < 0.3 : # negative anchor\n",
    "                        anchors_state[num] = 1\n",
    "                    else: # 애매한 앵커들\n",
    "                        anchors_state[num] = 0    \n",
    "        else:\n",
    "            anchor_minX = anchors[i][0] - (anchors[i][2]/2)\n",
    "            anchor_minY = anchors[i][1] - (anchors[i][3]/2)\n",
    "            anchor_maxX = anchors[i][0] + (anchors[i][2]/2)\n",
    "            anchor_maxY = anchors[i][1] + (anchors[i][3]/2)\n",
    "\n",
    "            anchor = [anchor_minX, anchor_minY, anchor_maxX, anchor_maxY]\n",
    "\n",
    "            # 연산 속도 때문에 Box대신 Ground Truth Box의 인덱스를 저장\n",
    "            IoU_max = 0\n",
    "            ground_truth_box_Highest_IoU = [0,0,0,0]\n",
    "\n",
    "            for j in range(0, len(Ground_Truth_Box_list)):\n",
    "\n",
    "                ground_truth_box = Ground_Truth_Box_list[j]\n",
    "\n",
    "                InterSection_min_x = max(anchor[0], ground_truth_box[0])\n",
    "                InterSection_min_y = max(anchor[1], ground_truth_box[1])\n",
    "\n",
    "                InterSection_max_x = min(anchor[2], ground_truth_box[2])\n",
    "                InterSection_max_y = min(anchor[3], ground_truth_box[3])\n",
    "\n",
    "                InterSection_Area = 0\n",
    "\n",
    "                if (InterSection_max_x - InterSection_min_x + 1) >= 0 and (InterSection_max_y - InterSection_min_y + 1) >= 0 :\n",
    "                    InterSection_Area = (InterSection_max_x - InterSection_min_x + 1) * (InterSection_max_y - InterSection_min_y + 1)\n",
    "\n",
    "                box1_area = (anchor[2] - anchor[0]) * (anchor[3] - anchor[1])\n",
    "                box2_area = (ground_truth_box[2] - ground_truth_box[0]) * (ground_truth_box[3] - ground_truth_box[1])\n",
    "                Union_Area = box1_area + box2_area - InterSection_Area\n",
    "\n",
    "                IoU = (InterSection_Area/Union_Area)\n",
    "                if IoU > IoU_max :\n",
    "                    IoU_max = IoU\n",
    "                    ground_truth_box_Highest_IoU = ground_truth_box\n",
    "\n",
    "            IoU_List = np.append(IoU_List, IoU_max)\n",
    "            Ground_truth_box_Highest_IoU_List.append(ground_truth_box_Highest_IoU)\n",
    "\n",
    "            # 한 위치에 9개의 앵커 존재 -> 9개 앵커에 대한 IoU를 계산할 때마다 모아서 Positive, Negative 앵커 분류\n",
    "            if i % 9 == 8 :\n",
    "                IoU_List_inOneSpot = IoU_List[i-8:i+1]\n",
    "                for num in list(range(i-8, i + 1)):\n",
    "                    if IoU_List[num] > 0.7 or (max(IoU_List_inOneSpot) == IoU_List[num] and IoU_List[num] >= 0.3): # positive anchor\n",
    "                        anchors_state[num] = 2\n",
    "                    elif IoU_List[num] < 0.3 : # negative anchor\n",
    "                        anchors_state[num] = 1\n",
    "                    else: # 애매한 앵커들\n",
    "                        anchors_state[num] = 0     \n",
    "\n",
    "    Ground_truth_box_Highest_IoU_List = np.asarray(Ground_truth_box_Highest_IoU_List)\n",
    "    Ground_truth_box_Highest_IoU_List = np.reshape(Ground_truth_box_Highest_IoU_List, (-1, 4))\n",
    "            \n",
    "    return anchors_state, Ground_truth_box_Highest_IoU_List # 각 앵커의 상태, (모든)앵커가 IoU 계산에 참조한 Ground Truth Box"
   ]
  },
  {
   "source": [
    "## RPN을 위한 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPN훈련을 위한 데이터셋. \n",
    "# RPN과 Detector는 별개의 모델이다. 즉, 두 모델을 훈련시킬 때 필요한 데이터셋은 따로따로 만들어야한다. \n",
    "# 여기선 RPN 훈련에 필요한 데이터만 만들거다. \n",
    "def make_dataset_forRPN(input_list) :\n",
    "    image_file_list = input_list[0]\n",
    "    xml_file_list = input_list[1]\n",
    "    anchors = input_list[2]\n",
    "    anchors_state = input_list[3]\n",
    "\n",
    "    image_list = make_input(image_file_list) # 입력\n",
    "    # 출력\n",
    "    cls_layer_label_list = np.array([])\n",
    "    reg_layer_label_list = np.array([])\n",
    "\n",
    "    # 값 계속 생성하는거 막기위한 변수\n",
    "    cls_label_forPositive = np.array([1.0,0.0])\n",
    "    cls_label_forNegative = np.array([0.0,1.0])\n",
    "    cls_label_forUseless  = np.array([0.0,0.0])\n",
    "\n",
    "    reg_label_forNotPositive = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for i in tqdm(range(0, len(xml_file_list)), desc=\"get label\"): # 각 이미지별로 데이터셋 생성(5011개)\n",
    "\n",
    "        anchors_state_for = anchors_state # anchors_state는 매 사진마다 다르니까 원본값(?)을 복사해서 쓴다. \n",
    "        Ground_Truth_Box_list = get_label_fromImage_RPN(xml_file_list[i]) # 여기서는 Ground Truth Box에 대한 정보만 필요하다\n",
    "        anchors_state_for, Ground_truth_box_Highest_IoU_List = align_anchor(anchors, anchors_state_for, Ground_Truth_Box_list)\n",
    "        # 어떤 앵커가 Pos, neg 앵커인지, (모든)앵커가 참조한 ground truth box는 뭔지\n",
    "    \n",
    "        #start = time.time()\n",
    "        # 연산 시간 때문에 생성(1761개치 모아놨다가 한 번에 추가하기)\n",
    "        cls_layer_label_list_for = np.array([])\n",
    "        reg_layer_label_list_for = np.array([])\n",
    "        for j in range(0, len(anchors_state_for)) :\n",
    "            if anchors_state_for[j] == 2 : # positive\n",
    "                Ground_truth_box = np.array([Ground_truth_box_Highest_IoU_List[j][0] + Ground_truth_box_Highest_IoU_List[j][2]/2, Ground_truth_box_Highest_IoU_List[j][1] + Ground_truth_box_Highest_IoU_List[j][2]/2, Ground_truth_box_Highest_IoU_List[j][2] - Ground_truth_box_Highest_IoU_List[j][0], Ground_truth_box_Highest_IoU_List[j][3] - Ground_truth_box_Highest_IoU_List[j][1]])\n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forPositive)\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, Ground_truth_box) # IoU계산에 참조한(pos, neg 분류에 기여한) Ground Truth Box의 정보 휙득\n",
    "            elif anchors_state_for[j] == 1 : # negative는 Ground Truth Box 정보가 필요없으니 [0,0,0,0]을 넣는다. \n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forNegative) # 해당 앵커 output이 [0,1] -> negative\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, reg_label_forNotPositive)\n",
    "            else : \n",
    "                cls_layer_label_list_for = np.append(cls_layer_label_list_for, cls_label_forUseless) # 해당 앵커 output이 [0.5, 0.5] -> 무의미한 값\n",
    "                reg_layer_label_list_for = np.append(reg_layer_label_list_for, reg_label_forNotPositive)\n",
    "        \n",
    "        # 넘파이 배열로 변환\n",
    "        cls_layer_label_list = np.append(cls_layer_label_list, cls_layer_label_list_for)\n",
    "        reg_layer_label_list = np.append(reg_layer_label_list, reg_layer_label_list_for)\n",
    "        #print(\"\\nmaking label time :\", time.time() - start)\n",
    "\n",
    "    # 논문에서 말한 출력값 크기에 맞게 reshape\n",
    "    cls_layer_label_list = np.reshape(cls_layer_label_list, (-1, 1764, 2)) \n",
    "    reg_layer_label_list = np.reshape(reg_layer_label_list, (-1, 1764, 4))\n",
    "\n",
    "    image_list = image_list.astype('float32')\n",
    "\n",
    "    return image_list, cls_layer_label_list, reg_layer_label_list # 훈련 데이터들(입, 출력)"
   ]
  },
  {
   "source": [
    "## RPN에 쓰일 데이터셋 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del image_file_list, xml_file_list, anchor_size, anchor_aspect_ratio, anchors, anchors_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get image: 100%|██████████| 5011/5011 [00:20<00:00, 240.93it/s]\n",
      "get label: 100%|██████████| 5011/5011 [15:54<00:00,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# 파일 리스트\n",
    "image_file_list = sorted([x for x in glob.glob(train_x_path + '/**')])\n",
    "xml_file_list = sorted([x for x in glob.glob(train_y_path + '/**')])\n",
    "\n",
    "anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준 \n",
    "anchors, anchors_state = make_anchor(anchor_size, anchor_aspect_ratio) # 앵커 생성 + 유효한 앵커 인덱스 휙득\n",
    "\n",
    "image_list, cls_layer_label_list, reg_layer_label_list = make_dataset_forRPN([image_file_list, xml_file_list, anchors, anchors_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5011, 1764, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "cls_layer_label_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image_list.shape :  (5011, 224, 224, 3)\ncls_layer_label_list.shape :  (5011, 1764, 2)\nreg_layer_label_list.shape :  (5011, 1764, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"image_list.shape : \", image_list.shape)\n",
    "print(\"cls_layer_label_list.shape : \", cls_layer_label_list.shape)\n",
    "print(\"reg_layer_label_list.shape : \", reg_layer_label_list.shape)"
   ]
  },
  {
   "source": [
    "## RPN 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(tf.keras.Model):\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(RPN, self).__init__(name='rpn')\n",
    "\n",
    "    self.anchors = anchors\n",
    "    self.anchor_size = anchor_size\n",
    "    self.anchor_aspect_ratio = anchor_aspect_ratio\n",
    "    self.Optimizers = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9)\n",
    "\n",
    "    # 공용 레이어\n",
    "    self.conv1_1 = SharedConvNet.layers[0]\n",
    "    self.conv1_2 = SharedConvNet.layers[1]\n",
    "    self.pooling_1 = SharedConvNet.layers[2]\n",
    "\n",
    "    self.conv2_1 = SharedConvNet.layers[3]\n",
    "    self.conv2_2 = SharedConvNet.layers[4]\n",
    "    self.pooling_2 = SharedConvNet.layers[5]\n",
    "\n",
    "    self.conv3_1 = SharedConvNet.layers[6]\n",
    "    self.conv3_2 = SharedConvNet.layers[7]\n",
    "    self.conv3_3 = SharedConvNet.layers[8]\n",
    "    self.pooling_3 = SharedConvNet.layers[9]\n",
    "\n",
    "    self.conv4_1 = SharedConvNet.layers[10]\n",
    "    self.conv4_2 = SharedConvNet.layers[11]\n",
    "    self.conv4_3 = SharedConvNet.layers[12]\n",
    "    self.pooling_4 = SharedConvNet.layers[13]\n",
    "\n",
    "    self.conv5_1 = SharedConvNet.layers[14]\n",
    "    self.conv5_2 = SharedConvNet.layers[15]\n",
    "    self.conv5_3 = SharedConvNet.layers[16]\n",
    "    \n",
    "    # RPN만의 레이어\n",
    "    self.intermediate_layer = tf.keras.layers.Conv2D(512, (3, 3), padding = 'SAME' , activation = 'relu', name = \"intermediate_layer\", dtype='float32')\n",
    "    self.cls_Layer = tf.keras.layers.Conv2D(18, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_1\", dtype='float32')\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(36, (1, 1), kernel_initializer=initializer, padding = 'SAME' ,kernel_regularizer = regularizer, name = \"output_2\", dtype='float32')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    # 정방향 연산\n",
    "    # inputs = np.array(inputs)\n",
    "    output = self.conv1_1(inputs)\n",
    "    output = self.conv1_2(output)\n",
    "    output = self.pooling_1(output)\n",
    "\n",
    "    output = self.conv2_1(output)\n",
    "    output = self.conv2_2(output)\n",
    "    output = self.pooling_2(output)\n",
    "\n",
    "    output = self.conv3_1(output)\n",
    "    output = self.conv3_2(output)\n",
    "    output = self.conv3_3(output)\n",
    "    output = self.pooling_3(output)\n",
    "\n",
    "    output = self.conv4_1(output)\n",
    "    output = self.conv4_2(output)\n",
    "    output = self.conv4_3(output)\n",
    "    output = self.pooling_4(output)\n",
    "\n",
    "    output = self.conv5_1(output)\n",
    "    output = self.conv5_2(output)\n",
    "    shared_output = self.conv5_3(output)\n",
    "    # RPN\n",
    "    shared_output = self.intermediate_layer(shared_output)\n",
    "    cls_Layer_output = self.cls_Layer(shared_output)\n",
    "    reg_Layer_output = self.reg_layer(shared_output) \n",
    "\n",
    "    return cls_Layer_output, reg_Layer_output # (1,14,14,18), (1,14,14,36) 반환\n",
    "\n",
    "  def get_minibatch_index(self, cls_layer_output): # 라벨이니까 (1764,2) 넘파이 온다\n",
    "\n",
    "    index_list = np.array([])\n",
    "      \n",
    "    index_list = np.zeros(1764) # 각 앵커가 미니배치 뽑혔나 안뽑혔나\n",
    "    index_pos = np.array([])\n",
    "    index_neg = np.array([])\n",
    "    # cls_layer_output을 보고 긍정, 부정 앵커 분류. 그렇게 데이터셋을 구성함\n",
    "    for i in range(0, 1764):\n",
    "        if cls_layer_output[i][0] == 1.0 : # positive anchor\n",
    "            index_pos = np.append(index_pos, i)\n",
    "        elif cls_layer_output[i][0] == 0.0 : # negative anchor\n",
    "            index_neg = np.append(index_neg, i)\n",
    "\n",
    "    max_for = min([128, len(index_pos)])\n",
    "    ran_list = random.sample(range(0, len(index_pos)), max_for)\n",
    "\n",
    "    for i in range(0, len(ran_list)) :\n",
    "        index = int(index_pos[ran_list[i]])\n",
    "        index_list[index] = 1\n",
    "\n",
    "    ran_list = random.sample(range(0, len(index_neg)), 256 - max_for) # 랜덤성 증가?를 위해 또다시 난수 생성\n",
    "    for i in range(0, len(ran_list)) :\n",
    "        index = int(index_neg[ran_list[i]])\n",
    "        index_list[index] = 1\n",
    "\n",
    "    return index_list # (1764,1) <- 1,0으로 이루어진 boolean 넘파이 배열\n",
    "\n",
    "  # multi task loss\n",
    "  def multi_task_loss(self ,image ,cls_layer_output_label, reg_layer_output_label):\n",
    "\n",
    "    cls_layer_output, reg_layer_output = self.call(image) # 텐서 반환(상관성 존재). (1,14,14,18), (1,14,14,36)\n",
    "    minibatch_index_list = self.get_minibatch_index(cls_layer_output_label) # 미니배치 인덱스 휙득\n",
    "\n",
    "    # 모든 cls output을 softmax\n",
    "    cls_layer_output = tf.reshape(cls_layer_output[0], [1764,2])\n",
    "    cls_activation = tf.nn.softmax(cls_layer_output) # 잘됨\n",
    "\n",
    "    # reg output는 앵커의 변화값임\n",
    "    anchor_Use = np.reshape(anchors, (-1,4))\n",
    "    anchor_tensor = tf.convert_to_tensor(anchor_Use, dtype=tf.float32)\n",
    "\n",
    "    reg_layer_output = tf.reshape(reg_layer_output[0], [1764,4])\n",
    "    reg_layer_output = tf.math.add(reg_layer_output, anchor_tensor)\n",
    "\n",
    "    # label은 (1764,2)와 (1764,4)임\n",
    "    tensor_cls_label = tf.convert_to_tensor(cls_layer_output_label, dtype=tf.float32)\n",
    "    tensor_reg_label = tf.convert_to_tensor(reg_layer_output_label, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    # loss 계산(Loss 텐서에서 미니배치에 해당되는 애들만 걸러내야함)\n",
    "    print(\"loss 계산\")\n",
    "    Cls_Loss = tf.nn.softmax_cross_entropy_with_logits(labels=tensor_cls_label, logits=cls_activation) # (1764,1) 텐서\n",
    "\n",
    "    ## 여기까지 이상 무\n",
    "\n",
    "    filter_x = tf.Variable([[1.0],[0.0],[0.0], [0.0]])\n",
    "    filter_y = tf.Variable([[0.0],[1.0],[0.0], [0.0]])\n",
    "    filter_w = tf.Variable([[0.0],[0.0],[1.0], [0.0]])\n",
    "    filter_h = tf.Variable([[0.0],[0.0],[0.0], [1.0]])\n",
    "\n",
    "    print(\"reg loss 계산\")\n",
    "    # 여기서 터짐\n",
    "    x = tf.matmul(reg_layer_output,filter_x)\n",
    "    y = tf.matmul(reg_layer_output,filter_y)\n",
    "    w = tf.matmul(reg_layer_output,filter_w)\n",
    "    h = tf.matmul(reg_layer_output,filter_h)\n",
    "\n",
    "    x_a = tf.matmul(anchor_tensor,filter_x)\n",
    "    y_a = tf.matmul(anchor_tensor,filter_y)\n",
    "    w_a = tf.matmul(anchor_tensor,filter_w)\n",
    "    h_a = tf.matmul(anchor_tensor,filter_h)\n",
    "\n",
    "    x_star = tf.matmul(tensor_reg_label,filter_x)\n",
    "    y_star = tf.matmul(tensor_reg_label,filter_y)\n",
    "    w_star = tf.matmul(tensor_reg_label,filter_w)\n",
    "    h_star = tf.matmul(tensor_reg_label,filter_h)\n",
    "\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype)) # 텐서 로그는 ln밖에 없어서 ln10을 구한 뒤 나누는 방식으로 log10을 구한다\n",
    "\n",
    "    t_x = tf.divide(tf.subtract(x, x_a), w_a)\n",
    "    t_y = tf.divide(tf.subtract(y, y_a), h_a)\n",
    "    t_w = tf.devide(tf.log(tf.divide(w, w_a)), denominator)\n",
    "    t_h = tf.devide(tf.log(tf.divide(h, h_a)), denominator)\n",
    "\n",
    "    t_x_star = tf.divide(tf.subtract(x_star, x_a), w_a)\n",
    "    t_y_star = tf.divide(tf.subtract(y_star, y_a), h_a)\n",
    "    t_w_star = tf.devide(tf.log(tf.divide(w_star, w_a)), denominator)\n",
    "    t_h_star = tf.devide(tf.log(tf.divide(h_star, h_a)), denominator)\n",
    "\n",
    "    Reg_Loss = tf.add_n(tf.compat.v1.losses.huber_loss(t_x_star, t_x), tf.compat.v1.losses.huber_loss(t_y_star, t_y), tf.compat.v1.losses.huber_loss(t_w_star, t_w), tf.compat.v1.losses.huber_loss(t_h_star, t_h))   \n",
    "\n",
    "    # 미니배치에 해당되는 애들만 0이 아닌 값으로 만들기\n",
    "\n",
    "    minibatch_index_list_cls = np.zeros((1764,2))\n",
    "    minibatch_index_list_reg = np.zeros((1764,4))\n",
    "    for i in range(0, len(minibatch_index_list)) :\n",
    "      if minibatch_index_list[i] == 1:\n",
    "        minibatch_index_list_cls[i][0] = 1\n",
    "        minibatch_index_list_cls[i][1] = 1\n",
    "\n",
    "        minibatch_index_list_reg[i][0] = 1\n",
    "        minibatch_index_list_reg[i][1] = 1\n",
    "        minibatch_index_list_reg[i][2] = 1\n",
    "        minibatch_index_list_reg[i][3] = 1\n",
    "\n",
    "    # (1764,2)와 (1764,4)의 텐서로 변환\n",
    "    minibatch_index_list_cls = tf.convert_to_tensor(minibatch_index_list_cls, dtype=tf.float32)\n",
    "    minibatch_index_list_reg = tf.convert_to_tensor(minibatch_index_list_reg, dtype=tf.float32)\n",
    "\n",
    "    # 원소끼리 곱함 -> 미니배치 원소는 곱하기 1이 되고 나머지 원소는 곱하기 0이 되니까 미니배치 부분만 값이 남게 된다. \n",
    "    minibatch_cls = tf.multiply(minibatch_index_list_cls, Cls_Loss)\n",
    "    minibatch_reg = tf.multiply(minibatch_index_list_reg, Reg_Loss)\n",
    "\n",
    "\n",
    "    N_reg = tf.constant([1764.0])\n",
    "    N_cls = tf.constant([10.0/256.0]) # lambda도 곱한 값\n",
    "\n",
    "    loss = tf.add(tf.multiply(N_reg, tf.reduce_sum(minibatch_cls)), tf.multiply(N_cls, tf.reduce_sum(minibatch_reg)))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "    # get gradients\n",
    "  def get_grad(self, Loss, training_step, cls_reg_boolean): # cls_reg_boolean = 0이면 cls, cls_reg_boolean = 1 이면 reg\n",
    "    with tf.GradientTape() as tape:\n",
    "      if training_step == 1 : # 처음에는 conv3_1까지만 훈련\n",
    "        tape.watch(self.conv3_1.variables)\n",
    "        tape.watch(self.conv3_2.variables)\n",
    "        tape.watch(self.conv3_3.variables)\n",
    "        tape.watch(self.conv4_1.variables)\n",
    "        tape.watch(self.conv4_2.variables)\n",
    "        tape.watch(self.conv4_3.variables)\n",
    "        tape.watch(self.conv5_1.variables)\n",
    "        tape.watch(self.conv5_2.variables)\n",
    "        tape.watch(self.conv5_3.variables)\n",
    "        tape.watch(self.intermediate_layer.variables)\n",
    "\n",
    "        if cls_reg_boolean == 0:\n",
    "          tape.watch(self.cls_Layer.variables)\n",
    "        else:\n",
    "          tape.watch(self.reg_layer.variables)\n",
    "\n",
    "      elif training_step == 3:\n",
    "        shared_output = self.intermediate_layer(shared_output)\n",
    "        if cls_reg_boolean == 0:\n",
    "          tape.watch(self.cls_Layer.variables)\n",
    "        else:\n",
    "          tape.watch(self.reg_layer.variables)\n",
    "\n",
    "      g = 0\n",
    "      if training_step == 1 :\n",
    "        if cls_reg_boolean == 0:\n",
    "          g = tape.gradient(Loss, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]])\n",
    "        else:\n",
    "          g = tape.gradient(Loss, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]])\n",
    "\n",
    "      elif training_step == 3:\n",
    "        if cls_reg_boolean == 0:\n",
    "          g = tape.gradient(Loss, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]])\n",
    "        else:\n",
    "          g = tape.gradient(Loss, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]])\n",
    "\n",
    "    return g\n",
    "    \n",
    "  def App_Gradient(self, Loss, training_step) :\n",
    "    if training_step == 1:\n",
    "      g_cls = self.get_grad(Loss, training_step, 0)\n",
    "      self.Optimizers.apply_gradients(zip(g_cls, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]]))\n",
    "\n",
    "      g_reg = self.get_grad(Loss, training_step, 1)\n",
    "      self.Optimizers.apply_gradients(zip(g_reg, [self.conv3_1.variables[0], self.conv3_1.variables[1], self.conv3_2.variables[0],self.conv3_2.variables[1], self.conv3_3.variables[0],self.conv3_3.variables[1], self.conv4_1.variables[0],self.conv4_1.variables[1], self.conv4_2.variables[0],self.conv4_2.variables[1], self.conv4_3.variables[0],self.conv4_3.variables[1], self.conv5_1.variables[0],self.conv5_2.variables[1], self.conv5_3.variables[0],self.conv5_3.variables[1], self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]]))\n",
    "\n",
    "    if training_step == 3:\n",
    "\n",
    "      g_cls = self.get_grad(Loss, training_step, 0)\n",
    "      self.Optimizers.apply_gradients(zip(g_cls, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.cls_Layer.variables[0],self.cls_Layer.variables[1]]))\n",
    "\n",
    "      g_reg = self.get_grad(Loss, training_step, 1)\n",
    "      self.Optimizers.apply_gradients(zip(g_reg, [self.intermediate_layer.variables[0],self.intermediate_layer.variables[1], self.reg_layer.variables[0],self.reg_layer.variables[1]]))\n",
    "\n",
    "  # perform gradient descent\n",
    "\n",
    "  \n",
    "  def Training_model(self, image_list, cls_layer_ouptut_label_list, reg_layer_ouptut_label_list, training_step):\n",
    "    Loss_acc = 0\n",
    "    for i in tqdm(range(0, len(image_list)), desc = \"training\"):\n",
    "\n",
    "      image = np.expand_dims(image_list[i], axis = 0) # (1,224,224,3)으로 제작\n",
    "      Loss = self.multi_task_loss(image, cls_layer_ouptut_label_list[i], reg_layer_ouptut_label_list[i])\n",
    "      Loss_acc = Loss_acc + Loss\n",
    "      if i % 19 == 0:\n",
    "        Loss_acc = Loss_acc / 20\n",
    "        self.App_Gradient(Loss, training_step)\n",
    "        Loss_acc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[0,0,0,0],[0,1,0,0],[0,0,1,0], [0,0,0,0]])\n",
    "y = tf.Variable([[2],[4],[7], [6]])\n",
    "z = tf.matmul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: id=17, shape=(4, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [4],\n",
       "       [7],\n",
       "       [0]], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[3,1]])\n",
    "one = tf.one_hot(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 2) dtype=int32, numpy=array([[3, 1]], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RPN 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "regularizer = tf.keras.regularizers.l2(0.0005)\n",
    "\n",
    "for layer in SharedConvNet.layers:\n",
    "    # 'kernel_regularizer' 속성이 있는 인스턴스를 찾아 regularizer를 추가\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        setattr(layer, 'kernel_regularizer', regularizer)\n",
    "\n",
    "RPN_Model = RPN(initializer, regularizer, SharedConvNet)"
   ]
  },
  {
   "source": [
    "## RPN 훈련"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training:   0%|          | 0/5011 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# RPN_Model.Training_model(image_list, cls_layer_label_list, reg_layer_label_list, 1)"
   ]
  },
  {
   "source": [
    "### RPN에서 값 얻어내기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio):\n",
    "\n",
    "    for y in range(0, 14):\n",
    "        for x in range(0, 14):\n",
    "            for i in range(0, len(anchor_size)):\n",
    "                for j in range(0, len(anchor_aspect_ratio)):\n",
    "                    # 오브젝트다 vs 아니다 2개 항목에 대한 스코어 저장(각 앵커당)\n",
    "                    index = 3 * i + j\n",
    "                    object_score_exp = np.exp(cls_layer_output[y][x][2 * index]) # 출력값이 작아서 1e32를 구현하다\n",
    "                    non_object_score_exp = np.exp(cls_layer_output[y][x][2 * index + 1]) \n",
    "\n",
    "                    exp_sum = object_score_exp + non_object_score_exp\n",
    "                    score = np.array([object_score_exp / exp_sum, non_object_score_exp / exp_sum])\n",
    "                    cls_layer_output[y][x][2*(3*i+j)] = score[0]\n",
    "                    cls_layer_output[y][x][2*(3*i+j) + 1] = score[1]\n",
    "                        \n",
    "                    center_x = 8 + 16 * x \n",
    "                    center_y = 8 + 16 * y\n",
    "                    w = anchor_size[i] * anchor_aspect_ratio[j][0]\n",
    "                    h = anchor_size[i] * anchor_aspect_ratio[j][1]\n",
    "\n",
    "                    # 원래 출력값은 각 위치 앵커에서 변화율로 사용한다. \n",
    "                    reg_layer_output[y][x][4*index] = center_x + reg_layer_output[y][x][4*index]*1e32\n",
    "                    reg_layer_output[y][x][4*index + 1] = center_y + reg_layer_output[y][x][4*index + 1]*1e32\n",
    "                    reg_layer_output[y][x][4*index + 2] = w + reg_layer_output[y][x][4*index + 2]*1e32\n",
    "                    reg_layer_output[y][x][4*index + 3] = h + reg_layer_output[y][x][4*index + 3]*1e32\n",
    "\n",
    "    cls_layer_output = np.reshape(cls_layer_output, (-1,2))\n",
    "    reg_layer_output = np.reshape(reg_layer_output, (-1,4))\n",
    "\n",
    "    return cls_layer_output, reg_layer_output\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "source": [
    "def Get_Output_RPN(RPN_Model, image_list) :\n",
    "\n",
    "    cls_layer_output_list = np.array([])\n",
    "    reg_layer_output_list = np.array([])\n",
    "\n",
    "    anchor_size = [32, 64, 128] # 이미지 크기가 224*224라 32, 64, 128로 지정\n",
    "    anchor_aspect_ratio = [[1,1],[1,0.5], [0.5,1]] # W*L기준\n",
    "\n",
    "    for i in tqdm(range(0, len(image_list)), desc = \"get_output\"):\n",
    "        cls_layer_output, reg_layer_output = RPN_Model(np.expand_dims(image_list[i], axis=0))\n",
    "\n",
    "        cls_layer_output = cls_layer_output[0].numpy()\n",
    "        reg_layer_output = reg_layer_output[0].numpy()\n",
    "\n",
    "        # 가공하자\n",
    "        cls_layer_output, reg_layer_output = Process_output(cls_layer_output, reg_layer_output, anchor_size, anchor_aspect_ratio)\n",
    "\n",
    "        cls_layer_output_list = np.append(cls_layer_output_list, cls_layer_output)\n",
    "        reg_layer_output_list = np.append(reg_layer_output_list, reg_layer_output)\n",
    "    \n",
    "    cls_layer_output_list = np.reshape(cls_layer_output_list, (-1,1764,2))\n",
    "    reg_layer_output_list = np.reshape(reg_layer_output_list, (-1,1764,4))\n",
    "\n",
    "    return cls_layer_output_list, reg_layer_output_list"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "source": [
    "cls_layer_output_list, reg_layer_output_list = Get_Output_RPN(RPN_Model, image_list) # 출력값 얻기"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get_output: 100%|██████████| 5011/5011 [56:07<00:00,  1.49it/s]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "#### 일단 RoI를 특성맵에 맞게 변환시켜서 표시해보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fast R-CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 입력값 : 공유 특성맵 + RoI\n",
    "### 출력값 : 예측한 객체 종류, 객체의 좌표(r, c, h, w). (r,c)는 왼쪽위 좌표고 h,w는 너비, 높이"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 한 이미지에 대한 출력값 중 object score가 0.7 이상인 앵커들을 선별한다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(cls_layer_output, reg_layer_output, reg_layer_label, index_classes): # 한 이미지에서 뽑아낸 RPN의 출력값 2개, 이미지에 있는 모든 앵커들이 참조한 Ground Truth Box, 그 Ground Truth Box의 Class\n",
    "    nms_RoI_List = [] # 각 이미지마다 RoI 갯수가 다르다. 그래서 리스트로 저장한다. 리스트 안에 리스트 저장하는 방식으로 len = 5011인 리스트를 생성할거다 \n",
    "    nms_GroundTruthBox_List = [] # NMS에 들어간 RoI가 참조한 Ground Truth Box \n",
    "    nms_Classes_List = []\n",
    "    for i in range(0, len(cls_layer_output)) :\n",
    "        if cls_layer_output[i][0] > 0.7 : # 해당 앵커의 object score가 0.7을 넘겼으면\n",
    "            # 저장\n",
    "            nms_RoI_List.append(reg_layer_output[i]) # RoI\n",
    "            nms_GroundTruthBox_List.append(reg_layer_output_list[i]) # Ground Truth Box\n",
    "            # 그 Box가 가리키는 클래스 인덱스\n",
    "            nms_Classes_List.append(index_classes[i])\n",
    "\n",
    "\n",
    "\n",
    "    return nms_RoI_List, nms_GroundTruthBox_List, nms_Classes_List # 선별한 애들을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nms_list(cls_layer_output_list, reg_layer_output_list, reg_layer_label_list, index_classes_list) :\n",
    "    # RPN output 2개, 이미지에서 각 앵커(1764)들이 어떤 Ground Truth Box보고 IoU 계산했는지, 이미지에서 각 앵커(1764)들이 어떤 '클래스'의 Ground Truth Box보고 IoU 계산했는지\n",
    "    # Detector 훈련에 필요한 데이터를 얻는 곳이다\n",
    "    \n",
    "    NMS_RoIs_List = [] # 전체 입력 이미지의 RoI를 이미지별로 저장(리스트 안에 리스트)\n",
    "    NMS_GroundTruthBoxes_List = []\n",
    "    NMS_Classes_List = []\n",
    "\n",
    "    for i in tqdm(range(0, len(cls_layer_output_list)), desc = \"get_RoI\"): # 5011개에 대한 nms 구한다\n",
    "        nms_RoI_List, nms_GroundTruthBox_List, nms_Classes_List = nms(cls_layer_output_list[i], reg_layer_output_list[i], reg_layer_label_list[i], index_classes_list[i])\n",
    "        # 각 이미지에서 RoI 얻은 정보를 담기\n",
    "        NMS_RoIs_List.append(nms_RoI_List)\n",
    "        NMS_GroundTruthBoxes_List.append(NMS_GroundTruthBoxes_List)\n",
    "        NMS_Classes_List.append(nms_Classes_List)\n",
    "\n",
    "    return NMS_RoIs_List, NMS_GroundTruthBoxes_List, NMS_Classes_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_RoI(nms_RoI_List) :\n",
    "    # RoI_list : (-1, 4) <- (x,y,w,h)\n",
    "    RoI_list_forDetector = np.array([])\n",
    "    for i in range(0, len(RoI_list)) :\n",
    "        r = RoI_list[i][0] - (RoI_list[i][2]/2)\n",
    "        c = RoI_list[i][1] - (RoI_list[i][3]/2)\n",
    "        w = RoI_list[i][2]\n",
    "        h = RoI_list[i][3]\n",
    "\n",
    "        roi_forDetector = np.array([r,c,w,h])\n",
    "        RoI_list_forDetector = np.append(RoI_list_forDetector, roi_forDetector)\n",
    "    RoI_list_forDetector = np.reshape(RoI_list_forDetector, (-1,4)) \n",
    "\n",
    "    return RoI_list_forDetector"
   ]
  },
  {
   "source": [
    "## Detector 훈련에 필요한 데이터셋 휙득"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMS_RoIs_List, NMS_GroundTruthBoxes_List, NMS_Classes_List = get_nms_list(cls_layer_output_list, reg_layer_output_list, reg_layer_label_list, index_classes_list)"
   ]
  },
  {
   "source": [
    "## Detector 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RoI Pooling Layer 제작"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling 작업을 RoI 지역에 대해 한다. 14*14*512를 7*7*512로\n",
    "class RoiPoolingLayer(tf.keras.layers):\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "        super(RoiPoolingLayer, self).__init__(name='RoI_Pooling_Layer')\n",
    "\n",
    "        self.pool_size = pool_size # VGG16에서는 7*7\n",
    "        self.num_rois = num_rois # RoI 개수\n",
    "        \n",
    "    def build(self, input_shape): # input shape로 (1,14,14,512)와 같이 받으니까 3번 원소 자리의 값인 512를 가져간다. \n",
    "        self.nb_channels = input_shape[0][3] # 채널 조정\n",
    "        # 맨처음에 값을 입력받을 때 가중치를 초기화 할 수 있는데 얘가 그걸 담당한다. \n",
    "        # 레이어가 받은 입력 크기를 기준으로 나중에 변수를 만들 수 있다는 장점이 있다. \n",
    "\n",
    "    def compute_output_shape(self, input_shape): # If the layer has not been built, this method will call build on the layer. \n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, image, RoI_List): # 정방향 연산\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # (1,14,14,512)에서 (14,14,512)를 가져감\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = tf.shape(img) # <tf.Tensor: shape=(14,14,512), dtype=int32, numpy=array([...], dtype=float32)>\n",
    "\n",
    "        RoiPooling_outputs_List = [] # RoI 부근을 잘라낸 뒤 7*7로 만들어낸 것들을 여기에 모은다\n",
    "\n",
    "        for roi_idx in range(0, self.num_rois): # 이미지 당 RoI 갯수만큼 for문 반복 -> RoI 갯수만큼 특성맵 얻으려고\n",
    "            x = RoI_List[i][0]\n",
    "            y = RoI_List[i][1]\n",
    "            w = RoI_List[i][2]\n",
    "            h = RoI_List[i][3]\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            Resize_featuremap = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size)) # RoI부분만 빼서 7*7로 리사이징 해준다. 원래 Pooling은 전체 범위에 대해 이 작업을 한다. \n",
    "            RoiPooling_outputs_List.append(Resize_featuremap) # 리스트에 추가\n",
    "\n",
    "        Pooling_output = tf.concat(outputs, axis=0)\n",
    "\n",
    "        Pooling_output = tf.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels)) # output 양식이 (1,...)니까 (1, RoI개수, 7,7,512)와 같이 만들어준다. \n",
    "        Pooling_output = tf.keras.backend.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return Pooling_output\n",
    "    \n",
    "    def get_config(self): # 구성요소 반환\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, initializer, regularizer, shared_convNet):\n",
    "    super(Detector, self).__init__(name='Detector')\n",
    "    # 레이어 \n",
    "    # 공용 레이어(14*14*512)를 받아서 RoI Pooling Layer에 넣어 7*7*512를 만들고 FCs를 거쳐 두가지 Output을 생성한다. \n",
    "\n",
    "    # 클래스 분류 레이어와 박스 위치 회귀 레이어의 초기화를 다르게 해야한다. \n",
    "    Classify_layer_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "    Box_regression_layer_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None)\n",
    "\n",
    "    # RoI Pooling : H*W(7*7)에 맞게 입력 특성맵을 pooling. RoI에 해당하는 영역을 7*7로 Pooling한다. \n",
    "    self.RoIPoolingLayer = RoiPoolingLayer(7, num_rois) # Pooling 이후 크기를 7*7*512로 만든다. \n",
    "    self.Flatten_layer = tf.keras.layers.Flatten() # 여기서 4096개가 되어야한다.\n",
    "    self.Fully_Connected = tf.keras.layers.Dense(4096, activation='relu') # 별 말이 없으니 기본적으로 지정된 kernel_initializer를 사용하자\n",
    "    self.Classify_layer = tf.keras.layers.Dense(21, activation='softmax', kernel_initializer = Classify_layer_initializer, name = \"output_1\")\n",
    "    self.reg_layer = tf.keras.layers.Conv2D(84, activation= None, kernel_initializer = Box_regression_layer_initializer, name = \"output_2\")\n",
    "    \n",
    "  def call(self, input1, input2): # input으로 공용 컨볼루션 레이어 텐서(1,14,14,512)와 RoI를 받는다. RoI는 앞서 RPN에서 뽑아낸걸 쓴다. \n",
    "    \n",
    "    output = self.RoIPoolingLayer(input1, input2) # input1 : (1,14,14,512), input2 : RoI -> 7*7 output\n",
    "    output = self.Flatten_layer(output) # Flatten으로 한줄 세우기\n",
    "    output = self.Fully_Connected(output) # FCs로 만들기\n",
    "    # 객체 분류 레이어, 박스 회귀 레이어\n",
    "    cls_output = Classify_layer_output = self.Classify_layer(output) # 각 객체에 \n",
    "    reg_layer_output = self.reg_layer(output)\n",
    "\n",
    "    return Classify_layer_output, reg_layer_output # 두 아웃풋을 내놓는다. \n",
    "\n",
    "  def get_grad(self, Loss, training_step, cls_reg_boolean):\n",
    "  \n",
    "  def App_Gradient(self, Loss, training_step) :\n",
    "\n",
    "  \n",
    "  def training_Detector():\n"
   ]
  },
  {
   "source": [
    "## Loss 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "SharedConvNet = tf.keras.models.Sequential()\n",
    "for i in range(0, max_num-1):\n",
    "    SharedConvNet.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "Detector_Model = Detector(SharedConvNet)\n",
    "RPN_Model.run_eagerly = True # 모델 내부 함수에서 쉽게 연산할 수 있게 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Detector(model, , traning_step, EPOCH):\n",
    "\n",
    "    losses = {'output_1' : 'categorical_crossentropy', 'output_2' : loss_detector_loc} # classify layer는 20 + 1가지 클래스에 대한 softmax, reg_layer는 따로 만든 loss함수\n",
    "\n",
    "    model.compile(optimizer=Optimizer, loss=losses, run_eagerly=True)\n",
    "\n",
    "    history = model.fit(np.asarray(image_list), [cls_layer_label_list, reg_layer_label_list], batch_size = 1, epochs = EPOCH)\n",
    "\n",
    "    return model, history\n"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dec8a5ff48d192d3613b9e6713a72fe103c8e5d5c44dc7b8ee41f3376e4a4f37",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5월 IMC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 임포트"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # 넘파이\n",
    "import matplotlib.pyplot as plt # 매트플롯립\n",
    "import pandas as pd # 판다스(csv)\n",
    "import re # 정규식 표현\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras.backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "train = pd.read_csv('/Users/minguinho/Documents/AI_Datasets/IMC_May_data/IMC_May_train.csv', encoding='cp949')\n",
    "test = pd.read_csv('/Users/minguinho/Documents/AI_Datasets/IMC_May_data/IMC_May_test.csv', encoding='cp949')"
   ]
  },
  {
   "source": [
    "### 트레이닝 데이터 975개, 테스트 데이터 107개가 있다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 975 entries, 0 to 974\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Message_body  975 non-null    object\n 1   Label         975 non-null    object\ndtypes: object(2)\nmemory usage: 15.4+ KB\ntrain :  None\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 107 entries, 0 to 106\nData columns (total 1 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Message_body  107 non-null    object\ndtypes: object(1)\nmemory usage: 984.0+ bytes\ntest :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"train : \", train.info())\n",
    "print()\n",
    "print(\"test : \", test.info())\n"
   ]
  },
  {
   "source": [
    "### dataframe에서 메일 제목과 라벨값을 추출해 데이터셋을 만들어준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n"
     ]
    }
   ],
   "source": [
    "training_sentences = train['Message_body'].tolist()\n",
    "testing_sentences = test['Message_body'].tolist()\n",
    "training_temp = train['Label'].tolist()\n",
    "\n",
    "training_labels = []\n",
    "for i in range(0, 975) : \n",
    "    print(training_temp[i])\n",
    "    if training_temp[i] == 'Non-Spam':\n",
    "        print(training_temp[i])\n",
    "        training_labels.append(0)\n",
    "    else:\n",
    "        training_labels.append(1)\n",
    "\n",
    "training_labels = np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 # 단어 크기\n",
    "embedding_dim = 16 # 임패딩 단위\n",
    "max_length = 128 # 리뷰에 들어있는 최대 단어 갯수. 아무리 길어도 단어 120개가 한계다...라는 것을 나타냄.\n",
    "trunc_type='post' # truncation type. 단어 갯수가 120개보다 많을 때 앞에서부터 자를건지 뒤에서부터 자를건지 결정함 post는 문자열이 120개보다 많으면 뒤에서 자르겠다는 뜻.\n",
    "oov_tok = \"<OOV>\" # 토큰 딕셔너리에 없는 단어는 OOV로 표시\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok) # num_words = vocab_size : 가장 많이 쓰이는 단어 'vocab_size'개만 토큰화 하는거. 그러니까 토큰화 대상인 문자여렝 단어가 20000개 있으면 많이 쓰이는 순으로 나열한 뒤 앞에 있는 10000개의 단어만 토큰화 하고 나머지 10000개는 토큰화하지 않는다. 얘들은 뭐 OOV로 나오겠지\n",
    "tokenizer.fit_on_texts(training_sentences) # 토큰화\n",
    "word_index = tokenizer.word_index # 토큰화 결과 가져오기\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) # 문장들을 토큰 순서로 나열하기\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type) # 패딩. \n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences) # 훈련셋으로 만든 토크나이저로 테스트셋의 시퀀스 출력. 토큰화 하지않은 단어가 많기 때문에 기존에 만들어놨던 걸로 한다. 만약 토크나이저를 새로 만든다면 두 토크나이저는 다른 토크나이저가 되기 때문에 원하는 결과를 얻을 수 없다. \n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length) # 패딩\n"
   ]
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "filename = 'checkpoint_bestModel.h5'.format(EPOCH, BATCH_SIZE)\n",
    "checkpoint = ModelCheckpoint(filename,             # 파일 이름\n",
    "                             monitor = 'val_f1_score',   # val_loss 값이 개선되면 개선 모델을 filename으로 저장\n",
    "                             verbose=1,            # 로그 출력\n",
    "                             save_best_only=True,  # 가장 최고의 val_loss를 보이는 모델만 저장\n",
    "                             mode='max'           \n",
    "                            )\n",
    "\n",
    "# 모델 생성\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[None]),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), # 첫번째 LSTM. return_sequences=True를 해줘야한다. 두번 째 LSTM에 모든 시퀀스(아웃풋)을 다음 LSTM에 넣어야한다. \n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # 두번째 LSTM\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[tfa.metrics.F1Score(num_classes=2, average='micro',threshold=0.5)]) # 컴파일. 긍정, 부정밖에 없으니 이진 분류를 하니까 binary_crossentropy를 손실 함수로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 10s 499ms/step - loss: 0.5448 - f1_score: 0.0000e+00 - val_loss: 0.6766 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to checkpoint_bestModel.h5\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4247 - f1_score: 0.0000e+00 - val_loss: 0.8143 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4061 - f1_score: 0.0000e+00 - val_loss: 0.8412 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4192 - f1_score: 0.0000e+00 - val_loss: 0.7447 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.3644 - f1_score: 0.0000e+00 - val_loss: 0.7018 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.3822 - f1_score: 0.0000e+00 - val_loss: 0.6823 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.3693 - f1_score: 0.0000e+00 - val_loss: 0.6377 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.3211 - f1_score: 0.0000e+00 - val_loss: 0.5513 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.2876 - f1_score: 0.0000e+00 - val_loss: 0.4814 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 0.2771 - f1_score: 0.0392 - val_loss: 0.4298 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.2731 - f1_score: 0.0749 - val_loss: 0.3961 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.00000\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.2811 - f1_score: 0.1984 - val_loss: 0.4930 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.00000\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.2786 - f1_score: 0.0628 - val_loss: 0.3913 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.00000\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2651 - f1_score: 0.2062 - val_loss: 0.4422 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.00000\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2667 - f1_score: 0.2345 - val_loss: 0.4052 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.00000\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2464 - f1_score: 0.3587 - val_loss: 0.3997 - val_f1_score: 0.4651\n",
      "\n",
      "Epoch 00016: val_f1_score improved from 0.00000 to 0.46512, saving model to checkpoint_bestModel.h5\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2560 - f1_score: 0.2119 - val_loss: 0.3739 - val_f1_score: 0.6476\n",
      "\n",
      "Epoch 00017: val_f1_score improved from 0.46512 to 0.64762, saving model to checkpoint_bestModel.h5\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2415 - f1_score: 0.4649 - val_loss: 0.4520 - val_f1_score: 0.1972\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.64762\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2317 - f1_score: 0.2973 - val_loss: 0.3963 - val_f1_score: 0.6061\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.64762\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.2553 - f1_score: 0.4518 - val_loss: 0.3891 - val_f1_score: 0.6857\n",
      "\n",
      "Epoch 00020: val_f1_score improved from 0.64762 to 0.68571, saving model to checkpoint_bestModel.h5\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.2503 - f1_score: 0.4772 - val_loss: 0.3919 - val_f1_score: 0.6981\n",
      "\n",
      "Epoch 00021: val_f1_score improved from 0.68571 to 0.69811, saving model to checkpoint_bestModel.h5\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2290 - f1_score: 0.5565 - val_loss: 0.3681 - val_f1_score: 0.7500\n",
      "\n",
      "Epoch 00022: val_f1_score improved from 0.69811 to 0.75000, saving model to checkpoint_bestModel.h5\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.2221 - f1_score: 0.4937 - val_loss: 0.3853 - val_f1_score: 0.6602\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.75000\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.2295 - f1_score: 0.5119 - val_loss: 0.3680 - val_f1_score: 0.6981\n",
      "\n",
      "Epoch 00024: val_f1_score did not improve from 0.75000\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.2298 - f1_score: 0.6508 - val_loss: 0.4123 - val_f1_score: 0.4471\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.75000\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.2473 - f1_score: 0.3180 - val_loss: 0.4649 - val_f1_score: 0.2254\n",
      "\n",
      "Epoch 00026: val_f1_score did not improve from 0.75000\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2190 - f1_score: 0.4514 - val_loss: 0.3570 - val_f1_score: 0.7458\n",
      "\n",
      "Epoch 00027: val_f1_score did not improve from 0.75000\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2517 - f1_score: 0.5603 - val_loss: 0.4099 - val_f1_score: 0.6139\n",
      "\n",
      "Epoch 00028: val_f1_score did not improve from 0.75000\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.2485 - f1_score: 0.4042 - val_loss: 0.3738 - val_f1_score: 0.6729\n",
      "\n",
      "Epoch 00029: val_f1_score did not improve from 0.75000\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.2201 - f1_score: 0.5129 - val_loss: 0.3620 - val_f1_score: 0.7544\n",
      "\n",
      "Epoch 00030: val_f1_score improved from 0.75000 to 0.75439, saving model to checkpoint_bestModel.h5\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2396 - f1_score: 0.5407 - val_loss: 0.3619 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00031: val_f1_score did not improve from 0.75439\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.2278 - f1_score: 0.5502 - val_loss: 0.4355 - val_f1_score: 0.0625\n",
      "\n",
      "Epoch 00032: val_f1_score did not improve from 0.75439\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2078 - f1_score: 0.3864 - val_loss: 0.3600 - val_f1_score: 0.7627\n",
      "\n",
      "Epoch 00033: val_f1_score improved from 0.75439 to 0.76271, saving model to checkpoint_bestModel.h5\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2240 - f1_score: 0.5997 - val_loss: 0.3837 - val_f1_score: 0.6796\n",
      "\n",
      "Epoch 00034: val_f1_score did not improve from 0.76271\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2433 - f1_score: 0.5451 - val_loss: 0.4097 - val_f1_score: 0.5684\n",
      "\n",
      "Epoch 00035: val_f1_score did not improve from 0.76271\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.2491 - f1_score: 0.5059 - val_loss: 0.3621 - val_f1_score: 0.7368\n",
      "\n",
      "Epoch 00036: val_f1_score did not improve from 0.76271\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.2284 - f1_score: 0.5024 - val_loss: 0.4197 - val_f1_score: 0.5532\n",
      "\n",
      "Epoch 00037: val_f1_score did not improve from 0.76271\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.2538 - f1_score: 0.5239 - val_loss: 0.3517 - val_f1_score: 0.7458\n",
      "\n",
      "Epoch 00038: val_f1_score did not improve from 0.76271\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2087 - f1_score: 0.6506 - val_loss: 0.4148 - val_f1_score: 0.6263\n",
      "\n",
      "Epoch 00039: val_f1_score did not improve from 0.76271\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.2302 - f1_score: 0.5719 - val_loss: 0.3560 - val_f1_score: 0.7544\n",
      "\n",
      "Epoch 00040: val_f1_score did not improve from 0.76271\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.2229 - f1_score: 0.5919 - val_loss: 0.3753 - val_f1_score: 0.6337\n",
      "\n",
      "Epoch 00041: val_f1_score did not improve from 0.76271\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2301 - f1_score: 0.5538 - val_loss: 0.4042 - val_f1_score: 0.5376\n",
      "\n",
      "Epoch 00042: val_f1_score did not improve from 0.76271\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.2156 - f1_score: 0.6328 - val_loss: 0.3390 - val_f1_score: 0.7731\n",
      "\n",
      "Epoch 00043: val_f1_score improved from 0.76271 to 0.77311, saving model to checkpoint_bestModel.h5\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2148 - f1_score: 0.6593 - val_loss: 0.3907 - val_f1_score: 0.6731\n",
      "\n",
      "Epoch 00044: val_f1_score did not improve from 0.77311\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.2140 - f1_score: 0.6792 - val_loss: 0.3923 - val_f1_score: 0.6337\n",
      "\n",
      "Epoch 00045: val_f1_score did not improve from 0.77311\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.2019 - f1_score: 0.6482 - val_loss: 0.3674 - val_f1_score: 0.6909\n",
      "\n",
      "Epoch 00046: val_f1_score did not improve from 0.77311\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2128 - f1_score: 0.6633 - val_loss: 0.3500 - val_f1_score: 0.7586\n",
      "\n",
      "Epoch 00047: val_f1_score did not improve from 0.77311\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2364 - f1_score: 0.6717 - val_loss: 0.3521 - val_f1_score: 0.7143\n",
      "\n",
      "Epoch 00048: val_f1_score did not improve from 0.77311\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.2222 - f1_score: 0.6555 - val_loss: 0.3699 - val_f1_score: 0.6408\n",
      "\n",
      "Epoch 00049: val_f1_score did not improve from 0.77311\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.2157 - f1_score: 0.6710 - val_loss: 0.3457 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00050: val_f1_score did not improve from 0.77311\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2054 - f1_score: 0.5912 - val_loss: 0.3533 - val_f1_score: 0.6727\n",
      "\n",
      "Epoch 00051: val_f1_score did not improve from 0.77311\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2079 - f1_score: 0.6415 - val_loss: 0.3711 - val_f1_score: 0.6346\n",
      "\n",
      "Epoch 00052: val_f1_score did not improve from 0.77311\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.2151 - f1_score: 0.5808 - val_loss: 0.3581 - val_f1_score: 0.6792\n",
      "\n",
      "Epoch 00053: val_f1_score did not improve from 0.77311\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2185 - f1_score: 0.6461 - val_loss: 0.3270 - val_f1_score: 0.7680\n",
      "\n",
      "Epoch 00054: val_f1_score did not improve from 0.77311\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.2230 - f1_score: 0.5824 - val_loss: 0.5319 - val_f1_score: 0.4286\n",
      "\n",
      "Epoch 00055: val_f1_score did not improve from 0.77311\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2565 - f1_score: 0.4817 - val_loss: 0.3623 - val_f1_score: 0.6667\n",
      "\n",
      "Epoch 00056: val_f1_score did not improve from 0.77311\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.2041 - f1_score: 0.6507 - val_loss: 0.3610 - val_f1_score: 0.6972\n",
      "\n",
      "Epoch 00057: val_f1_score did not improve from 0.77311\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1961 - f1_score: 0.6316 - val_loss: 0.3758 - val_f1_score: 0.6667\n",
      "\n",
      "Epoch 00058: val_f1_score did not improve from 0.77311\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.2289 - f1_score: 0.6346 - val_loss: 0.3541 - val_f1_score: 0.7257\n",
      "\n",
      "Epoch 00059: val_f1_score did not improve from 0.77311\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1857 - f1_score: 0.6838 - val_loss: 0.3522 - val_f1_score: 0.6972\n",
      "\n",
      "Epoch 00060: val_f1_score did not improve from 0.77311\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1866 - f1_score: 0.6784 - val_loss: 0.3418 - val_f1_score: 0.7521\n",
      "\n",
      "Epoch 00061: val_f1_score did not improve from 0.77311\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.2088 - f1_score: 0.6985 - val_loss: 0.3658 - val_f1_score: 0.6972\n",
      "\n",
      "Epoch 00062: val_f1_score did not improve from 0.77311\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.2092 - f1_score: 0.6415 - val_loss: 0.3343 - val_f1_score: 0.7586\n",
      "\n",
      "Epoch 00063: val_f1_score did not improve from 0.77311\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.2139 - f1_score: 0.6948 - val_loss: 0.3617 - val_f1_score: 0.6604\n",
      "\n",
      "Epoch 00064: val_f1_score did not improve from 0.77311\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.1943 - f1_score: 0.6410 - val_loss: 0.3656 - val_f1_score: 0.6852\n",
      "\n",
      "Epoch 00065: val_f1_score did not improve from 0.77311\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.2100 - f1_score: 0.6714 - val_loss: 0.3263 - val_f1_score: 0.7667\n",
      "\n",
      "Epoch 00066: val_f1_score did not improve from 0.77311\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.2041 - f1_score: 0.7156 - val_loss: 0.3276 - val_f1_score: 0.7414\n",
      "\n",
      "Epoch 00067: val_f1_score did not improve from 0.77311\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1973 - f1_score: 0.6871 - val_loss: 0.4162 - val_f1_score: 0.6000\n",
      "\n",
      "Epoch 00068: val_f1_score did not improve from 0.77311\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1938 - f1_score: 0.5800 - val_loss: 0.3150 - val_f1_score: 0.7805\n",
      "\n",
      "Epoch 00069: val_f1_score improved from 0.77311 to 0.78049, saving model to checkpoint_bestModel.h5\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.1816 - f1_score: 0.7487 - val_loss: 0.3767 - val_f1_score: 0.6789\n",
      "\n",
      "Epoch 00070: val_f1_score did not improve from 0.78049\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1826 - f1_score: 0.6977 - val_loss: 0.3292 - val_f1_score: 0.7350\n",
      "\n",
      "Epoch 00071: val_f1_score did not improve from 0.78049\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1683 - f1_score: 0.7134 - val_loss: 0.4366 - val_f1_score: 0.5567\n",
      "\n",
      "Epoch 00072: val_f1_score did not improve from 0.78049\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.1845 - f1_score: 0.6881 - val_loss: 0.3344 - val_f1_score: 0.7769\n",
      "\n",
      "Epoch 00073: val_f1_score did not improve from 0.78049\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.1865 - f1_score: 0.6751 - val_loss: 0.4144 - val_f1_score: 0.6852\n",
      "\n",
      "Epoch 00074: val_f1_score did not improve from 0.78049\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.2049 - f1_score: 0.6030 - val_loss: 0.3393 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00075: val_f1_score did not improve from 0.78049\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1768 - f1_score: 0.7107 - val_loss: 0.3103 - val_f1_score: 0.7840\n",
      "\n",
      "Epoch 00076: val_f1_score improved from 0.78049 to 0.78400, saving model to checkpoint_bestModel.h5\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1868 - f1_score: 0.7307 - val_loss: 0.3594 - val_f1_score: 0.7241\n",
      "\n",
      "Epoch 00077: val_f1_score did not improve from 0.78400\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.1978 - f1_score: 0.5919 - val_loss: 0.3831 - val_f1_score: 0.6214\n",
      "\n",
      "Epoch 00078: val_f1_score did not improve from 0.78400\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1681 - f1_score: 0.7173 - val_loss: 0.3203 - val_f1_score: 0.7805\n",
      "\n",
      "Epoch 00079: val_f1_score did not improve from 0.78400\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1950 - f1_score: 0.7381 - val_loss: 0.3649 - val_f1_score: 0.7193\n",
      "\n",
      "Epoch 00080: val_f1_score did not improve from 0.78400\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1878 - f1_score: 0.7182 - val_loss: 0.5012 - val_f1_score: 0.4946\n",
      "\n",
      "Epoch 00081: val_f1_score did not improve from 0.78400\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.2004 - f1_score: 0.5877 - val_loss: 0.2994 - val_f1_score: 0.8000\n",
      "\n",
      "Epoch 00082: val_f1_score improved from 0.78400 to 0.80000, saving model to checkpoint_bestModel.h5\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1887 - f1_score: 0.6401 - val_loss: 0.3127 - val_f1_score: 0.7742\n",
      "\n",
      "Epoch 00083: val_f1_score did not improve from 0.80000\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.1924 - f1_score: 0.6876 - val_loss: 0.3636 - val_f1_score: 0.6972\n",
      "\n",
      "Epoch 00084: val_f1_score did not improve from 0.80000\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.1927 - f1_score: 0.5788 - val_loss: 0.3667 - val_f1_score: 0.7143\n",
      "\n",
      "Epoch 00085: val_f1_score did not improve from 0.80000\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1746 - f1_score: 0.7182 - val_loss: 0.3214 - val_f1_score: 0.7500\n",
      "\n",
      "Epoch 00086: val_f1_score did not improve from 0.80000\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1775 - f1_score: 0.6940 - val_loss: 0.3850 - val_f1_score: 0.6729\n",
      "\n",
      "Epoch 00087: val_f1_score did not improve from 0.80000\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.1426 - f1_score: 0.7262 - val_loss: 0.3489 - val_f1_score: 0.7241\n",
      "\n",
      "Epoch 00088: val_f1_score did not improve from 0.80000\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1728 - f1_score: 0.7245 - val_loss: 0.3401 - val_f1_score: 0.7563\n",
      "\n",
      "Epoch 00089: val_f1_score did not improve from 0.80000\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1583 - f1_score: 0.7749 - val_loss: 0.3962 - val_f1_score: 0.6852\n",
      "\n",
      "Epoch 00090: val_f1_score did not improve from 0.80000\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1753 - f1_score: 0.7151 - val_loss: 0.3149 - val_f1_score: 0.7805\n",
      "\n",
      "Epoch 00091: val_f1_score did not improve from 0.80000\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.1815 - f1_score: 0.7739 - val_loss: 0.3774 - val_f1_score: 0.6476\n",
      "\n",
      "Epoch 00092: val_f1_score did not improve from 0.80000\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.1620 - f1_score: 0.7458 - val_loss: 0.3207 - val_f1_score: 0.7705\n",
      "\n",
      "Epoch 00093: val_f1_score did not improve from 0.80000\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1552 - f1_score: 0.8048 - val_loss: 0.3413 - val_f1_score: 0.7563\n",
      "\n",
      "Epoch 00094: val_f1_score did not improve from 0.80000\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1395 - f1_score: 0.8317 - val_loss: 0.4284 - val_f1_score: 0.6972\n",
      "\n",
      "Epoch 00095: val_f1_score did not improve from 0.80000\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.1718 - f1_score: 0.7107 - val_loss: 0.3789 - val_f1_score: 0.6903\n",
      "\n",
      "Epoch 00096: val_f1_score did not improve from 0.80000\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.1591 - f1_score: 0.6707 - val_loss: 0.3356 - val_f1_score: 0.7350\n",
      "\n",
      "Epoch 00097: val_f1_score did not improve from 0.80000\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.1893 - f1_score: 0.7337 - val_loss: 0.3537 - val_f1_score: 0.7193\n",
      "\n",
      "Epoch 00098: val_f1_score did not improve from 0.80000\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.1776 - f1_score: 0.7140 - val_loss: 0.3270 - val_f1_score: 0.7500\n",
      "\n",
      "Epoch 00099: val_f1_score did not improve from 0.80000\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1689 - f1_score: 0.7644 - val_loss: 0.3706 - val_f1_score: 0.7207\n",
      "\n",
      "Epoch 00100: val_f1_score did not improve from 0.80000\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1419 - f1_score: 0.8154 - val_loss: 0.3308 - val_f1_score: 0.7805\n",
      "\n",
      "Epoch 00101: val_f1_score did not improve from 0.80000\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1705 - f1_score: 0.7243 - val_loss: 0.4100 - val_f1_score: 0.7091\n",
      "\n",
      "Epoch 00102: val_f1_score did not improve from 0.80000\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1631 - f1_score: 0.6827 - val_loss: 0.3350 - val_f1_score: 0.7667\n",
      "\n",
      "Epoch 00103: val_f1_score did not improve from 0.80000\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1482 - f1_score: 0.7914 - val_loss: 0.4077 - val_f1_score: 0.6667\n",
      "\n",
      "Epoch 00104: val_f1_score did not improve from 0.80000\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1388 - f1_score: 0.7931 - val_loss: 0.3678 - val_f1_score: 0.7627\n",
      "\n",
      "Epoch 00105: val_f1_score did not improve from 0.80000\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.1425 - f1_score: 0.7662 - val_loss: 0.3132 - val_f1_score: 0.8209\n",
      "\n",
      "Epoch 00106: val_f1_score improved from 0.80000 to 0.82090, saving model to checkpoint_bestModel.h5\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1770 - f1_score: 0.7560 - val_loss: 0.3985 - val_f1_score: 0.7478\n",
      "\n",
      "Epoch 00107: val_f1_score did not improve from 0.82090\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.1493 - f1_score: 0.7505 - val_loss: 0.3600 - val_f1_score: 0.7627\n",
      "\n",
      "Epoch 00108: val_f1_score did not improve from 0.82090\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1546 - f1_score: 0.6978 - val_loss: 0.3268 - val_f1_score: 0.7667\n",
      "\n",
      "Epoch 00109: val_f1_score did not improve from 0.82090\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1575 - f1_score: 0.7915 - val_loss: 0.4236 - val_f1_score: 0.7037\n",
      "\n",
      "Epoch 00110: val_f1_score did not improve from 0.82090\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1766 - f1_score: 0.6378 - val_loss: 0.3153 - val_f1_score: 0.7705\n",
      "\n",
      "Epoch 00111: val_f1_score did not improve from 0.82090\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.1684 - f1_score: 0.7265 - val_loss: 0.3659 - val_f1_score: 0.7692\n",
      "\n",
      "Epoch 00112: val_f1_score did not improve from 0.82090\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1510 - f1_score: 0.7261 - val_loss: 0.3250 - val_f1_score: 0.7705\n",
      "\n",
      "Epoch 00113: val_f1_score did not improve from 0.82090\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1532 - f1_score: 0.7991 - val_loss: 0.4582 - val_f1_score: 0.6857\n",
      "\n",
      "Epoch 00114: val_f1_score did not improve from 0.82090\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1485 - f1_score: 0.6710 - val_loss: 0.3260 - val_f1_score: 0.7603\n",
      "\n",
      "Epoch 00115: val_f1_score did not improve from 0.82090\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1502 - f1_score: 0.8036 - val_loss: 0.3903 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00116: val_f1_score did not improve from 0.82090\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1369 - f1_score: 0.7596 - val_loss: 0.3572 - val_f1_score: 0.7458\n",
      "\n",
      "Epoch 00117: val_f1_score did not improve from 0.82090\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1397 - f1_score: 0.8147 - val_loss: 0.3924 - val_f1_score: 0.7414\n",
      "\n",
      "Epoch 00118: val_f1_score did not improve from 0.82090\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1429 - f1_score: 0.7676 - val_loss: 0.3468 - val_f1_score: 0.7500\n",
      "\n",
      "Epoch 00119: val_f1_score did not improve from 0.82090\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1263 - f1_score: 0.8175 - val_loss: 0.3594 - val_f1_score: 0.7350\n",
      "\n",
      "Epoch 00120: val_f1_score did not improve from 0.82090\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.1525 - f1_score: 0.7719 - val_loss: 0.3934 - val_f1_score: 0.6786\n",
      "\n",
      "Epoch 00121: val_f1_score did not improve from 0.82090\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1563 - f1_score: 0.7498 - val_loss: 0.3440 - val_f1_score: 0.7458\n",
      "\n",
      "Epoch 00122: val_f1_score did not improve from 0.82090\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1530 - f1_score: 0.7835 - val_loss: 0.4176 - val_f1_score: 0.6667\n",
      "\n",
      "Epoch 00123: val_f1_score did not improve from 0.82090\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1579 - f1_score: 0.7155 - val_loss: 0.3321 - val_f1_score: 0.7742\n",
      "\n",
      "Epoch 00124: val_f1_score did not improve from 0.82090\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.1540 - f1_score: 0.7471 - val_loss: 0.3484 - val_f1_score: 0.7395\n",
      "\n",
      "Epoch 00125: val_f1_score did not improve from 0.82090\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1477 - f1_score: 0.8024 - val_loss: 0.3950 - val_f1_score: 0.7257\n",
      "\n",
      "Epoch 00126: val_f1_score did not improve from 0.82090\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1305 - f1_score: 0.7897 - val_loss: 0.3607 - val_f1_score: 0.7667\n",
      "\n",
      "Epoch 00127: val_f1_score did not improve from 0.82090\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.1360 - f1_score: 0.8468 - val_loss: 0.3362 - val_f1_score: 0.7742\n",
      "\n",
      "Epoch 00128: val_f1_score did not improve from 0.82090\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1756 - f1_score: 0.7652 - val_loss: 0.3923 - val_f1_score: 0.7368\n",
      "\n",
      "Epoch 00129: val_f1_score did not improve from 0.82090\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1197 - f1_score: 0.8246 - val_loss: 0.3449 - val_f1_score: 0.7833\n",
      "\n",
      "Epoch 00130: val_f1_score did not improve from 0.82090\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1269 - f1_score: 0.8521 - val_loss: 0.4000 - val_f1_score: 0.6604\n",
      "\n",
      "Epoch 00131: val_f1_score did not improve from 0.82090\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1441 - f1_score: 0.7741 - val_loss: 0.3561 - val_f1_score: 0.7627\n",
      "\n",
      "Epoch 00132: val_f1_score did not improve from 0.82090\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1092 - f1_score: 0.8435 - val_loss: 0.3579 - val_f1_score: 0.7521\n",
      "\n",
      "Epoch 00133: val_f1_score did not improve from 0.82090\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.1189 - f1_score: 0.8670 - val_loss: 0.3334 - val_f1_score: 0.7742\n",
      "\n",
      "Epoch 00134: val_f1_score did not improve from 0.82090\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1410 - f1_score: 0.8037 - val_loss: 0.3651 - val_f1_score: 0.7521\n",
      "\n",
      "Epoch 00135: val_f1_score did not improve from 0.82090\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.1200 - f1_score: 0.8669 - val_loss: 0.3980 - val_f1_score: 0.7521\n",
      "\n",
      "Epoch 00136: val_f1_score did not improve from 0.82090\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1154 - f1_score: 0.8001 - val_loss: 0.3814 - val_f1_score: 0.7458\n",
      "\n",
      "Epoch 00137: val_f1_score did not improve from 0.82090\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1084 - f1_score: 0.8457 - val_loss: 0.5002 - val_f1_score: 0.6602\n",
      "\n",
      "Epoch 00138: val_f1_score did not improve from 0.82090\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.1830 - f1_score: 0.5996 - val_loss: 0.3330 - val_f1_score: 0.7717\n",
      "\n",
      "Epoch 00139: val_f1_score did not improve from 0.82090\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1463 - f1_score: 0.8030 - val_loss: 0.4064 - val_f1_score: 0.6847\n",
      "\n",
      "Epoch 00140: val_f1_score did not improve from 0.82090\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.0972 - f1_score: 0.8518 - val_loss: 0.3483 - val_f1_score: 0.7769\n",
      "\n",
      "Epoch 00141: val_f1_score did not improve from 0.82090\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.1145 - f1_score: 0.8363 - val_loss: 0.4184 - val_f1_score: 0.7027\n",
      "\n",
      "Epoch 00142: val_f1_score did not improve from 0.82090\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1102 - f1_score: 0.7964 - val_loss: 0.3656 - val_f1_score: 0.7541\n",
      "\n",
      "Epoch 00143: val_f1_score did not improve from 0.82090\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1146 - f1_score: 0.8413 - val_loss: 0.4245 - val_f1_score: 0.7193\n",
      "\n",
      "Epoch 00144: val_f1_score did not improve from 0.82090\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.1033 - f1_score: 0.8414 - val_loss: 0.3660 - val_f1_score: 0.7778\n",
      "\n",
      "Epoch 00145: val_f1_score did not improve from 0.82090\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1079 - f1_score: 0.8375 - val_loss: 0.3491 - val_f1_score: 0.7907\n",
      "\n",
      "Epoch 00146: val_f1_score did not improve from 0.82090\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1618 - f1_score: 0.7552 - val_loss: 0.4768 - val_f1_score: 0.6731\n",
      "\n",
      "Epoch 00147: val_f1_score did not improve from 0.82090\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1168 - f1_score: 0.7975 - val_loss: 0.3295 - val_f1_score: 0.8182\n",
      "\n",
      "Epoch 00148: val_f1_score did not improve from 0.82090\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.1877 - f1_score: 0.7229 - val_loss: 0.5303 - val_f1_score: 0.5773\n",
      "\n",
      "Epoch 00149: val_f1_score did not improve from 0.82090\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1709 - f1_score: 0.6622 - val_loss: 0.3372 - val_f1_score: 0.7778\n",
      "\n",
      "Epoch 00150: val_f1_score did not improve from 0.82090\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1330 - f1_score: 0.7605 - val_loss: 0.3772 - val_f1_score: 0.7193\n",
      "\n",
      "Epoch 00151: val_f1_score did not improve from 0.82090\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1159 - f1_score: 0.7916 - val_loss: 0.3588 - val_f1_score: 0.7438\n",
      "\n",
      "Epoch 00152: val_f1_score did not improve from 0.82090\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.1132 - f1_score: 0.8154 - val_loss: 0.4057 - val_f1_score: 0.7350\n",
      "\n",
      "Epoch 00153: val_f1_score did not improve from 0.82090\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1147 - f1_score: 0.8352 - val_loss: 0.3604 - val_f1_score: 0.7581\n",
      "\n",
      "Epoch 00154: val_f1_score did not improve from 0.82090\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.1142 - f1_score: 0.8427 - val_loss: 0.4353 - val_f1_score: 0.7478\n",
      "\n",
      "Epoch 00155: val_f1_score did not improve from 0.82090\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.0993 - f1_score: 0.8701 - val_loss: 0.3632 - val_f1_score: 0.7717\n",
      "\n",
      "Epoch 00156: val_f1_score did not improve from 0.82090\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.1185 - f1_score: 0.7815 - val_loss: 0.3730 - val_f1_score: 0.7603\n",
      "\n",
      "Epoch 00157: val_f1_score did not improve from 0.82090\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1390 - f1_score: 0.7943 - val_loss: 0.5385 - val_f1_score: 0.6729\n",
      "\n",
      "Epoch 00158: val_f1_score did not improve from 0.82090\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.1239 - f1_score: 0.7975 - val_loss: 0.3557 - val_f1_score: 0.7874\n",
      "\n",
      "Epoch 00159: val_f1_score did not improve from 0.82090\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1055 - f1_score: 0.8412 - val_loss: 0.4483 - val_f1_score: 0.7103\n",
      "\n",
      "Epoch 00160: val_f1_score did not improve from 0.82090\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.1110 - f1_score: 0.8404 - val_loss: 0.3730 - val_f1_score: 0.7731\n",
      "\n",
      "Epoch 00161: val_f1_score did not improve from 0.82090\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.1005 - f1_score: 0.8322 - val_loss: 0.4507 - val_f1_score: 0.7156\n",
      "\n",
      "Epoch 00162: val_f1_score did not improve from 0.82090\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1171 - f1_score: 0.7890 - val_loss: 0.3993 - val_f1_score: 0.7241\n",
      "\n",
      "Epoch 00163: val_f1_score did not improve from 0.82090\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1096 - f1_score: 0.8263 - val_loss: 0.3780 - val_f1_score: 0.7480\n",
      "\n",
      "Epoch 00164: val_f1_score did not improve from 0.82090\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1043 - f1_score: 0.8553 - val_loss: 0.3947 - val_f1_score: 0.7414\n",
      "\n",
      "Epoch 00165: val_f1_score did not improve from 0.82090\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1020 - f1_score: 0.8542 - val_loss: 0.3736 - val_f1_score: 0.7500\n",
      "\n",
      "Epoch 00166: val_f1_score did not improve from 0.82090\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.0908 - f1_score: 0.8835 - val_loss: 0.3720 - val_f1_score: 0.7500\n",
      "\n",
      "Epoch 00167: val_f1_score did not improve from 0.82090\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.1178 - f1_score: 0.8477 - val_loss: 0.4536 - val_f1_score: 0.7207\n",
      "\n",
      "Epoch 00168: val_f1_score did not improve from 0.82090\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.1154 - f1_score: 0.8499 - val_loss: 0.3740 - val_f1_score: 0.7680\n",
      "\n",
      "Epoch 00169: val_f1_score did not improve from 0.82090\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.1074 - f1_score: 0.8372 - val_loss: 0.5381 - val_f1_score: 0.6667\n",
      "\n",
      "Epoch 00170: val_f1_score did not improve from 0.82090\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.1109 - f1_score: 0.8051 - val_loss: 0.3644 - val_f1_score: 0.7813\n",
      "\n",
      "Epoch 00171: val_f1_score did not improve from 0.82090\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.1243 - f1_score: 0.8087 - val_loss: 0.5426 - val_f1_score: 0.6731\n",
      "\n",
      "Epoch 00172: val_f1_score did not improve from 0.82090\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.0949 - f1_score: 0.8371 - val_loss: 0.3649 - val_f1_score: 0.8000\n",
      "\n",
      "Epoch 00173: val_f1_score did not improve from 0.82090\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.1475 - f1_score: 0.8033 - val_loss: 0.5638 - val_f1_score: 0.6667\n",
      "\n",
      "Epoch 00174: val_f1_score did not improve from 0.82090\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1296 - f1_score: 0.7985 - val_loss: 0.3762 - val_f1_score: 0.7541\n",
      "\n",
      "Epoch 00175: val_f1_score did not improve from 0.82090\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1034 - f1_score: 0.8781 - val_loss: 0.4443 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00176: val_f1_score did not improve from 0.82090\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.0909 - f1_score: 0.8470 - val_loss: 0.4367 - val_f1_score: 0.7414\n",
      "\n",
      "Epoch 00177: val_f1_score did not improve from 0.82090\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.0829 - f1_score: 0.8787 - val_loss: 0.4558 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00178: val_f1_score did not improve from 0.82090\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.0716 - f1_score: 0.8893 - val_loss: 0.4367 - val_f1_score: 0.7438\n",
      "\n",
      "Epoch 00179: val_f1_score did not improve from 0.82090\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.0807 - f1_score: 0.8937 - val_loss: 0.5723 - val_f1_score: 0.6729\n",
      "\n",
      "Epoch 00180: val_f1_score did not improve from 0.82090\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.0884 - f1_score: 0.8461 - val_loss: 0.4489 - val_f1_score: 0.7377\n",
      "\n",
      "Epoch 00181: val_f1_score did not improve from 0.82090\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 0.0916 - f1_score: 0.8551 - val_loss: 0.5036 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00182: val_f1_score did not improve from 0.82090\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.1235 - f1_score: 0.8178 - val_loss: 0.4465 - val_f1_score: 0.7541\n",
      "\n",
      "Epoch 00183: val_f1_score did not improve from 0.82090\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1420 - f1_score: 0.7779 - val_loss: 0.4694 - val_f1_score: 0.7317\n",
      "\n",
      "Epoch 00184: val_f1_score did not improve from 0.82090\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.1076 - f1_score: 0.8378 - val_loss: 0.5232 - val_f1_score: 0.7069\n",
      "\n",
      "Epoch 00185: val_f1_score did not improve from 0.82090\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1079 - f1_score: 0.8814 - val_loss: 0.4483 - val_f1_score: 0.7167\n",
      "\n",
      "Epoch 00186: val_f1_score did not improve from 0.82090\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.0657 - f1_score: 0.9243 - val_loss: 0.6719 - val_f1_score: 0.5474\n",
      "\n",
      "Epoch 00187: val_f1_score did not improve from 0.82090\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.1540 - f1_score: 0.7033 - val_loss: 0.3950 - val_f1_score: 0.7737\n",
      "\n",
      "Epoch 00188: val_f1_score did not improve from 0.82090\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.1791 - f1_score: 0.7154 - val_loss: 0.4869 - val_f1_score: 0.6604\n",
      "\n",
      "Epoch 00189: val_f1_score did not improve from 0.82090\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.1451 - f1_score: 0.7484 - val_loss: 0.4086 - val_f1_score: 0.7273\n",
      "\n",
      "Epoch 00190: val_f1_score did not improve from 0.82090\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.1046 - f1_score: 0.8902 - val_loss: 0.4309 - val_f1_score: 0.7458\n",
      "\n",
      "Epoch 00191: val_f1_score did not improve from 0.82090\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.0976 - f1_score: 0.8614 - val_loss: 0.4604 - val_f1_score: 0.7521\n",
      "\n",
      "Epoch 00192: val_f1_score did not improve from 0.82090\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.0897 - f1_score: 0.8735 - val_loss: 0.4750 - val_f1_score: 0.7288\n",
      "\n",
      "Epoch 00193: val_f1_score did not improve from 0.82090\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.0687 - f1_score: 0.8810 - val_loss: 0.4425 - val_f1_score: 0.7333\n",
      "\n",
      "Epoch 00194: val_f1_score did not improve from 0.82090\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.0945 - f1_score: 0.8479 - val_loss: 0.4705 - val_f1_score: 0.7395\n",
      "\n",
      "Epoch 00195: val_f1_score did not improve from 0.82090\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.0763 - f1_score: 0.9034 - val_loss: 0.4997 - val_f1_score: 0.7130\n",
      "\n",
      "Epoch 00196: val_f1_score did not improve from 0.82090\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.0767 - f1_score: 0.8933 - val_loss: 0.4464 - val_f1_score: 0.7288\n",
      "\n",
      "Epoch 00197: val_f1_score did not improve from 0.82090\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.0897 - f1_score: 0.8619 - val_loss: 0.4184 - val_f1_score: 0.7778\n",
      "\n",
      "Epoch 00198: val_f1_score did not improve from 0.82090\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1068 - f1_score: 0.8737 - val_loss: 0.5612 - val_f1_score: 0.6916\n",
      "\n",
      "Epoch 00199: val_f1_score did not improve from 0.82090\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.1184 - f1_score: 0.8108 - val_loss: 0.4278 - val_f1_score: 0.7840\n",
      "\n",
      "Epoch 00200: val_f1_score did not improve from 0.82090\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc988980700>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model.fit(padded, training_labels, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2, callbacks=[checkpoint]) # 훈련"
   ]
  },
  {
   "source": [
    "## 모델 예측\n",
    "### 가장 정확도가 높은 모델을 사용 후 예측해준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filename)\n",
    "\n",
    "prediction = model.predict(testing_padded)"
   ]
  },
  {
   "source": [
    "### 출력값이 0~1 사이의 실수로 되어있으니 이를 0.5 기준으로 'Spam', 'Non-Spam'으로 나눠준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = []\n",
    "\n",
    "for i in range(0, len(prediction)) :\n",
    "    if prediction[i] > 0.5 :\n",
    "        pred_final.append(1)\n",
    "    else:\n",
    "        pred_final.append(0)\n",
    "\n",
    "pred_final = np.asarray(pred_final)"
   ]
  },
  {
   "source": [
    "### csv파일에 저장해준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_toCsv = {\"Prediction\": pred_final}\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['Prediction'] = predict_toCsv[\"Prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('김민규_Adv_result.csv', index=False) # 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
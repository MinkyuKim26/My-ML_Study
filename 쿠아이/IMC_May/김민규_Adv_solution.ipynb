{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dec8a5ff48d192d3613b9e6713a72fe103c8e5d5c44dc7b8ee41f3376e4a4f37",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5월 IMC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 임포트"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # 넘파이\n",
    "import matplotlib.pyplot as plt # 매트플롯립\n",
    "import pandas as pd # 판다스(csv)\n",
    "import re # 정규식 표현\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "train = pd.read_csv('/Users/minguinho/Documents/AI_Datasets/IMC_May_data/IMC_May_train.csv', encoding='cp949')\n",
    "test = pd.read_csv('/Users/minguinho/Documents/AI_Datasets/IMC_May_data/IMC_May_test.csv', encoding='cp949')"
   ]
  },
  {
   "source": [
    "### 트레이닝 데이터 975개, 테스트 데이터 107개가 있다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 975 entries, 0 to 974\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Message_body  975 non-null    object\n 1   Label         975 non-null    object\ndtypes: object(2)\nmemory usage: 15.4+ KB\ntrain :  None\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 107 entries, 0 to 106\nData columns (total 1 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Message_body  107 non-null    object\ndtypes: object(1)\nmemory usage: 984.0+ bytes\ntest :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"train : \", train.info())\n",
    "print()\n",
    "print(\"test : \", test.info())\n"
   ]
  },
  {
   "source": [
    "### dataframe에서 메일 제목과 라벨값을 추출해 데이터셋을 만들어준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Non-Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n",
      "Spam\n"
     ]
    }
   ],
   "source": [
    "training_sentences = train['Message_body'].tolist()\n",
    "testing_sentences = test['Message_body'].tolist()\n",
    "training_temp = train['Label'].tolist()\n",
    "\n",
    "training_labels = []\n",
    "for i in range(0, 975) : \n",
    "    print(training_temp[i])\n",
    "    if training_temp[i] == 'Non-Spam':\n",
    "        print(training_temp[i])\n",
    "        training_labels.append(0)\n",
    "    else:\n",
    "        training_labels.append(1)\n",
    "\n",
    "training_labels = np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 # 단어 크기\n",
    "embedding_dim = 16 # 임패딩 단위\n",
    "max_length = 128 # 리뷰에 들어있는 최대 단어 갯수. 아무리 길어도 단어 120개가 한계다...라는 것을 나타냄.\n",
    "trunc_type='post' # truncation type. 단어 갯수가 120개보다 많을 때 앞에서부터 자를건지 뒤에서부터 자를건지 결정함 post는 문자열이 120개보다 많으면 뒤에서 자르겠다는 뜻.\n",
    "oov_tok = \"<OOV>\" # 토큰 딕셔너리에 없는 단어는 OOV로 표시\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok) # num_words = vocab_size : 가장 많이 쓰이는 단어 'vocab_size'개만 토큰화 하는거. 그러니까 토큰화 대상인 문자여렝 단어가 20000개 있으면 많이 쓰이는 순으로 나열한 뒤 앞에 있는 10000개의 단어만 토큰화 하고 나머지 10000개는 토큰화하지 않는다. 얘들은 뭐 OOV로 나오겠지\n",
    "tokenizer.fit_on_texts(training_sentences) # 토큰화\n",
    "word_index = tokenizer.word_index # 토큰화 결과 가져오기\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) # 문장들을 토큰 순서로 나열하기\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type) # 패딩. \n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences) # 훈련셋으로 만든 토크나이저로 테스트셋의 시퀀스 출력. 토큰화 하지않은 단어가 많기 때문에 기존에 만들어놨던 걸로 한다. 만약 토크나이저를 새로 만든다면 두 토크나이저는 다른 토크나이저가 되기 때문에 원하는 결과를 얻을 수 없다. \n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length) # 패딩\n"
   ]
  },
  {
   "source": [
    "## 모델 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "filename = 'checkpoint_bestModel.h5'.format(EPOCH, BATCH_SIZE)\n",
    "checkpoint = ModelCheckpoint(filename,             # 파일 이름\n",
    "                             monitor='val_accuracy',   # val_loss 값이 개선되면 개선 모델을 filename으로 저장\n",
    "                             verbose=1,            # 로그 출력\n",
    "                             save_best_only=True,  # 가장 최고의 val_loss를 보이는 모델만 저장\n",
    "                             mode='auto'           # auto 모드. 자동으로 최고의 모델을 찾는다\n",
    "                            )\n",
    "\n",
    "# 모델 생성\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[None]),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), # 첫번째 LSTM. return_sequences=True를 해줘야한다. 두번 째 LSTM에 모든 시퀀스(아웃풋)을 다음 LSTM에 넣어야한다. \n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # 두번째 LSTM\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) # 컴파일. 긍정, 부정밖에 없으니 이진 분류를 하니까 binary_crossentropy를 손실 함수로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 10s 575ms/step - loss: 0.7461 - accuracy: 0.1705 - val_loss: 0.6923 - val_accuracy: 0.7128\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.71282, saving model to checkpoint_bestModel.h5\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.6923 - accuracy: 0.6776 - val_loss: 0.6906 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.71282\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.6876 - accuracy: 0.8668 - val_loss: 0.6894 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.71282\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.6853 - accuracy: 0.8539 - val_loss: 0.6882 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.71282\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.6825 - accuracy: 0.8660 - val_loss: 0.6870 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.71282\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.6803 - accuracy: 0.8581 - val_loss: 0.6858 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.71282\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.6780 - accuracy: 0.8592 - val_loss: 0.6847 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.71282\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.6747 - accuracy: 0.8802 - val_loss: 0.6836 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.71282\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.6729 - accuracy: 0.8674 - val_loss: 0.6824 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.71282\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.6702 - accuracy: 0.8714 - val_loss: 0.6813 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.71282\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.6676 - accuracy: 0.8748 - val_loss: 0.6802 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.71282\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 0.6656 - accuracy: 0.8685 - val_loss: 0.6791 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.71282\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.6633 - accuracy: 0.8682 - val_loss: 0.6780 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.71282\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.6622 - accuracy: 0.8540 - val_loss: 0.6769 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.71282\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 0.6584 - accuracy: 0.8698 - val_loss: 0.6758 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.71282\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.6553 - accuracy: 0.8771 - val_loss: 0.6748 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.71282\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 0.6545 - accuracy: 0.8639 - val_loss: 0.6738 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.71282\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.6519 - accuracy: 0.8683 - val_loss: 0.6728 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.71282\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.6495 - accuracy: 0.8693 - val_loss: 0.6719 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.71282\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 0.6456 - accuracy: 0.8830 - val_loss: 0.6709 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.71282\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.6451 - accuracy: 0.8700 - val_loss: 0.6699 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.71282\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 0.6419 - accuracy: 0.8768 - val_loss: 0.6689 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.71282\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.6399 - accuracy: 0.8747 - val_loss: 0.6680 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.71282\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.6380 - accuracy: 0.8729 - val_loss: 0.6671 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.71282\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.6350 - accuracy: 0.8781 - val_loss: 0.6661 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.71282\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.6330 - accuracy: 0.8764 - val_loss: 0.6652 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.71282\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 0.6309 - accuracy: 0.8755 - val_loss: 0.6643 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.71282\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 0.6297 - accuracy: 0.8703 - val_loss: 0.6634 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.71282\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.6289 - accuracy: 0.8637 - val_loss: 0.6625 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.71282\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.6255 - accuracy: 0.8708 - val_loss: 0.6617 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.71282\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.6218 - accuracy: 0.8791 - val_loss: 0.6609 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.71282\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.6194 - accuracy: 0.8808 - val_loss: 0.6601 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.71282\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 0.6228 - accuracy: 0.8553 - val_loss: 0.6593 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.71282\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.6157 - accuracy: 0.8791 - val_loss: 0.6585 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.71282\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.6142 - accuracy: 0.8763 - val_loss: 0.6577 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.71282\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.6160 - accuracy: 0.8598 - val_loss: 0.6569 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.71282\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.6125 - accuracy: 0.8669 - val_loss: 0.6561 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.71282\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.6079 - accuracy: 0.8776 - val_loss: 0.6554 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.71282\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.6092 - accuracy: 0.8647 - val_loss: 0.6546 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.71282\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.6072 - accuracy: 0.8649 - val_loss: 0.6539 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.71282\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.6034 - accuracy: 0.8723 - val_loss: 0.6531 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.71282\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.6020 - accuracy: 0.8703 - val_loss: 0.6524 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.71282\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.6019 - accuracy: 0.8634 - val_loss: 0.6517 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.71282\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.5989 - accuracy: 0.8673 - val_loss: 0.6510 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.71282\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.5954 - accuracy: 0.8730 - val_loss: 0.6503 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.71282\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.5977 - accuracy: 0.8591 - val_loss: 0.6496 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.71282\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.5937 - accuracy: 0.8666 - val_loss: 0.6490 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.71282\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.5900 - accuracy: 0.8727 - val_loss: 0.6483 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.71282\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.5874 - accuracy: 0.8750 - val_loss: 0.6477 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.71282\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.5845 - accuracy: 0.8785 - val_loss: 0.6471 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.71282\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.5805 - accuracy: 0.8851 - val_loss: 0.6464 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.71282\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.5850 - accuracy: 0.8660 - val_loss: 0.6458 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.71282\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.5849 - accuracy: 0.8612 - val_loss: 0.6452 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.71282\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.5819 - accuracy: 0.8649 - val_loss: 0.6446 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.71282\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.5805 - accuracy: 0.8640 - val_loss: 0.6440 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.71282\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.5772 - accuracy: 0.8685 - val_loss: 0.6435 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.71282\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.5812 - accuracy: 0.8526 - val_loss: 0.6429 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.71282\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.5717 - accuracy: 0.8740 - val_loss: 0.6423 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.71282\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.5703 - accuracy: 0.8729 - val_loss: 0.6418 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.71282\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 0.5686 - accuracy: 0.8730 - val_loss: 0.6412 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.71282\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 0.5676 - accuracy: 0.8713 - val_loss: 0.6407 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.71282\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.5657 - accuracy: 0.8718 - val_loss: 0.6402 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.71282\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.5646 - accuracy: 0.8703 - val_loss: 0.6397 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.71282\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.5628 - accuracy: 0.8707 - val_loss: 0.6392 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.71282\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.5602 - accuracy: 0.8731 - val_loss: 0.6387 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.71282\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.5538 - accuracy: 0.8846 - val_loss: 0.6383 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.71282\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.5577 - accuracy: 0.8713 - val_loss: 0.6378 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.71282\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.5558 - accuracy: 0.8721 - val_loss: 0.6374 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.71282\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.5570 - accuracy: 0.8658 - val_loss: 0.6369 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.71282\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.5474 - accuracy: 0.8842 - val_loss: 0.6365 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.71282\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.5469 - accuracy: 0.8819 - val_loss: 0.6361 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.71282\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.5503 - accuracy: 0.8708 - val_loss: 0.6357 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.71282\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.5459 - accuracy: 0.8772 - val_loss: 0.6353 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.71282\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.5500 - accuracy: 0.8651 - val_loss: 0.6350 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.71282\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.5402 - accuracy: 0.8831 - val_loss: 0.6346 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.71282\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.5471 - accuracy: 0.8655 - val_loss: 0.6342 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.71282\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.5365 - accuracy: 0.8846 - val_loss: 0.6338 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.71282\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.5397 - accuracy: 0.8748 - val_loss: 0.6335 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.71282\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.5454 - accuracy: 0.8600 - val_loss: 0.6331 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.71282\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.5367 - accuracy: 0.8747 - val_loss: 0.6328 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.71282\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.5351 - accuracy: 0.8750 - val_loss: 0.6324 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.71282\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.5413 - accuracy: 0.8600 - val_loss: 0.6321 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.71282\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.5385 - accuracy: 0.8629 - val_loss: 0.6318 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.71282\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 0.5366 - accuracy: 0.8640 - val_loss: 0.6315 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.71282\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.5318 - accuracy: 0.8706 - val_loss: 0.6312 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.71282\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.5309 - accuracy: 0.8697 - val_loss: 0.6309 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.71282\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.5277 - accuracy: 0.8730 - val_loss: 0.6306 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.71282\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.5262 - accuracy: 0.8733 - val_loss: 0.6304 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.71282\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.5287 - accuracy: 0.8663 - val_loss: 0.6301 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.71282\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 0.5257 - accuracy: 0.8694 - val_loss: 0.6298 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.71282\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.5255 - accuracy: 0.8674 - val_loss: 0.6296 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.71282\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.5241 - accuracy: 0.8675 - val_loss: 0.6293 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.71282\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.5156 - accuracy: 0.8802 - val_loss: 0.6291 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.71282\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.5215 - accuracy: 0.8678 - val_loss: 0.6289 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.71282\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.5231 - accuracy: 0.8629 - val_loss: 0.6287 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.71282\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.5117 - accuracy: 0.8803 - val_loss: 0.6285 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.71282\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.5126 - accuracy: 0.8768 - val_loss: 0.6283 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.71282\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.5203 - accuracy: 0.8618 - val_loss: 0.6281 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.71282\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.5083 - accuracy: 0.8797 - val_loss: 0.6279 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.71282\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.5173 - accuracy: 0.8628 - val_loss: 0.6277 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.71282\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 0.5051 - accuracy: 0.8808 - val_loss: 0.6275 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.71282\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 0.5203 - accuracy: 0.8544 - val_loss: 0.6274 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.71282\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 0.5147 - accuracy: 0.8615 - val_loss: 0.6272 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.71282\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.5013 - accuracy: 0.8808 - val_loss: 0.6270 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.71282\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4994 - accuracy: 0.8818 - val_loss: 0.6269 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.71282\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.5134 - accuracy: 0.8580 - val_loss: 0.6268 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.71282\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.5010 - accuracy: 0.8756 - val_loss: 0.6266 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.71282\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.5020 - accuracy: 0.8723 - val_loss: 0.6265 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.71282\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.5003 - accuracy: 0.8731 - val_loss: 0.6264 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.71282\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4971 - accuracy: 0.8762 - val_loss: 0.6263 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.71282\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.4990 - accuracy: 0.8717 - val_loss: 0.6262 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.71282\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4962 - accuracy: 0.8742 - val_loss: 0.6261 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.71282\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.5029 - accuracy: 0.8627 - val_loss: 0.6260 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.71282\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.4931 - accuracy: 0.8754 - val_loss: 0.6259 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.71282\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4959 - accuracy: 0.8698 - val_loss: 0.6258 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.71282\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4940 - accuracy: 0.8710 - val_loss: 0.6257 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.71282\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 0.4850 - accuracy: 0.8823 - val_loss: 0.6257 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.71282\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4874 - accuracy: 0.8773 - val_loss: 0.6256 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.71282\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4903 - accuracy: 0.8717 - val_loss: 0.6255 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.71282\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4947 - accuracy: 0.8642 - val_loss: 0.6255 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.71282\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4928 - accuracy: 0.8653 - val_loss: 0.6255 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.71282\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4890 - accuracy: 0.8692 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.71282\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4882 - accuracy: 0.8690 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.71282\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.4832 - accuracy: 0.8744 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.71282\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.4844 - accuracy: 0.8714 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.71282\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.4814 - accuracy: 0.8741 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.71282\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.4811 - accuracy: 0.8733 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.71282\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4782 - accuracy: 0.8760 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.71282\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.4908 - accuracy: 0.8581 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.71282\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4782 - accuracy: 0.8734 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.71282\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.4785 - accuracy: 0.8719 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.71282\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.4750 - accuracy: 0.8752 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.71282\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.4816 - accuracy: 0.8656 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.71282\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4779 - accuracy: 0.8692 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.71282\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.4687 - accuracy: 0.8797 - val_loss: 0.6254 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.71282\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.4796 - accuracy: 0.8648 - val_loss: 0.6255 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.71282\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.4775 - accuracy: 0.8664 - val_loss: 0.6255 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.71282\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.4798 - accuracy: 0.8626 - val_loss: 0.6256 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.71282\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4742 - accuracy: 0.8684 - val_loss: 0.6256 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.71282\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.4675 - accuracy: 0.8756 - val_loss: 0.6257 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.71282\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.4766 - accuracy: 0.8635 - val_loss: 0.6257 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.71282\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4728 - accuracy: 0.8670 - val_loss: 0.6258 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.71282\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.4774 - accuracy: 0.8605 - val_loss: 0.6259 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.71282\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4679 - accuracy: 0.8708 - val_loss: 0.6260 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.71282\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 0.4552 - accuracy: 0.8850 - val_loss: 0.6260 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.71282\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.4745 - accuracy: 0.8612 - val_loss: 0.6261 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.71282\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4603 - accuracy: 0.8769 - val_loss: 0.6262 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.71282\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4627 - accuracy: 0.8732 - val_loss: 0.6263 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.71282\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.4628 - accuracy: 0.8722 - val_loss: 0.6264 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.71282\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.4664 - accuracy: 0.8673 - val_loss: 0.6265 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.71282\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.4627 - accuracy: 0.8706 - val_loss: 0.6266 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.71282\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.4537 - accuracy: 0.8800 - val_loss: 0.6267 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.71282\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.4617 - accuracy: 0.8700 - val_loss: 0.6268 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.71282\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4745 - accuracy: 0.8546 - val_loss: 0.6269 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.71282\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4626 - accuracy: 0.8673 - val_loss: 0.6271 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.71282\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4468 - accuracy: 0.8842 - val_loss: 0.6272 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.71282\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4587 - accuracy: 0.8700 - val_loss: 0.6273 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.71282\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.4609 - accuracy: 0.8667 - val_loss: 0.6274 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.71282\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.4578 - accuracy: 0.8694 - val_loss: 0.6276 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.71282\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4499 - accuracy: 0.8773 - val_loss: 0.6277 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.71282\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4635 - accuracy: 0.8615 - val_loss: 0.6279 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.71282\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4487 - accuracy: 0.8769 - val_loss: 0.6280 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.71282\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.4627 - accuracy: 0.8609 - val_loss: 0.6282 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.71282\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.4540 - accuracy: 0.8696 - val_loss: 0.6284 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.71282\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4498 - accuracy: 0.8733 - val_loss: 0.6285 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.71282\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 0.4509 - accuracy: 0.8715 - val_loss: 0.6287 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.71282\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.4462 - accuracy: 0.8757 - val_loss: 0.6289 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.71282\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.4477 - accuracy: 0.8733 - val_loss: 0.6290 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.71282\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.4473 - accuracy: 0.8731 - val_loss: 0.6292 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.71282\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4556 - accuracy: 0.8637 - val_loss: 0.6294 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.71282\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.4424 - accuracy: 0.8768 - val_loss: 0.6296 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.71282\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.4555 - accuracy: 0.8625 - val_loss: 0.6297 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.71282\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4554 - accuracy: 0.8620 - val_loss: 0.6299 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.71282\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.4455 - accuracy: 0.8716 - val_loss: 0.6301 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.71282\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.4468 - accuracy: 0.8696 - val_loss: 0.6303 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.71282\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.4424 - accuracy: 0.8734 - val_loss: 0.6305 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.71282\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.4358 - accuracy: 0.8794 - val_loss: 0.6307 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.71282\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.4421 - accuracy: 0.8724 - val_loss: 0.6309 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.71282\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4371 - accuracy: 0.8769 - val_loss: 0.6312 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.71282\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.4380 - accuracy: 0.8753 - val_loss: 0.6314 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.71282\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.4379 - accuracy: 0.8747 - val_loss: 0.6316 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.71282\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.4285 - accuracy: 0.8835 - val_loss: 0.6318 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.71282\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 0.4444 - accuracy: 0.8671 - val_loss: 0.6320 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.71282\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4467 - accuracy: 0.8642 - val_loss: 0.6323 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.71282\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.4436 - accuracy: 0.8667 - val_loss: 0.6325 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.71282\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 0.4385 - accuracy: 0.8711 - val_loss: 0.6327 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.71282\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.4451 - accuracy: 0.8642 - val_loss: 0.6330 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.71282\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 0.4331 - accuracy: 0.8753 - val_loss: 0.6332 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.71282\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 0.4350 - accuracy: 0.8730 - val_loss: 0.6334 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.71282\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.4320 - accuracy: 0.8753 - val_loss: 0.6336 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.71282\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 0.4263 - accuracy: 0.8803 - val_loss: 0.6339 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.71282\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.4349 - accuracy: 0.8715 - val_loss: 0.6341 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.71282\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.4466 - accuracy: 0.8599 - val_loss: 0.6344 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.71282\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 0.4300 - accuracy: 0.8752 - val_loss: 0.6346 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.71282\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 0.4381 - accuracy: 0.8670 - val_loss: 0.6348 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.71282\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.4343 - accuracy: 0.8702 - val_loss: 0.6351 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.71282\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 0.4354 - accuracy: 0.8686 - val_loss: 0.6353 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.71282\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 0.4328 - accuracy: 0.8706 - val_loss: 0.6356 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.71282\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.4329 - accuracy: 0.8701 - val_loss: 0.6358 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.71282\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 1s 172ms/step - loss: 0.4150 - accuracy: 0.8862 - val_loss: 0.6361 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.71282\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb08130b220>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model.fit(padded, training_labels, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2, callbacks=[checkpoint]) # 훈련"
   ]
  },
  {
   "source": [
    "## 모델 예측\n",
    "### 가장 정확도가 높은 모델을 사용 후 예측해준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filename)\n",
    "\n",
    "prediction = model.predict(testing_padded)"
   ]
  },
  {
   "source": [
    "### 출력값이 0~1 사이의 실수로 되어있으니 이를 0.5 기준으로 'Spam', 'Non-Spam'으로 나눠준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str = []\n",
    "\n",
    "for i in range(0, len(prediction)) :\n",
    "    if prediction[i] > 0.5 :\n",
    "        pred_str.append('Spam')\n",
    "    else:\n",
    "        pred_str.append('Non-Spam')\n",
    "\n",
    "pred_str = np.asarray(pred_str)"
   ]
  },
  {
   "source": [
    "### csv파일에 저장해준다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_toCsv = {\"Message_body\": testing_sentences, \"Label\": pred_str}\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['Product'] = predict_toCsv[\"Message_body\"]\n",
    "submission['Price'] = predict_toCsv[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('김민규_Adv_result.csv', index=False) # 파일 저장"
   ]
  }
 ]
}